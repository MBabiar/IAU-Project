{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "063d3ffe",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import kagglehub\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    InputLayer,\n",
    "    MaxPooling2D,\n",
    "    Rescaling,\n",
    ")\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a67b7",
   "metadata": {},
   "source": [
    "## Import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e2c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_raw_data():\n",
    "    # Load dataset\n",
    "    path = kagglehub.dataset_download(\"arashnic/faces-age-detection-dataset\")\n",
    "\n",
    "    # Define path to files\n",
    "    faces_path = os.path.join(path, \"faces\")\n",
    "    train_path = os.path.join(faces_path, \"Train\")\n",
    "\n",
    "    # Create directory for raw data\n",
    "    export_path = \"../data/raw/faces\"\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "\n",
    "    # Copy files to raw data directory\n",
    "    shutil.copytree(train_path, os.path.join(export_path, \"images\"), dirs_exist_ok=True)\n",
    "    shutil.copy(os.path.join(faces_path, \"train.csv\"), os.path.join(export_path, \"train.csv\"))\n",
    "\n",
    "    export_path = \"../data/raw\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"../data/raw/faces\"):\n",
    "    import_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb901a",
   "metadata": {},
   "source": [
    "-   Faces_02 has no csv file for classification so we are gonna skip it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4adb0b5",
   "metadata": {},
   "source": [
    "## Definitons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6d925",
   "metadata": {},
   "source": [
    "### Determinism\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18031abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making model more deterministic (not perfect but better than nothing)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.random.set_global_generator(tf.random.Generator.from_seed(SEED))\n",
    "\n",
    "# This is perfect deterministic\n",
    "# tf.config.experimental.enable_op_determinism()\n",
    "# os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a88232",
   "metadata": {},
   "source": [
    "### Helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/raw/faces/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def ignore(line, cell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bf087",
   "metadata": {},
   "source": [
    "### EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2879719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get low quality images\n",
    "def get_low_quality_images(\n",
    "    images, min_width=15, min_height=15, min_aspect_ratio=0.2, max_aspect_ratio=4, min_laplacian=10, min_ssim=0.5\n",
    "):\n",
    "    low_quality_images = []\n",
    "\n",
    "    for _, row in images.iterrows():\n",
    "        image_id = row[\"ID\"]\n",
    "        image_width = row[\"width\"]\n",
    "        image_height = row[\"height\"]\n",
    "        aspect_ratio = row[\"aspect_ratio\"]\n",
    "        laplacian = row[\"laplacian\"]\n",
    "        ssim_score = row[\"ssim_score\"]\n",
    "\n",
    "        if (\n",
    "            image_width < min_width\n",
    "            or image_height < min_height\n",
    "            or aspect_ratio < min_aspect_ratio\n",
    "            or aspect_ratio > max_aspect_ratio\n",
    "            or laplacian < min_laplacian\n",
    "            or ssim_score < min_ssim\n",
    "        ):\n",
    "            low_quality_images.append(image_id)\n",
    "\n",
    "    print(f\"Number of low quality images: {len(low_quality_images)}\")\n",
    "    print(f\"Percentage of low quality images: {len(low_quality_images) / len(images) * 100:.2f}%\")\n",
    "    images = images[~images[\"ID\"].isin(low_quality_images)]\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b3ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_images(df):\n",
    "    max_id = max([int(id.replace(\".jpg\", \"\")) for id in df[\"ID\"]])\n",
    "    next_id = max_id + 1\n",
    "\n",
    "    number_of_images = df[\"Class\"].value_counts()\n",
    "\n",
    "    number_of_young = number_of_images[\"YOUNG\"]\n",
    "    number_of_middle = number_of_images[\"MIDDLE\"]\n",
    "    number_of_old = number_of_images[\"OLD\"]\n",
    "\n",
    "    maximum_images = max(number_of_young, number_of_middle, number_of_old)\n",
    "\n",
    "    multiply_young = round(maximum_images / number_of_young, 0)\n",
    "    multiply_middle = round(maximum_images / number_of_middle, 0)\n",
    "    multiply_old = round(maximum_images / number_of_old, 0)\n",
    "\n",
    "    print(f\"Number of total images: {len(df)}\")\n",
    "    print(f\"Number of young images: {number_of_young}\")\n",
    "    print(f\"Number of middle images: {number_of_middle}\")\n",
    "    print(f\"Number of old images: {number_of_old}\")\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Number of young images to multiply: {multiply_young}\")\n",
    "    print(f\"Number of middle images to multiply: {multiply_middle}\")\n",
    "    print(f\"Number of old images to multiply: {multiply_old}\")\n",
    "\n",
    "    # Create copies\n",
    "    young_df = df[df[\"Class\"] == \"YOUNG\"].copy()\n",
    "    middle_df = df[df[\"Class\"] == \"MIDDLE\"].copy()\n",
    "    old_df = df[df[\"Class\"] == \"OLD\"].copy()\n",
    "\n",
    "    # Create additional copies\n",
    "    young_additional = pd.concat([young_df] * int(multiply_young - 1)) if multiply_young > 1 else pd.DataFrame()\n",
    "    middle_additional = (\n",
    "        pd.concat([middle_df] * int(multiply_middle - 1)) if multiply_middle > 1 else pd.DataFrame()\n",
    "    )\n",
    "    old_additional = pd.concat([old_df] * int(multiply_old - 1)) if multiply_old > 1 else pd.DataFrame()\n",
    "\n",
    "    # Combine all additional copies\n",
    "    additional_df = pd.concat([young_additional, middle_additional, old_additional])\n",
    "\n",
    "    # Create new_name column for additional copies\n",
    "    additional_df[\"new_name\"] = [f\"{next_id + i}.jpg\" for i in range(len(additional_df))]\n",
    "\n",
    "    # Add new_name column to original DataFrame (same as ID for original entries)\n",
    "    df = df.assign(new_name=df[\"ID\"])\n",
    "\n",
    "    # Combine original and additional DataFrames\n",
    "    balanced_df = pd.concat([df, additional_df])\n",
    "    balanced_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "    print(f\"Number of total images after multiplication: {len(balanced_df)}\")\n",
    "    print(f\"Number of young images after multiplication: {len(balanced_df[balanced_df['Class'] == 'YOUNG'])}\")\n",
    "    print(f\"Number of middle images after multiplication: {len(balanced_df[balanced_df['Class'] == 'MIDDLE'])}\")\n",
    "    print(f\"Number of old images after multiplication: {len(balanced_df[balanced_df['Class'] == 'OLD'])}\")\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e47952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images and save them into categorical directories\n",
    "def process_images(images, target_size, color_mode, directory, rgb_skip):\n",
    "    if color_mode == \"rgb\" and rgb_skip:\n",
    "        output_dir = f\"../data/processed/{directory}/{color_mode}_skip/{target_size[0]}x{target_size[1]}\"\n",
    "    else:\n",
    "        output_dir = f\"../data/processed/{directory}/{color_mode}/{target_size[0]}x{target_size[1]}\"\n",
    "\n",
    "    young_dir = os.path.join(output_dir, \"young\")\n",
    "    middle_dir = os.path.join(output_dir, \"middle\")\n",
    "    old_dir = os.path.join(output_dir, \"old\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(young_dir, exist_ok=True)\n",
    "    os.makedirs(middle_dir, exist_ok=True)\n",
    "    os.makedirs(old_dir, exist_ok=True)\n",
    "\n",
    "    for _, row in images.iterrows():\n",
    "        img_name = row[\"ID\"]\n",
    "        img_new_name = row[\"new_name\"]\n",
    "        img_path = os.path.join(\"../data/raw/faces/images\", img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        # Check if the image is grayscale\n",
    "        is_grayscale = True\n",
    "        b, g, r = cv2.split(img)\n",
    "        if not (np.array_equal(r, g) and np.array_equal(g, b)):\n",
    "            is_grayscale = False\n",
    "\n",
    "        # Skip grayscale images if color_mode is \"rgb\"\n",
    "        if rgb_skip and is_grayscale and color_mode == \"rgb\":\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, target_size)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if row[\"Class\"].upper() == \"YOUNG\":\n",
    "            img.save(os.path.join(young_dir, img_new_name))\n",
    "        elif row[\"Class\"].upper() == \"MIDDLE\":\n",
    "            img.save(os.path.join(middle_dir, img_new_name))\n",
    "        elif row[\"Class\"].upper() == \"OLD\":\n",
    "            img.save(os.path.join(old_dir, img_new_name))\n",
    "\n",
    "\n",
    "def process_all_dimensions_colors(df, directory):\n",
    "    target_sizes = [(64, 64), (96, 96), (128, 128)]\n",
    "    color_modes = [\"grayscale\", \"rgb\"]\n",
    "    rgb_skip = [False, True]\n",
    "    for target_size in target_sizes:\n",
    "        for color_mode in color_modes:\n",
    "            for skip in rgb_skip:\n",
    "                print(f\"Processing images with target size {target_size} and color mode {color_mode}\")\n",
    "                process_images(df, target_size, color_mode, directory, skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808819cd",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "def get_dataset(size, color_mode, clearing, rgb_skip):\n",
    "    if rgb_skip and color_mode == \"rgb\":\n",
    "        ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            f\"../data/processed/{clearing}/{color_mode}_skip/{size}x{size}\",\n",
    "            shuffle=True,\n",
    "            image_size=(size, size),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            color_mode=color_mode,\n",
    "        )\n",
    "    else:\n",
    "        ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            f\"../data/processed/{clearing}/{color_mode}/{size}x{size}\",\n",
    "            shuffle=True,\n",
    "            image_size=(size, size),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            color_mode=color_mode,\n",
    "        )\n",
    "\n",
    "    # Split dataset\n",
    "    data_size = len(ds)\n",
    "    train_split = 0.7\n",
    "    val_split = 0.2\n",
    "    test_split = 0.1\n",
    "\n",
    "    train_size = int(train_split * data_size)\n",
    "    val_size = int(val_split * data_size)\n",
    "\n",
    "    train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "\n",
    "    # Cache, shuffle and prefetch\n",
    "    buffer_size = len(train_ds) * BATCH_SIZE\n",
    "    buffer_size\n",
    "\n",
    "    number_of_images = train_ds.cardinality().numpy() * BATCH_SIZE\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    train_ds = train_ds.cache().shuffle(number_of_images, seed=SEED).prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def get_input_shape(train_ds):\n",
    "    for batch, _ in train_ds.take(1):\n",
    "        return batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36698d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomContrast(0.1),\n",
    "        tf.keras.layers.RandomBrightness(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f7f64",
   "metadata": {},
   "source": [
    "### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a04bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_record(file):\n",
    "    history = pd.read_csv(f\"../model_history/{file}\")\n",
    "    best_val_accuracy_row = history[history[\"val_accuracy\"] == max(history[\"val_accuracy\"])].tail(1)\n",
    "    return best_val_accuracy_row\n",
    "\n",
    "\n",
    "def create_best_records_table(cvs_files):\n",
    "    # Create an empty DataFrame with file names as the index\n",
    "    best_records_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over each file and append the best record to the DataFrame\n",
    "    best_records_list = []\n",
    "    for file in cvs_files:\n",
    "        best_record = get_best_record(file)\n",
    "        if best_record[\"val_accuracy\"].item() < 0.8:\n",
    "            continue\n",
    "        best_record[\"file\"] = file  # Add a column for the file name\n",
    "        best_records_list.append(best_record)\n",
    "\n",
    "    # Concatenate all best records into a single DataFrame\n",
    "    best_records_df = pd.concat(best_records_list, ignore_index=True)\n",
    "\n",
    "    # Set the index of the DataFrame to the file names\n",
    "    best_records_df.set_index(\"file\", inplace=True)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    best_records_df = best_records_df.sort_values(by=\"val_accuracy\", ascending=False)\n",
    "    return best_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d46100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_accuracy(history):\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Model accuracy\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_loss(history):\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Model loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_evaluation(model, val_ds, history):\n",
    "    loss, acc = model.evaluate(val_ds, batch_size=BATCH_SIZE)\n",
    "    print(f\"Loss: {round(loss, 3)}, Acc: {round(acc*100, 3)}%\")\n",
    "    show_accuracy(history)\n",
    "    show_loss(history)\n",
    "\n",
    "\n",
    "def show_evaluation_df(history, name):\n",
    "    best_val_accuracy_row = history[history[\"val_accuracy\"] == max(history[\"val_accuracy\"])]\n",
    "    best_val_accuracy_epoch = best_val_accuracy_row.index[0]\n",
    "    total_epochs = len(history)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    fig.suptitle(name)\n",
    "\n",
    "    # Plot accuracy\n",
    "    axes[0].plot(history[\"accuracy\"], label=\"Training Accuracy\")\n",
    "    axes[0].plot(history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
    "    axes[0].set_title(\"Model accuracy\")\n",
    "    axes[0].set_ylabel(\"Accuracy\")\n",
    "    axes[0].set_xlabel(\"Epoch\")\n",
    "    axes[0].legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "    # Plot loss\n",
    "    axes[1].plot(history[\"loss\"], label=\"Training Loss\")\n",
    "    axes[1].plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    axes[1].set_title(\"Model loss\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_xlabel(\"Epoch\")\n",
    "    axes[1].legend([\"Train\", \"Validation\"], loc=\"upper left\")\n",
    "\n",
    "    plt.show()\n",
    "    print(f\"Best validation accuracy: {round(best_val_accuracy_row['val_accuracy'].values[0]*100, 3)}%\")\n",
    "    print(f\"Best validation accuracy epoch: {best_val_accuracy_epoch}\")\n",
    "    print(f\"Total epochs: {total_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd1087",
   "metadata": {},
   "source": [
    "### Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ed2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_balance_index(df):\n",
    "    class_counts = df[\"Number of images\"]\n",
    "    total_images = class_counts.sum()\n",
    "    ideal_count = total_images / len(class_counts)\n",
    "    balance_index = 100 - (class_counts - ideal_count).abs().sum() / (2 * total_images) * 100\n",
    "    return balance_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420be583",
   "metadata": {},
   "source": [
    "# 4.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ade08",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18549f99",
   "metadata": {},
   "source": [
    "-   EDA a data preprocessing pre Vami vybrané charakteristiky z datasetu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e30eb",
   "metadata": {},
   "source": [
    "### EDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e212f5b",
   "metadata": {},
   "source": [
    "#### EDA - simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c57241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62138c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d35342",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/raw/faces/images\"\n",
    "num_files = len([name for name in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, name))])\n",
    "print(f\"Number of images: {num_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": df_train[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": df_train[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "ax = sns.barplot(x=\"Class\", y=\"Number of images\", data=df)\n",
    "for i, v in enumerate(df[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92787a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each class\n",
    "class_counts = df[\"Number of images\"]\n",
    "class_labels = df[\"Class\"]\n",
    "\n",
    "# Plot the pie chart\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(class_counts, labels=class_labels, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Percentage of Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedddb96",
   "metadata": {},
   "source": [
    "#### Findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631579f3",
   "metadata": {},
   "source": [
    "-   There are no nulls\n",
    "-   There are no duplicates\n",
    "-   Number of images is same as number of data in cvs file\n",
    "-   There is quite big disbalance in number of images for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1916b92",
   "metadata": {},
   "source": [
    "#### EDA - image size and quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c07a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"../data/raw/faces/images\"\n",
    "\n",
    "if os.path.exists(\"../data/raw/faces/image_info.csv\"):\n",
    "    df_image_info = pd.read_csv(\"../data/raw/faces/image_info.csv\")\n",
    "else:\n",
    "    df_image_info = df_train.copy()\n",
    "\n",
    "    for image_id in df_image_info[\"ID\"]:\n",
    "        img = cv2.imread(os.path.join(image_dir, image_id))\n",
    "\n",
    "        is_grayscale = True\n",
    "        b, g, r = cv2.split(img)\n",
    "        if not (np.array_equal(r, g) and np.array_equal(g, b)):\n",
    "            is_grayscale = False\n",
    "\n",
    "        laplacian = cv2.Laplacian(img, cv2.CV_64F).var()\n",
    "\n",
    "        compressed = cv2.resize(img, (img.shape[1] // 2, img.shape[0] // 2))\n",
    "        compressed = cv2.resize(compressed, (img.shape[1], img.shape[0]))\n",
    "        ssim_score, _ = ssim(img, compressed, channel_axis=2, full=True)\n",
    "\n",
    "        original_size = img.shape[:2]\n",
    "        compressed = cv2.resize(img, (64, 64))\n",
    "        compressed = cv2.resize(compressed, (original_size[1], original_size[0]))\n",
    "        ssim_score_64, _ = ssim(img, compressed, channel_axis=2, full=True)\n",
    "\n",
    "        compressed = cv2.resize(img, (128, 128))\n",
    "        compressed = cv2.resize(compressed, (original_size[1], original_size[0]))\n",
    "        ssim_score_128, _ = ssim(img, compressed, channel_axis=2, full=True)\n",
    "\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"width\"] = img.shape[0]\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"height\"] = img.shape[1]\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"aspect_ratio\"] = img.shape[0] / img.shape[1]\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"image_size\"] = img.shape[0] * img.shape[1]\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"is_grayscale\"] = is_grayscale\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"laplacian\"] = laplacian\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"ssim_score\"] = ssim_score\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"ssim_score_64\"] = ssim_score_64\n",
    "        df_image_info.loc[df_image_info[\"ID\"] == image_id, \"ssim_score_128\"] = ssim_score_128\n",
    "\n",
    "    os.makedirs(\"../data/raw/faces\", exist_ok=True)\n",
    "    df_image_info.to_csv(\"../data/raw/faces/image_info.csv\", index=False)\n",
    "\n",
    "df_image_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d8620",
   "metadata": {},
   "source": [
    "-   Here we tried to scale images to 64 and 128 and look at image similarity\n",
    "-   It looks like 128 is in 99% cases better than 64 (even for very low size images)\n",
    "-   Our initial thought was that 64 would have better score for lower resolution images but it looks like it is not the case\n",
    "-   Therefore we are going to look only at ssim_score, which is calculated by reducing size by 50%, so it tells us how much information is lost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875925a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_image_info.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1102a",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_grayscale_images = df_image_info[\"is_grayscale\"].sum()\n",
    "number_of_color_images = len(df_image_info) - number_of_grayscale_images\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Grayscale\": [\"Yes\", \"No\"],\n",
    "        \"Number of images\": [number_of_grayscale_images, number_of_color_images],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.barplot(x=\"Grayscale\", y=\"Number of images\", data=df)\n",
    "for i, v in enumerate(df[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Number of Grayscale and Color Images\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.pie(df[\"Number of images\"], labels=df[\"Grayscale\"], autopct=\"%1.1f%%\")\n",
    "plt.title(\"Percentage of Grayscale and Color Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd611ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.histplot(df_image_info[\"width\"], ax=axes[0])\n",
    "axes[0].set_title(\"Width\")\n",
    "sns.histplot(df_image_info[\"height\"], ax=axes[1])\n",
    "axes[1].set_title(\"Height\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da952986",
   "metadata": {},
   "source": [
    "Analysis of image size (64,128) are our main focus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36458533",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images_64 = len(df_image_info[(df_image_info[\"width\"] <= 64) & (df_image_info[\"height\"] <= 64)])\n",
    "number_of_images_128 = len(df_image_info[(df_image_info[\"width\"] >= 128) & (df_image_info[\"height\"] >= 128)])\n",
    "number_of_images_between = len(\n",
    "    df_image_info[\n",
    "        (df_image_info[\"width\"] > 64)\n",
    "        & (df_image_info[\"width\"] < 128)\n",
    "        & (df_image_info[\"height\"] > 64)\n",
    "        & (df_image_info[\"height\"] < 128)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Number of images\": [number_of_images_64, number_of_images_between, number_of_images_128],\n",
    "        \"Resolution\": [\"<=64\", \"64-128\", \">=128\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.barplot(x=\"Resolution\", y=\"Number of images\", data=df)\n",
    "for i, v in enumerate(df[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Number of images by resolution - both dimensions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3784bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images_64 = len(df_image_info[(df_image_info[\"width\"] <= 64) | (df_image_info[\"height\"] <= 64)])\n",
    "number_of_images_128 = len(df_image_info[(df_image_info[\"width\"] >= 128) | (df_image_info[\"height\"] >= 128)])\n",
    "number_of_images_between = len(\n",
    "    df_image_info[\n",
    "        ((df_image_info[\"width\"] > 64) & (df_image_info[\"height\"] > 64))\n",
    "        | ((df_image_info[\"width\"] < 128) & (df_image_info[\"height\"] < 128))\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Number of images\": [number_of_images_64, number_of_images_between, number_of_images_128],\n",
    "        \"Resolution\": [\"<=64\", \"64-128\", \">=128\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.barplot(x=\"Resolution\", y=\"Number of images\", data=df)\n",
    "for i, v in enumerate(df[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Number of images by resolution - one dimension\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447793c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_64 = 64 * 64\n",
    "size_128 = 128 * 128\n",
    "\n",
    "number_of_images_64 = len(df_image_info[df_image_info[\"image_size\"] <= size_64])\n",
    "number_of_images_128 = len(df_image_info[df_image_info[\"image_size\"] >= size_128])\n",
    "number_of_images_between = len(\n",
    "    df_image_info[(df_image_info[\"image_size\"] > size_64) & (df_image_info[\"image_size\"] < size_128)]\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"Number of images\": [number_of_images_64, number_of_images_between, number_of_images_128],\n",
    "        \"Resolution\": [\"<=64x64\", \"64x64-128x128\", \">=128x128\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.barplot(x=\"Resolution\", y=\"Number of images\", data=df)\n",
    "for i, v in enumerate(df[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "\n",
    "plt.title(\"Number of images by resolution - both dimensions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "sns.boxplot(x=df_image_info[\"width\"], ax=axes[0])\n",
    "axes[0].set_title(\"Width\")\n",
    "sns.boxplot(x=df_image_info[\"height\"], ax=axes[1])\n",
    "axes[1].set_title(\"Height\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e121546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "sns.histplot(df_image_info[\"laplacian\"], ax=axes[0, 0])\n",
    "axes[0, 0].set_title(\"Laplacian\")\n",
    "sns.histplot(df_image_info[\"ssim_score\"], ax=axes[0, 1])\n",
    "axes[0, 1].set_title(\"SSIM\")\n",
    "\n",
    "sns.histplot(df_image_info[\"laplacian\"], ax=axes[1, 0])\n",
    "axes[1, 0].set_title(\"Laplacian - Low values\")\n",
    "axes[1, 0].set_xlim(0, 200)\n",
    "sns.histplot(df_image_info[\"ssim_score\"], ax=axes[1, 1])\n",
    "axes[1, 1].set_title(\"SSIM - Low values\")\n",
    "axes[1, 1].set_xlim(0.7, 0.9)\n",
    "axes[1, 1].set_ylim(0, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3dc4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"ssim_score\", y=\"image_size\", data=df_image_info)\n",
    "plt.title(\"SSIM vs Image Size\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e11509",
   "metadata": {},
   "source": [
    "#### Findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d2edb",
   "metadata": {},
   "source": [
    "BASIC\n",
    "\n",
    "-   There are a lot of different sizes of images.\n",
    "-   Images don't have sma x,y dimensions.\n",
    "-   When analyzing image size for both dimensions there are more 64x64 or lower images than 128x128 images and higher.\n",
    "-   When analyzing image size for only one dimension there are significantly more images that range from 64-128 pixels.\n",
    "-   When analyzing image size by multiplying x and y dimensions there are more images with size 64x64 than 128x128.\n",
    "-   This means it is slightly better to resize images to 64x64, this will also mean that model is faster.\n",
    "-   Most of the images that have low ssim_score are images that have low resolution.\n",
    "-   There is 5.8% of grayscale images, but they are not low resolution images.\n",
    "\n",
    "PLAN\n",
    "\n",
    "-   We are going to resize images to 64x64.\n",
    "-   Later we are also going to look at 96x96 and 128x128.\n",
    "-   We also got feedback to remove high-res images, but upon further analysis we decided to keep them as they have important features preserved when resizing.\n",
    "-   We are algo going to multiply images in other datasets to make it more balanced.\n",
    "\n",
    "REMOVAL\n",
    "\n",
    "-   At first we are gonna remove only big outliers, then we are gonna remove more, and see results if model is training better.\n",
    "-   We are going are going to remove small images, under 15x15 or even 20x20 based on how does model perform.\n",
    "-   We are going to remove images that have very low Laplacian variance, under 15 or even 20-30 based on how does model perform.\n",
    "-   Since we are removing low resolution images, we don't necesery need to remove images that have low ssim_score, but we are going to try it and see how does model perform. (0.5, later even 0.8)\n",
    "-   Aspect ration under 0.2 or above 4 (there are none under 0.2, and few above 4), later <0.3 and >3.33\n",
    "-   None / all grayslake images if model trains on rgb.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e20a0",
   "metadata": {},
   "source": [
    "### Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a9648",
   "metadata": {},
   "source": [
    "#### Low quality images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedee0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(\"../data/raw/faces/image_info.csv\")\n",
    "df_images = get_low_quality_images(df_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23072e",
   "metadata": {},
   "source": [
    "#### Mutiply images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = multiply_images(df_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940bdbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": df_train[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": df_train[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "\n",
    "df_new = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": balanced_df[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": balanced_df[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "ax = sns.barplot(x=\"Class\", y=\"Number of images\", data=df_old, ax=axes[0])\n",
    "for i, v in enumerate(df_old[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "ax.set_title(\"Original dataset\")\n",
    "\n",
    "ax = sns.barplot(x=\"Class\", y=\"Number of images\", data=df_new, ax=axes[1])\n",
    "for i, v in enumerate(df_new[\"Number of images\"]):\n",
    "    ax.text(i, v, str(v), ha=\"center\", va=\"bottom\")\n",
    "ax.set_title(\"Balanced dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a852e1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of each class\n",
    "class_counts_old = df_old[\"Number of images\"]\n",
    "class_labels_old = df_old[\"Class\"]\n",
    "class_counts_new = df_new[\"Number of images\"]\n",
    "class_labels_new = df_new[\"Class\"]\n",
    "\n",
    "# Plot the pie chart\n",
    "_, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].pie(class_counts_old, labels=class_labels_old, autopct=\"%1.1f%%\")\n",
    "axes[0].set_title(\"Original dataset\")\n",
    "axes[1].pie(class_counts_new, labels=class_labels_new, autopct=\"%1.1f%%\")\n",
    "axes[1].set_title(\"Balanced dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6493cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df.to_csv(\"../data/raw/faces/balanced_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b478acf",
   "metadata": {},
   "source": [
    "-   We see we improved the balance of images in dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7081cea",
   "metadata": {},
   "source": [
    "#### Process images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ad18e",
   "metadata": {},
   "source": [
    "This takes a lot of time. only_final_images is set to True, so it will only process images for final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0df711",
   "metadata": {},
   "outputs": [],
   "source": [
    "only_final_images = True\n",
    "\n",
    "if os.path.exists(\"../data/processed\"):\n",
    "    shutil.rmtree(\"../data/processed\")\n",
    "\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "if only_final_images:\n",
    "    df_images = pd.read_csv(\"../data/raw/faces/image_info.csv\")\n",
    "\n",
    "    df_1 = get_low_quality_images(\n",
    "        df_images,\n",
    "        min_width=15,\n",
    "        min_height=15,\n",
    "        min_aspect_ratio=0.2,\n",
    "        max_aspect_ratio=4,\n",
    "        min_laplacian=10,\n",
    "        min_ssim=0.5,\n",
    "    )\n",
    "\n",
    "    balanced_df = multiply_images(df_images)\n",
    "\n",
    "    process_images(balanced_df, (64, 64), \"rgb\", \"final_images\", True)\n",
    "else:\n",
    "    df_images = pd.read_csv(\"../data/raw/faces/image_info.csv\")\n",
    "    print(\"Getting low quality images -1st clearing\")\n",
    "    df_1 = get_low_quality_images(\n",
    "        df_images,\n",
    "        min_width=15,\n",
    "        min_height=15,\n",
    "        min_aspect_ratio=0.2,\n",
    "        max_aspect_ratio=4,\n",
    "        min_laplacian=10,\n",
    "        min_ssim=0.5,\n",
    "    )\n",
    "    print(\"Getting low quality images -2nd clearing\")\n",
    "    df_2 = get_low_quality_images(\n",
    "        df_images,\n",
    "        min_width=20,\n",
    "        min_height=20,\n",
    "        min_aspect_ratio=0.2,\n",
    "        max_aspect_ratio=4,\n",
    "        min_laplacian=10,\n",
    "        min_ssim=0.5,\n",
    "    )\n",
    "    print(\"Getting low quality images -3rd clearing\")\n",
    "    df_3 = get_low_quality_images(\n",
    "        df_images,\n",
    "        min_width=20,\n",
    "        min_height=20,\n",
    "        min_aspect_ratio=0.3,\n",
    "        max_aspect_ratio=3.33,\n",
    "        min_laplacian=25,\n",
    "        min_ssim=0.8,\n",
    "    )\n",
    "\n",
    "    balanced_df = multiply_images(df_images)\n",
    "    balanced_df_1 = multiply_images(df_1)\n",
    "    balanced_df_2 = multiply_images(df_2)\n",
    "    balanced_df_3 = multiply_images(df_3)\n",
    "\n",
    "    print(\"Processing images - no clearing\")\n",
    "    process_all_dimensions_colors(balanced_df, \"no_clearing\")\n",
    "    print(\"Processing images - 1st clearing\")\n",
    "    process_all_dimensions_colors(balanced_df_1, \"1_clearing\")\n",
    "    print(\"Processing images - 2nd clearing\")\n",
    "    process_all_dimensions_colors(balanced_df_2, \"2_clearing\")\n",
    "    print(\"Processing images - 3rd clearing\")\n",
    "    process_all_dimensions_colors(balanced_df_3, \"3_clearing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8fcc6",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbae96a",
   "metadata": {},
   "source": [
    "-   Zdôvodnite výber ML/DL metód vzhľadom na Vami vybraný dataset pre 4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2fbd6f",
   "metadata": {},
   "source": [
    "Our chosen model is CCN, using Sequential from Tensorflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ff8c5d",
   "metadata": {},
   "source": [
    "# 4.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47c9b05",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf6444",
   "metadata": {},
   "source": [
    "-   Modeluje Vami tie vybrané charakteristiky pomocou vhodných ML/DL\n",
    "    metód. Výsledok modelovania je najlepší model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c366a0ea",
   "metadata": {},
   "source": [
    "#### Main Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523cc83b",
   "metadata": {},
   "source": [
    "We were already trying different models, just what felt right. We don't have records of this testing. We were doing comparisons of model manually, adding/removing layers, changing BATCH_SIZE. We also tried keras tuner, but it takes too long to run to get some meaningful results (we did not get better model, because we had to lower number of epochs, so it ran in reasonable time).\n",
    "\n",
    "This is the best model we got in first week of testing. We are going to do another small test (looking especially at learning rate).\n",
    "\n",
    "The learning rate test showed best learning rate 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc7619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset(64, \"rgb\", \"final_images\", True)\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "input_shape = get_input_shape(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        # Preprocessing layers\n",
    "        InputLayer(shape=input_shape[1:]),\n",
    "        Rescaling(1.0 / 255),\n",
    "        # Input and first conv block\n",
    "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Second conv block\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Third conv block\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f853f8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b297374",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_evaluation(model, val_ds, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44607b49",
   "metadata": {},
   "source": [
    "#### Clearing comparision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f22d18",
   "metadata": {},
   "source": [
    "The testing was done in Testing-GPU with deterministic results. We are going to do another small test (looking especially at learning rate).\n",
    "\n",
    "Here we tested 1_clearing dataset on all color modes and resolutions.\n",
    "\n",
    "We exported models history to csv files, so we can compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f388502",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_files = os.listdir(\"../model_history\")\n",
    "best_records_df = create_best_records_table(cvs_files)\n",
    "\n",
    "for file in best_records_df.index:\n",
    "    history = pd.read_csv(f\"../model_history/{file}\")\n",
    "    show_evaluation_df(history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be00dca",
   "metadata": {},
   "source": [
    "Analysis\n",
    "\n",
    "-   As the goal of this test was to find what is the best color and resolutions we got these results:\n",
    "-   64x64 rgb with skipping grayscale images is the best. It also doesn't have overfit (due to image augmentation and dropout).\n",
    "-   96x96 rgb with skip and without are similar, close behind 64x64 rgb-skip.\n",
    "-   128x128 rgb is the worst out ot these 3.\n",
    "\n",
    "Final\n",
    "\n",
    "-   We are going to use 64x64 rgb with skipping grayscale images, for our final model if subsequent tests show it is the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97990de7",
   "metadata": {},
   "source": [
    "#### Testing increased kernel size for larger images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32dbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_files = [\n",
    "    \"history_1_clearing_rgb_128.csv\",\n",
    "    \"history_1_clearing_rgb_skip_128.csv\",\n",
    "    \"history_1_clearing_rgb_128_kernel.csv\",\n",
    "    \"history_1_clearing_rgb_skip_128_kernel.csv\",\n",
    "]\n",
    "\n",
    "best_records_df = create_best_records_table(cvs_files)\n",
    "\n",
    "\n",
    "for file in best_records_df.index:\n",
    "    history = pd.read_csv(f\"../model_history/{file}\")\n",
    "    show_evaluation_df(history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9dafc",
   "metadata": {},
   "source": [
    "-   Using increased kernel size (5,5) instead of (3,3) did not help much (only a little in rgb_skip).\n",
    "-   It is still not better than 64x64 rgb_skip.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f2638",
   "metadata": {},
   "source": [
    "#### Cleaning comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images = pd.read_csv(\"../data/raw/faces/image_info.csv\")\n",
    "print(\"Getting low quality images -1st clearing\")\n",
    "df_1 = get_low_quality_images(\n",
    "    df_images,\n",
    "    min_width=15,\n",
    "    min_height=15,\n",
    "    min_aspect_ratio=0.2,\n",
    "    max_aspect_ratio=4,\n",
    "    min_laplacian=10,\n",
    "    min_ssim=0.5,\n",
    ")\n",
    "print(\"Getting low quality images -2nd clearing\")\n",
    "df_2 = get_low_quality_images(\n",
    "    df_images,\n",
    "    min_width=20,\n",
    "    min_height=20,\n",
    "    min_aspect_ratio=0.2,\n",
    "    max_aspect_ratio=4,\n",
    "    min_laplacian=10,\n",
    "    min_ssim=0.5,\n",
    ")\n",
    "print(\"Getting low quality images -3rd clearing\")\n",
    "df_3 = get_low_quality_images(\n",
    "    df_images,\n",
    "    min_width=20,\n",
    "    min_height=20,\n",
    "    min_aspect_ratio=0.3,\n",
    "    max_aspect_ratio=3.33,\n",
    "    min_laplacian=25,\n",
    "    min_ssim=0.8,\n",
    ")\n",
    "\n",
    "balanced_df = multiply_images(df_images)\n",
    "balanced_df_1 = multiply_images(df_1)\n",
    "balanced_df_2 = multiply_images(df_2)\n",
    "balanced_df_3 = multiply_images(df_3)\n",
    "\n",
    "df_no = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": balanced_df[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": balanced_df[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "df_1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": balanced_df_1[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": balanced_df_1[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "df_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": balanced_df_2[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": balanced_df_2[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "df_3 = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": balanced_df_3[\"Class\"].value_counts().index,\n",
    "        \"Number of images\": balanced_df_3[\"Class\"].value_counts().values,\n",
    "    }\n",
    ")\n",
    "\n",
    "_, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes[0, 0].pie(df_no[\"Number of images\"], labels=df_no[\"Class\"], autopct=\"%1.1f%%\")\n",
    "axes[0, 0].set_title(\"No clearing\")\n",
    "axes[0, 1].pie(df_1[\"Number of images\"], labels=df_1[\"Class\"], autopct=\"%1.1f%%\")\n",
    "axes[0, 1].set_title(\"1st clearing\")\n",
    "axes[1, 0].pie(df_2[\"Number of images\"], labels=df_2[\"Class\"], autopct=\"%1.1f%%\")\n",
    "axes[1, 0].set_title(\"2nd clearing\")\n",
    "axes[1, 1].pie(df_3[\"Number of images\"], labels=df_3[\"Class\"], autopct=\"%1.1f%%\")\n",
    "axes[1, 1].set_title(\"3rd clearing\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def calculate_balance_index(df):\n",
    "    class_counts = df[\"Number of images\"]\n",
    "    total_images = class_counts.sum()\n",
    "    ideal_count = total_images / len(class_counts)\n",
    "    balance_index = 100 - (class_counts - ideal_count).abs().sum() / (2 * total_images) * 100\n",
    "    return balance_index\n",
    "\n",
    "\n",
    "balance_index_no = calculate_balance_index(df_no)\n",
    "balance_index_1 = calculate_balance_index(df_1)\n",
    "balance_index_2 = calculate_balance_index(df_2)\n",
    "balance_index_3 = calculate_balance_index(df_3)\n",
    "\n",
    "print(f\"Balance Index - No clearing: {balance_index_no:.2f}\")\n",
    "print(f\"Balance Index - 1st clearing: {balance_index_1:.2f}\")\n",
    "print(f\"Balance Index - 2nd clearing: {balance_index_2:.2f}\")\n",
    "print(f\"Balance Index - 3rd clearing: {balance_index_3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a6f0b",
   "metadata": {},
   "source": [
    "-   no_clearing and 1_clearing have best balance of images.\n",
    "-   2_clearing and 3_clearing are slightly behind.\n",
    "-   Lower balance can lower model prefrormance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33118043",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs_files = [\n",
    "    \"history_no_clearing_rgb_skip_64.csv\",\n",
    "    \"history_1_clearing_rgb_skip_64.csv\",\n",
    "    \"history_2_clearing_rgb_skip_64.csv\",\n",
    "    \"history_3_clearing_rgb_skip_64.csv\",\n",
    "]\n",
    "\n",
    "best_records_df = create_best_records_table(cvs_files)\n",
    "\n",
    "\n",
    "for file in best_records_df.index:\n",
    "    history = pd.read_csv(f\"../model_history/{file}\")\n",
    "    show_evaluation_df(history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4607133",
   "metadata": {},
   "source": [
    "-   1_clearing is the best, probably because it has very bad pictures cleared and also balanced dataset.\n",
    "-   2.nd was 3_clearing, with ~2.8% difference in accuracy. Maybe more balanced dataset would help.\n",
    "-   Interestingly, no_clearing has the biggest following with 2_clearing and 3_clearing, difference in train/val accuracy, but this is not overfit, this is caused by bigger dropout than needed to avoid overfit. Maybe lowering dropout would increase accuracy a little, but it will most likely not reach accuracy of 1_clearing. Therefore we will proceed with 1_clearing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cca50d",
   "metadata": {},
   "source": [
    "#### Learning rate comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061f9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [\n",
    "    0.01,\n",
    "    0.005,\n",
    "    0.002,\n",
    "    0.0015,\n",
    "    0.00075,\n",
    "    0.0005,\n",
    "    0.0001,\n",
    "]\n",
    "\n",
    "cvs_files = [\n",
    "    \"history_1_clearing_rgb_skip_64.csv\",\n",
    "]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    cvs_files.append(f\"history_1_clearing_rgb_skip_64_learn{learning_rate}.csv\")\n",
    "\n",
    "best_records_df = create_best_records_table(cvs_files)\n",
    "for file in best_records_df.index:\n",
    "    history = pd.read_csv(f\"../model_history/{file}\")\n",
    "    show_evaluation_df(history, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd659a",
   "metadata": {},
   "source": [
    "-   File without \\_learn{number}, is base adam model with learning rate 0.001.\n",
    "-   Lower learning rate is better.\n",
    "-   Decreasing learning rate to 0.0001 did increase model performance from 89.851% to 92.113%. Although model is now training longer, it is worth it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b368f37",
   "metadata": {},
   "source": [
    "### Final model evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "571c407f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34750 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset(64, \"rgb\", \"final_images\", True)\n",
    "train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y))\n",
    "input_shape = get_input_shape(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5c8e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        # Preprocessing layers\n",
    "        InputLayer(shape=input_shape[1:]),\n",
    "        Rescaling(1.0 / 255),\n",
    "        # Input and first conv block\n",
    "        Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Second conv block\n",
    "        Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Third conv block\n",
    "        Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.1),\n",
    "        # Flatten and dense layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(64, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(16, activation=\"relu\"),\n",
    "        Dropout(0.1),\n",
    "        Dense(3, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9320",
   "metadata": {},
   "source": [
    "Model training is long, we exported trained model to final_model.keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19817e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 113ms/step - accuracy: 0.4121 - loss: 1.0711 - val_accuracy: 0.5119 - val_loss: 1.0141\n",
      "Epoch 2/100\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 107ms/step - accuracy: 0.5404 - loss: 0.9665 - val_accuracy: 0.5571 - val_loss: 0.9587\n",
      "Epoch 3/100\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 113ms/step - accuracy: 0.5828 - loss: 0.9140 - val_accuracy: 0.5949 - val_loss: 0.9092\n",
      "Epoch 4/100\n",
      "\u001b[1m380/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 109ms/step - accuracy: 0.6009 - loss: 0.8817 - val_accuracy: 0.6224 - val_loss: 0.8705\n",
      "Epoch 5/100\n",
      "\u001b[1m318/380\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m6s\u001b[0m 107ms/step - accuracy: 0.6219 - loss: 0.8601"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_batch_size=BATCH_SIZE,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "model.save(\"final_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44393c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_evaluation(model, val_ds, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3362415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test_ds\n",
    "model = load_model(\"final_model.keras\")\n",
    "loss, accuracy = model.evaluate(test_ds, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {loss}\")\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fac1a1",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d8a1",
   "metadata": {},
   "source": [
    "-   Zhodnotíte Váš prístup a získaný výsledok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70d030",
   "metadata": {},
   "source": [
    "First week\n",
    "\n",
    "-   Our approach at first was just random, we loaded dataset, looked at it a little, removed some outliers and proceed to model.\n",
    "-   For models, we at first go very basic model and tried different methods to improve it. Namely changing parameters inside model, like number of layers, number of neurons, dropout, batch size, learning rate, etc.\n",
    "-   We were testing these models manually, as we count exactly see what changed in model based on graphs.\n",
    "-   We also tried to use keras tuner, but it took too long even for few parameters, and it did not give us good result as we had to lower number of epochs to get results in reasonable time.\n",
    "-   Our best model was similar to the final one.\n",
    "\n",
    "Second week\n",
    "\n",
    "-   After consultations, we got valuable feedback, especially about data preprocessing.\n",
    "-   We increased number of parameters we are eliminating outliers, this helped only little as primary parameter is image size(we we already using it).\n",
    "-   We also balanced dataset, not perfectly, but it has pretty good balance, and this helped massively with model performance.\n",
    "-   We also tested different image sizes and color modes, and we found out that 64x64 rgb with skipping grayscale images is the best.\n",
    "-   We also tested models on GPU with deterministic results, this took longer but gave us more accurate results as to which model performs better.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
