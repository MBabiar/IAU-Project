{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from graphviz import Source\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import HTML, SVG\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    learning_curve,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import FunctionTransformer, Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MaxAbsScaler,\n",
    "    MinMaxScaler,\n",
    "    Normalizer,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting cleaned data\n",
    "os.makedirs(\"../data/clean\", exist_ok=True)\n",
    "\n",
    "train_data.to_csv(\"../data/clean/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"../data/clean/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/clean/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/clean/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=10))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/processed/train_data.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/processed/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    file_path,\n",
    "    files,\n",
    "    file,\n",
    "    df,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    columns_for_zscore,\n",
    "    outliers_count,\n",
    "    max_iterations,\n",
    "    iteration,\n",
    "    all_columns,\n",
    "    non_gaussian_columns,\n",
    "    gaussian_columns,\n",
    "    transformed_feature_order,\n",
    "    general_pipe,\n",
    "    vending_pipeline,\n",
    "    preprocessor,\n",
    "    complete_pipeline,\n",
    "    train_data_processed,\n",
    "    test_data_processed,\n",
    "    dataset,\n",
    "    selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = pd.read_csv(\"../data/processed/train_data.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/processed/test_data.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]\n",
    "\n",
    "del train_data_processed, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=[\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"roc_auc\"],\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Train\", \"Test\", \"Difference\"]]),\n",
    "    )\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Train\")] = accuracy_train\n",
    "    df.loc[\"precision\", (model_name, \"Train\")] = precision_train\n",
    "    df.loc[\"recall\", (model_name, \"Train\")] = recall_train\n",
    "    df.loc[\"f1-score\", (model_name, \"Train\")] = f1_train\n",
    "    df.loc[\"roc_auc\", (model_name, \"Train\")] = roc_auc_train\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Test\")] = accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Test\")] = precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Test\")] = recall_test\n",
    "    df.loc[\"f1-score\", (model_name, \"Test\")] = f1_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Test\")] = roc_auc_test\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Difference\")] = accuracy_train - accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Difference\")] = precision_train - precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Difference\")] = recall_train - recall_test\n",
    "    df.loc[\"f1-score\", (model_name, \"Difference\")] = f1_train - f1_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Difference\")] = roc_auc_train - roc_auc_test\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_cv(model, model_name, X, y, cv):\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "\n",
    "    # Create MultiIndex DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.MultiIndex.from_product([[\"Train\", \"Test\", \"Difference\"], metrics]),\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Mean\", \"Std\"]]),\n",
    "    )\n",
    "\n",
    "    # Calculate scores for each model\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring={\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "            \"f1\": \"f1\",\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "        },\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Fill DataFrame with training and test scores\n",
    "    for metric in metrics:\n",
    "        # Training scores\n",
    "        train_key = f\"train_{metric}\"\n",
    "        df.loc[(\"Train\", metric), (model_name, \"Mean\")] = scores[train_key].mean()\n",
    "        df.loc[(\"Train\", metric), (model_name, \"Std\")] = scores[train_key].std()\n",
    "\n",
    "        # Test scores\n",
    "        test_key = f\"test_{metric}\"\n",
    "        df.loc[(\"Test\", metric), (model_name, \"Mean\")] = scores[test_key].mean()\n",
    "        df.loc[(\"Test\", metric), (model_name, \"Std\")] = scores[test_key].std()\n",
    "\n",
    "        # Difference between training and test scores\n",
    "        diff = scores[train_key] - scores[test_key]\n",
    "        df.loc[(\"Difference\", metric), (model_name, \"Mean\")] = diff.mean()\n",
    "        df.loc[(\"Difference\", metric), (model_name, \"Std\")] = diff.std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, train_sizes, cv, scoring, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = 1 - train_scores.mean(axis=1)\n",
    "    test_scores_mean = 1 - test_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-Validation Score\")\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(f\"{scoring} Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_complexity_curve(model, X_train, y_train, X_test, y_test, max_depth_range):\n",
    "    # Initialize lists to store the training and validation errors\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    # Loop over the range of max_depth values\n",
    "    for max_depth in max_depth_range:\n",
    "        # Initialize the model with the current max_depth\n",
    "        model = DecisionTreeClassifier(\n",
    "            criterion=\"gini\",\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=1,\n",
    "            ccp_alpha=0.001,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Compute the training and testing data errors\n",
    "        train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "        # Compute the mean errors\n",
    "        train_errors.append(1 - train_score)\n",
    "        val_errors.append(1 - test_score)\n",
    "\n",
    "    # Plot the learning curve for model complexity\n",
    "    plt.figure()\n",
    "    plt.plot(max_depth_range, train_errors, label=\"Training Error\")\n",
    "    plt.plot(max_depth_range, val_errors, label=\"Validation Error\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Learning Curve (Model Complexity)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_confusion_matrix(models, X_train, y_train, X_test, y_test):\n",
    "    _, axes = plt.subplots(1, len(models), figsize=(15, 5))\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[i])\n",
    "        axes[i].set_title(model_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"ccp_alpha\": 0.001,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "lgr_best_params = {\"C\": 0.01, \"max_iter\": 100, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"tol\": 0.001}\n",
    "\n",
    "gbg_best_params = {\n",
    "    \"subsample\": 0.8,\n",
    "    \"n_estimators\": 100,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"max_depth\": 4,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "hbg_best_params = {\n",
    "    \"l2_regularization\": 2.0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 7,\n",
    "    \"max_iter\": 200,\n",
    "    \"min_samples_leaf\": 20,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def ignore(line, cell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Jednoduchý klasifikátor na základe závislosti v dátach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujte jednoduchý ID3 klasifikátor s hĺbkou min 2 (vrátane root/koreň).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None, info_gain=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "        self.info_gain = info_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3Classifier:\n",
    "    def __init__(self, max_depth: int = 2):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def build_tree(self, X, y, curr_depth: int = 0):\n",
    "        _, num_features = X.shape\n",
    "\n",
    "        if curr_depth <= self.max_depth:\n",
    "            best_split = self.get_best_split(X, y, num_features)\n",
    "            if best_split[\"info_gain\"] > 0:\n",
    "                left_subtree = self.build_tree(best_split[\"X_left\"], best_split[\"y_left\"], curr_depth + 1)\n",
    "                right_subtree = self.build_tree(best_split[\"X_right\"], best_split[\"y_right\"], curr_depth + 1)\n",
    "\n",
    "                return Node(\n",
    "                    feature=best_split[\"feature\"],\n",
    "                    threshold=best_split[\"threshold\"],\n",
    "                    left=left_subtree,\n",
    "                    right=right_subtree,\n",
    "                )\n",
    "\n",
    "        leaf_value = self.calculate_leaf_value(y)\n",
    "        return Node(value=leaf_value)\n",
    "\n",
    "    def get_best_split(self, X, y, num_features):\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "\n",
    "        for feature in range(num_features):\n",
    "            feature_values = X[:, feature]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "\n",
    "            for threshold in possible_thresholds:\n",
    "                X_left, X_right, y_left, y_right = self.split(X, y, feature, threshold)\n",
    "                curr_info_gain = self.information_gain(y, y_left, y_right)\n",
    "\n",
    "                if curr_info_gain > max_info_gain:\n",
    "                    best_split[\"feature\"] = feature\n",
    "                    best_split[\"threshold\"] = threshold\n",
    "                    best_split[\"X_left\"] = X_left\n",
    "                    best_split[\"X_right\"] = X_right\n",
    "                    best_split[\"y_left\"] = y_left\n",
    "                    best_split[\"y_right\"] = y_right\n",
    "                    best_split[\"info_gain\"] = curr_info_gain\n",
    "                    max_info_gain = curr_info_gain\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    @staticmethod\n",
    "    def split(X, y, feature, threshold):\n",
    "        left_indices = np.where(X[:, feature] <= threshold)\n",
    "        right_indices = np.where(X[:, feature] > threshold)\n",
    "\n",
    "        X_left = X[left_indices]\n",
    "        X_right = X[right_indices]\n",
    "        y_left = y[left_indices]\n",
    "        y_right = y[right_indices]\n",
    "\n",
    "        return X_left, X_right, y_left, y_right\n",
    "\n",
    "    def information_gain(self, parent, l_child, r_child):\n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "\n",
    "        gain = self.entropy(parent) - (weight_l * self.entropy(l_child) + weight_r * self.entropy(r_child))\n",
    "        return gain\n",
    "\n",
    "    @staticmethod\n",
    "    def entropy(y):\n",
    "        class_labels = np.unique(y)\n",
    "        ent = 0\n",
    "\n",
    "        for cls in class_labels:\n",
    "            p = np.sum(y == cls) / len(y)\n",
    "            ent += p * np.log2(p)\n",
    "\n",
    "        return -ent\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_leaf_value(y):\n",
    "\n",
    "        Y = list(y)\n",
    "        return max(y, key=Y.count)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self.traverse_tree(x, self.tree) for x in X]\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.traverse_tree(x, node.left)\n",
    "\n",
    "        return self.traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte Váš ID3 klasifikátor pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id3_classifier = ID3Classifier(max_depth=5)\n",
    "id3_classifier.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_id3(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train.values)\n",
    "    y_pred_test = model.predict(X_test.values)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=[\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"roc_auc\"],\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Train\", \"Test\", \"Difference\"]]),\n",
    "    )\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Train\")] = accuracy_train\n",
    "    df.loc[\"precision\", (model_name, \"Train\")] = precision_train\n",
    "    df.loc[\"recall\", (model_name, \"Train\")] = recall_train\n",
    "    df.loc[\"f1-score\", (model_name, \"Train\")] = f1_train\n",
    "    df.loc[\"roc_auc\", (model_name, \"Train\")] = roc_auc_train\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Test\")] = accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Test\")] = precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Test\")] = recall_test\n",
    "    df.loc[\"f1-score\", (model_name, \"Test\")] = f1_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Test\")] = roc_auc_test\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Difference\")] = accuracy_train - accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Difference\")] = precision_train - precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Difference\")] = recall_train - recall_test\n",
    "    df.loc[\"f1-score\", (model_name, \"Difference\")] = f1_train - f1_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Difference\")] = roc_auc_train - roc_auc_test\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Using ID3 Classifier\n",
    "scores_id3 = get_scores_id3(id3_classifier, \"ID3\", X_train, y_train, X_test, y_test)\n",
    "scores_id3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zístite či Váš ID3 klasifikátor má overfit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_id3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train and test metrics are close, the model is likely not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénovanie a vyhodnotenie klasifikátorov strojového učenia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na trénovanie využite jeden stromový algoritmus v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some basic parameters to avoid overfitting\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = get_scores_cv(dst_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(dst_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Even quite simple model gives good results without overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte s jedným iným nestromovým algoritmom v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = get_scores_cv(log_reg, \"LogisticRegression\", X_train, y_train, cv=5)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Basic logistic regression gives quite good results without overfitting as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_confusion_matrix(\n",
    "    [(\"RandomForestClassifier\", dst_classifier), (\"LogisticRegression\", log_reg)], X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Basic Logistic Regression gives slightly better results than basic Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte výsledky s ID3 z prvého kroku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "dst_classifier.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "df_1 = get_scores(dst_classifier, \"DecisionTreeClassifier\", X_train, y_train, X_test, y_test)\n",
    "df_2 = get_scores(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)\n",
    "df = pd.concat([scores_id3, df_1, df_2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Our ID3 somehow outperforms DecisionTreeClassifier.\n",
    "-   But we don't have best parameters for DecisionTreeClassifier so it can be also better.\n",
    "-   Also our model doesn't have many parameters to tune.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizujte natrénované pravidlá minimálne pre jeden Vami vybraný algoritmus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForestClassifier\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dst_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Generate graph\n",
    "graph = Source(\n",
    "    export_graphviz(\n",
    "        dst_classifier, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display graph\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte natrénované modely pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We already did this but let's do it again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "df_1 = get_scores_cv(dst_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_2 = get_scores_cv(log_reg, \"LogisticRegression\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare matrixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_confusion_matrix(\n",
    "    [\n",
    "        (\"DecisionTreeClassifier\", dst_classifier),\n",
    "        (\"LogisticRegression\", log_reg),\n",
    "    ],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   As we already said, Logistic Regression gives slightly better results than Decision Tree Classifier.\n",
    "-   However, both models are quite good and don't overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimalizácia alias hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte rôzne nastavenie hyperparametrov (tuning) pre zvolený algoritmus tak,\n",
    "aby ste optimalizovali výkonnosť (bez underfitingu).\n",
    "\n",
    "We will test parameters for RandomForestClassifier as we know this will be better than DecisionTreeClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree vs Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"ccp_alpha\": [0.0, 0.0005, 0.001, 0.0015],\n",
    "    \"min_impurity_decrease\": [0.0, 0.0005, 0.001, 0.0015],\n",
    "}\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'ccp_alpha': 0.0005, 'max_depth': 10, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
    "-   Best Score: 0.9026635738931255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(\n",
    "    max_depth=10, min_samples_split=2, min_samples_leaf=4, ccp_alpha=0.0005, random_state=42\n",
    ")\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "\n",
    "df_1 = get_scores_cv(dt_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_2 = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This shows difference between optimized Decision Tree and some basic Random Forest (with basic paramters to avoid overfitting).\n",
    "-   We can clearly see that Random Forest is better and therefore we are gonna use it for further steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, find baseline parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that more estimators did help but not greatly. Also at some point the is very little improvement in adding more estimators.\n",
    "-   It also takes more time to train the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ccp_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.002)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.005)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see ccp_alpha 0.002 is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"sqrt\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"log2\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   When also using max_depth, there is difference in ROC AUC, sqrt is better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Entropy is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In previous step, we found that n_estimators=100, ccp_alpha=0.002, max_features=sqrt, criterion=entropy are better.\n",
    "-   However we are not gonna look at max_features and criterion in first tuning method as we will look at them at the end when we have best primary hyperparameters.\n",
    "-   Not using max_features and criterion in first tuning gives more priority to other hyperparameters.\n",
    "-   Our primary hyperparameters are n_estimators, max_depth, min_samples_split, min_samples_leaf, ccp_alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the broad parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"ccp_alpha\": [0.001, 0.002, 0.003],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 15, 'ccp_alpha': 0.002}\n",
    "-   Best Score: 0.91579\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Best parameters from RandomizedSearchCV\n",
    "best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"ccp_alpha\": 0.002,\n",
    "}\n",
    "\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        best_params[\"n_estimators\"] - 100,\n",
    "        best_params[\"n_estimators\"],\n",
    "        best_params[\"n_estimators\"] + 100,\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        best_params[\"max_depth\"] - 5,\n",
    "        best_params[\"max_depth\"],\n",
    "        best_params[\"max_depth\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        best_params[\"min_samples_split\"] - 5,\n",
    "        best_params[\"min_samples_split\"],\n",
    "        best_params[\"min_samples_split\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        best_params[\"min_samples_leaf\"],\n",
    "        best_params[\"min_samples_leaf\"] + 1,\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        best_params[\"ccp_alpha\"],\n",
    "        best_params[\"ccp_alpha\"] + 0.001,\n",
    "        best_params[\"ccp_alpha\"] + 0.002,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.002, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 15, 'n_estimators': 400}\n",
    "-   Best Score after Refining: 0.91606\n",
    "-   The trend for n_estimators is, bigger is better (until some point). So in next iteration we will ignore this parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My colleague found different parameters to be better, so we will look at them in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        200\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        15\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        5,10\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        1,2\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        0.001, 0.002\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "-   Best Score after Refining: 0.91596\n",
    "-   From these observations:\n",
    "    -   We can conclude that max_depth=~15 is the best, as it was 15 in every iteration.\n",
    "    -   We can conclude that ccp_alpha=~0.001 or cpp_alpha=~0.002 is the best.\n",
    "    -   We can conclude that min_samples_leaf=~1 or a little higher is the best.\n",
    "    -   We can conclude that min_samples_split=~10 is the best.\n",
    "    -   We can conclude that higher n_estimators is better (at some value it will be worse).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at cpp_alpha and max_depth in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [14, 15, 16],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.0095, 0.01, 0.0105],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   cpp_alpha=0.001 is best\n",
    "-   max_depth=15 is best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at best n_estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   n_estimators=300 is best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, we will also look at max_features and criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "-   Best Score after Refining: 0.91632\n",
    "-   We can see that max_features=sqrt and criterion=gini are best and they are also default values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=15\n",
    "-   min_samples_split=10\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n",
    "-   max_features=\"sqrt\"\n",
    "-   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note we did these test without having feature selection in pipeline as we were getting better result without it. (Will be discussed further in 3.4)\n",
    "\n",
    "Since we discovered that it is best practise to always use feature selection, we will use it it next step and look if parameters change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [10, 15],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"ccp_alpha\": [0.001, 0.0015],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "-   Best Score after Refining: 0.91591\n",
    "-   Best score without feature selection was 0.91632 vs 0.91591 with feature selection.\n",
    "-   As hinted before, using feature selection gives slightly worse results. But we will look at it in more detail in 3.4.\n",
    "-   We can also see that parameters did change and therefore it is always crucial to try different parameters when changing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters with feature selection are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=10\n",
    "-   min_samples_split=5\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n",
    "-   max_features=\"sqrt\"\n",
    "-   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Let's also look at Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [None, \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "    \"tol\": [0.0001, 0.0005, 0.001, 0.005],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'tol': 0.0005}\n",
    "-   Best Score: 0.91217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "    \"tol\": [0.0001, 0.0005, 0.001, 0.005],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
    "-   Best Score: 0.91243\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.01, max_iter=100, penalty=\"l2\", solver=\"lbfgs\", tol=0.001, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_1 = get_scores(log_reg, \"LogisticRegression - lbfgs\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.01, max_iter=100, penalty=\"l1\", solver=\"liblinear\", tol=0.001, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_2 = get_scores(log_reg, \"LogisticRegression - liblinear\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', solver=\"lbfgs\",'tol': 0.001}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte kombinácie modelov (ensemble) pre zvolený algoritmus tak, aby ste\n",
    "optimalizovali výkonnosť (bez underfitingu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already used RandomForestClassifier in previous step, but we will now explore more ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ensemble Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    **rf_best_params,\n",
    ")\n",
    "\n",
    "# Print the scores\n",
    "df_rf = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Print the scores\n",
    "df_gb = get_scores_cv(gb_classifier, \"GradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Print the scores\n",
    "df_hgb = get_scores_cv(hgb_classifier, \"HistGradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_rf, df_gb, df_hgb], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that RandomForestClassifier is the best model for our data as we used hyperparameter tuning on it.\n",
    "-   HistGradientBoostingClassifier shows promising results but overfits more than RandomForestClassifier or GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"subsample\": [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize the GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gb_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"roc_auc\", # ROC_AUC for lowering overfitting\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'subsample': 0.8, 'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 4, 'learning_rate': 0.01}\n",
    "-   Best Score: 0.91491\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_classifier = GradientBoostingClassifier(**gbg_best_params)\n",
    "\n",
    "df_gb = get_scores_cv(gb_classifier, \"GradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning for HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"min_samples_leaf\": [20, 50],\n",
    "    \"l2_regularization\": [0, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "# Base classifier\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=hgb,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\", # ROC_AUC for lowering overfitting\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Assuming X and y are your features and target\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Create optimized model with best parameters\n",
    "best_model = HistGradientBoostingClassifier(**grid_search.best_params_, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best parameters: {'l2_regularization': 0, 'learning_rate': 0.01, 'max_depth': 5, 'max_iter': 200, 'min_samples_leaf': 20}\n",
    "-   Best cross-validation score: 0.9132202050166413\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(\n",
    "    l2_regularization=0, learning_rate=0.01, max_depth=5, max_iter=200, min_samples_leaf=20, random_state=42\n",
    ")\n",
    "\n",
    "df_hgb = get_scores_cv(hgb, \"HistGradientBoostingClassifier - Refined\", X_train, y_train, cv=5)\n",
    "df_hgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_rf, df_gb, df_hgb], axis=1)\n",
    "del df_rf, df_gb, df_hgb\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that RandomForestClassifier is still the best model.\n",
    "-   Using roc_auc for scoring in GridSearchCV we lover the overfitting of HistGradientBoostingClassifier but it is still worse than RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "gb_classifier = GradientBoostingClassifier(**gbg_best_params)\n",
    "hbg_classifier = HistGradientBoostingClassifier(**hbg_best_params)\n",
    "\n",
    "df_rf = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "df_gb = get_scores_cv(gb_classifier, \"GradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_hgb = get_scores_cv(hbg_classifier, \"HistGradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_rf, df_gb, df_hgb], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First best accuracy has RandomForestClassifier then HistGradientBoostingClassifier and last GradientBoostingClassifier.\n",
    "-   Then best F1 score has RandomForestClassifier then HistGradientBoostingClassifier and last GradientBoostingClassifier.\n",
    "-   For Roc_Auc score, RandomForestClassifier is the best model, then GradientBoostingClassifier and last HistGradientBoostingClassifier.\n",
    "-   For overfitting, no model is overfitting greatly, but HistGradientBoostingClassifier still has biggest overfit out of three models.\n",
    "-   Because of this we are going to use GradientBoostingClassifier over HistGradientBoostingClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting & Stacikng Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with different algorithms\n",
    "rf = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "lgr = LogisticRegression(**lgr_best_params)\n",
    "\n",
    "gbg = GradientBoostingClassifier(**gbg_best_params)\n",
    "\n",
    "# Create voting ensemble with different algorithms\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"gbg\", gbg)], n_jobs=-1, voting=\"soft\")\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"gbg\", gbg)],\n",
    "    final_estimator=LogisticRegression(random_state=42),\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Logistic Regression\": lgr,\n",
    "    \"Gradient Boosting\": gbg,\n",
    "    \"Voting Ensemble\": voting_clf,\n",
    "    \"Stacking Ensemble\": stacking_clf,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    df_tmp = get_scores_cv(model, name, X_train, y_train, cv=5)\n",
    "    df = pd.concat([df, df_tmp], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Stacking Classifier performs better than Voting Classifier.\n",
    "-   Since Random Forest Classifier was best model until this point, it will be our primary model to compare against.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rf = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "df_voting = get_scores_cv(stacking_clf, \"Stacking Classifier\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_rf, df_voting], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Random Forest Classifier is still the best model across all metrics.\n",
    "-   The only thing Classifier excels at is lower overfitting, but it is very small difference, therefore negligible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at second VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with different algorithms\n",
    "rf = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "lgr = LogisticRegression(**lgr_best_params)\n",
    "\n",
    "gbg = GradientBoostingClassifier(**gbg_best_params)\n",
    "\n",
    "# Create voting ensemble with different algorithms\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"gbg\", gbg)], n_jobs=-1, voting=\"soft\")\n",
    "\n",
    "# Create stacking ensemble\n",
    "voting_clf_2 = VotingClassifier(estimators=[(\"rf\", rf), (\"gbg\", gbg)], n_jobs=-1, voting=\"soft\")\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Voting Ensemble\": voting_clf,\n",
    "    \"Voting Ensemble 2\": voting_clf_2,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    df_tmp = get_scores_cv(model, name, X_train, y_train, cv=5)\n",
    "    df = pd.concat([df, df_tmp], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Removing Logistic Regression from Voting Classifier gives better results slightly better results in roc_auc metric.\n",
    "-   Random Forest Classifier is still the best model across all metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Využite krížovú validáciu (cross validation) na trénovacej množine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We were already using cross validation in previous steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokážte že Váš nastavený najlepší model je bez overfitingu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with different algorithms\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train metrics and test metrics are close, the model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with different algorithms\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for train data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, rf_classifier.predict_proba(X_train)[:, 1])\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, rf_classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color=\"blue\", lw=2, label=f\"Train ROC curve (area = {roc_auc_train:.5f})\")\n",
    "plt.plot(fpr_test, tpr_test, color=\"red\", lw=2, label=f\"Test ROC curve (area = {roc_auc_test:.5f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can also see stable ROC curve for train and test data, so we can conclude that model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnotenie vplyvu zvolenej stratégie riešenia na klasifikáciu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratégie riešenia chýbajúcich hodnôt a outlierov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_raw():\n",
    "    file_path: str = \"../data/raw\"\n",
    "    files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "    dataset: dict[str, pd.DataFrame] = {}\n",
    "    for file in files:\n",
    "        dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "        dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "    df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "    df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Define columns\n",
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(data, columns):\n",
    "    Q1 = data[columns].quantile(0.25)\n",
    "    Q3 = data[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[columns] >= lower_bound).all(axis=1) & (data[columns] <= upper_bound).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers = pd.DataFrame(\n",
    "    index=[\"Number of Outliers\", \"Percentage of Outliers\"],\n",
    "    columns=[\"None-iterative IQR\", \"Iterative IQR\", \"None-iterative Z-score\", \"Iterative Z-score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "# Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Print number of outliers removed\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of rows removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Save number of outliers removed\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1iqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1iqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Define columns for IQR\n",
    "columns_for_iqr = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "\n",
    "# Remove outliers using IQR method (in all columns)\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "# Get number of outliers removed in next iteration\n",
    "outliers_count = (\n",
    "    ~(\n",
    "        (\n",
    "            train_data[columns_for_iqr]\n",
    "            >= train_data[columns_for_iqr].quantile(0.25)\n",
    "            - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "        & (\n",
    "            train_data[columns_for_iqr]\n",
    "            <= train_data[columns_for_iqr].quantile(0.75)\n",
    "            + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "    ).all(axis=1)\n",
    ").sum()\n",
    "\n",
    "# Define maximum number of iterations\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Remove outliers iteratively\n",
    "while outliers_count > 0:\n",
    "    # Remove outliers using IQR method (in all columns except p.android.vending)\n",
    "    train_data = remove_outliers_iqr(train_data, columns_for_iqr)\n",
    "\n",
    "    # Get number of outliers removed in next iteration\n",
    "    outliers_count = (\n",
    "        ~(\n",
    "            (\n",
    "                train_data[columns_for_iqr]\n",
    "                >= train_data[columns_for_iqr].quantile(0.25)\n",
    "                - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "            & (\n",
    "                train_data[columns_for_iqr]\n",
    "                <= train_data[columns_for_iqr].quantile(0.75)\n",
    "                + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "        ).all(axis=1)\n",
    "    ).sum()\n",
    "\n",
    "    # Increment iteration and stop if maximum number of iterations reached\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "# Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Print number of outliers removed\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Save number of outliers removed\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itiqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itiqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1zscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itzscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itzscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using on Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not Deleting outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "df_none_cv = get_scores_cv(rf_classifier, \"None\", X_train, y_train, cv=5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "df_none_scores = get_scores(rf_classifier, \"None\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1iqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1iqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Show the scores\n",
    "df_1iqr_cv = get_scores_cv(rf_classifier, \"1IQR\", X_train, y_train, cv=5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "df_1iqr_scores = get_scores(rf_classifier, \"1IQR\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itiqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itiqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Show the scores\n",
    "df_itiqr_cv = get_scores_cv(rf_classifier, \"1ITIQR\", X_train, y_train, cv=5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "df_itiqr_scores = get_scores(rf_classifier, \"1ITIQR\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1zscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1zscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Show the scores\n",
    "df_1zscore_cv = get_scores_cv(rf_classifier, \"1ZSCORE\", X_train, y_train, cv=5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "df_1zscore_scores = get_scores(rf_classifier, \"1ZSCORE\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itzscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itzscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "# Show the scores\n",
    "df_itzscore_cv = get_scores_cv(rf_classifier, \"1ITZSCORE\", X_train, y_train, cv=5)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "df_itzscore_scores = get_scores(rf_classifier, \"1ITZSCORE\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv = pd.concat([df_none_cv, df_1iqr_cv, df_itiqr_cv, df_1zscore_cv, df_itzscore_cv], axis=1)\n",
    "df_scores = pd.concat(\n",
    "    [df_none_scores, df_1iqr_scores, df_itiqr_scores, df_1zscore_scores, df_itzscore_scores], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best accuracy: 1ITIQR\n",
    "-   Best F1:1ITIQR\n",
    "-   Best roc_auc: 1IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Train/Test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best accuracy: 1ITZSCORE\n",
    "-   Best F1: 1ITZSCORE\n",
    "-   Best roc_auc: 1ITZSCORE\n",
    "\n",
    "-   We can see that using cross validation, it prefers lower outlier removal, this can maybe be because there is not enough data for it to reach optimal learning.\n",
    "-   Using whole train data, to train model and test on test data, it prefers higher outlier removal, this is what we expected.\n",
    "-   Using cross_validation for model comparison is best practice, but since 1ITIQR removes 24.57% of data which is quite substantial amount and 1ITZSCORE which removes only 4.77% and only targets very extreme outliers, we are going to use 1ITZSCORE for further steps as it had best performance in Train/Test split and it also has lower std in cross validation indicating that it is more stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dátová transformácia (scaling, transformer, ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our old clean data as we used iterative Z-score for cleaning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_clean():\n",
    "    file_path: str = \"../data/clean\"\n",
    "\n",
    "    # Load cleaned data (Iterative Z-score)\n",
    "    train_data = pd.read_csv(f\"{file_path}/train_data.csv\")\n",
    "    test_data = pd.read_csv(f\"{file_path}/test_data.csv\")\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "train_data, test_data = load_data_clean()\n",
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "\n",
    "gaussian_columns = [\n",
    "    \"c.dogalize\",\n",
    "    \"c.android.gm\",\n",
    "    \"c.android.youtube\",\n",
    "    \"c.android.chrome\",\n",
    "    \"c.katana\",\n",
    "    \"c.raider\",\n",
    "    \"p.android.packageinstaller\",\n",
    "    \"p.android.settings\",\n",
    "    \"p.android.documentsui\",\n",
    "    \"p.android.chrome\",\n",
    "    \"p.android.gm\",\n",
    "    \"p.system\",\n",
    "    \"p.android.externalstorage\",\n",
    "    \"p.process.gapps\",\n",
    "    \"p.google\",\n",
    "    \"p.browser.provider\",\n",
    "    \"p.android.defcontainer\",\n",
    "]\n",
    "\n",
    "log_columns = [\"c.android.vending\"]\n",
    "\n",
    "uniform_columns = [\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "\n",
    "\n",
    "transformed_feature_order = pd.Series(gaussian_columns + log_columns + uniform_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define different pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining different scalers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "min_max_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", MinMaxScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "standard_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "robust_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantile_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformers\n",
    "min_max_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"min_max\", min_max_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "standard_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard\", standard_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "robust_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"robust\", robust_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipelines\n",
    "complete_min_max_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", min_max_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "complete_standard_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", standard_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "complete_robust_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", robust_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using on Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "\n",
    "# Transform data\n",
    "train_data_processed = complete_min_max_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_min_max_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "feature_mask = complete_min_max_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "# Use on model\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "df_min_max_cv = get_scores_cv(\n",
    "    rf_classifier, \"MinMax\", train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "\n",
    "# Transform data\n",
    "train_data_processed = complete_standard_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_standard_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "feature_mask = complete_standard_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "# Use on model\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "df_standard_cv = get_scores_cv(\n",
    "    rf_classifier, \"Standard\", train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "\n",
    "# Transform data\n",
    "train_data_processed = complete_robust_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_robust_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "feature_mask = complete_robust_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "# Use on model\n",
    "df_robust_cv = get_scores_cv(\n",
    "    rf_classifier, \"Robust\", train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_min_max_cv, df_standard_cv, df_robust_cv], axis=1)\n",
    "del df_min_max_cv, df_standard_cv, df_robust_cv\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   All data is the same, so it looks like Scaler doesn't matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More unique pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new pipelines\n",
    "\n",
    "standard_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "standard_norm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"normalizer\", Normalizer()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "minmax_norm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"minmax\", MinMaxScaler()),\n",
    "        (\"normalizer\", Normalizer()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "maxabs_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"maxabs\", MaxAbsScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "max_abs_norm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"maxabs\", MaxAbsScaler()),\n",
    "        (\"normalizer\", Normalizer()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantile_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformers\n",
    "standard_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard\", standard_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "standard_norm_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard_norm\", standard_norm_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "minmax_norm_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"minmax_norm\", minmax_norm_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "maxabs_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"maxabs\", maxabs_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "max_abs_norm_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"max_abs_norm\", max_abs_norm_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipelines\n",
    "complete_standard_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", standard_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "complete_standard_norm_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", standard_norm_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "complete_min_max_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", minmax_norm_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "\n",
    "complete_maxabs_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", maxabs_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "complete_max_abs_norm_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", max_abs_norm_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "\n",
    "pipelines = {\n",
    "    \"Standard\": complete_standard_pipeline,\n",
    "    \"Standard Normalized\": complete_standard_norm_pipeline,\n",
    "    \"MinMax Normalized\": complete_min_max_pipeline,\n",
    "    \"MaxAbs Normalized\": complete_maxabs_pipeline,\n",
    "    \"MaxAbs Normalizer\": complete_max_abs_norm_pipeline,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using on Models and Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "df = pd.DataFrame()\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Transform data\n",
    "    train_data_processed = pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "    test_data_processed = pipeline.transform(test_data[all_columns])\n",
    "\n",
    "    feature_mask = pipeline.named_steps[\"selector\"].get_support()\n",
    "    selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "    train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "    test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "    train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "    test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "    # Use on model\n",
    "    df_tmp = get_scores_cv(\n",
    "        rf_classifier, name, train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    "    )\n",
    "    df = pd.concat([df, df_tmp], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   MinMax Normalized is best pipeline in all metrics except roc_auc.\n",
    "-   Standard is best pipeline in roc_auc metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Standard and MinMax Normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new pipelines\n",
    "standard_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "minmax_norm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"minmax\", MinMaxScaler()),\n",
    "        (\"normalizer\", Normalizer()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantile_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformers\n",
    "standard_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard\", standard_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "minmax_norm_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"minmax_norm\", minmax_norm_pipeline, gaussian_columns),\n",
    "        (\"other\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipelines\n",
    "complete_standard_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", standard_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")\n",
    "\n",
    "complete_min_max_pipeline = Pipeline(\n",
    "    [(\"preprocessor\", minmax_norm_preprocessor), (\"selector\", SelectKBest(f_classif, k=10))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "\n",
    "# Transform data\n",
    "train_data_processed = complete_standard_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_standard_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "feature_mask = complete_standard_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "train_data_processed.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "\n",
    "# Transform data\n",
    "train_data_processed = complete_min_max_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_min_max_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "feature_mask = complete_min_max_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "train_data_processed.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Data seems ok in both cases, except p.android.documentsui, which looks better in Standard Scaler.\n",
    "-   Our primary metric could be accuracy, but since our data is moderately imbalanced roc_auc is also important.\n",
    "-   Using Normalizer and then PowerTransformer seems quite weird and add more complexity and the is rick of over-processing.\n",
    "-   Therefore we are going to stick to our Standard Scaler pipeline from 2nd Phase, since metrics are not that different and it is simpler (It has also higher roc_auc than complete_min_max_pipeline).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výber atribútov, výber algoritmov, hyperparameter tuning, ensemble learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atribute selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipelines\n",
    "standard_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "quantile_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"standard\", standard_pipeline, gaussian_columns),\n",
    "        (\"quantile\", quantile_pipeline, log_columns + uniform_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipelines\n",
    "complete_pipeline_nofs = Pipeline([(\"preprocessor\", preprocessor)])\n",
    "complete_pipeline_fs_5 = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=5))])\n",
    "complete_pipeline_fs_7 = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=7))])\n",
    "complete_pipeline_fs_10 = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=10))])\n",
    "complete_pipeline_fs_15 = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=15))])\n",
    "\n",
    "pipelines = {\n",
    "    \"5 Features\": complete_pipeline_fs_5,\n",
    "    \"7 Features\": complete_pipeline_fs_7,\n",
    "    \"10 Features\": complete_pipeline_fs_10,\n",
    "    \"15 Features\": complete_pipeline_fs_15,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_clean()\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# No feature selection\n",
    "train_data_processed = complete_pipeline_nofs.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline_nofs.transform(test_data[all_columns])\n",
    "\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "df_tmp = get_scores_cv(\n",
    "    rf_classifier, \"None\", train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    ")\n",
    "df = pd.concat([df, df_tmp], axis=1)\n",
    "\n",
    "# Feature selection\n",
    "for name, pipeline in pipelines.items():\n",
    "    # Transform data\n",
    "    train_data_processed = pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "    test_data_processed = pipeline.transform(test_data[all_columns])\n",
    "\n",
    "    feature_mask = pipeline.named_steps[\"selector\"].get_support()\n",
    "    selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "    train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "    test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "    train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "    test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "    # Use on model\n",
    "    rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "    df_tmp = get_scores_cv(\n",
    "        rf_classifier, name, train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"], cv=5\n",
    "    )\n",
    "    df = pd.concat([df, df_tmp], axis=1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   As mentioned in previous steps, we were initially not using feature selection (because it gave best results) and then used it as we discoverd it to be best practice.\n",
    "-   One reasoning for this is that Random Forest uses it's own feature selection.\n",
    "-   One interesting thing is that in Phase 2 we showed impotences and there were around ~10 features with some usefulness, but Random Forest seems to be maybe using these non-important features in some way.\n",
    "-   Since None feature selection gave better results in every category such as: train/test metrics, but also in stability (std) and also in overfitting.\n",
    "-   Therefore we are going to use None feature selection for further steps, although it is weird not to use feature selection maybe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Pick\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Base algorithm is decision tree.\n",
    "-   We are going to use Random Forest as we compared it to number of different algorithms and it was always the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We already did hyperparameter tuning in previous steps for Random Forest with no feature selection.\n",
    "-   Results were:\n",
    "    -   n_estimators=300\n",
    "    -   max_depth=15\n",
    "    -   min_samples_split=10\n",
    "    -   min_samples_leaf=1\n",
    "    -   ccp_alpha=0.001\n",
    "    -   max_features=\"sqrt\"\n",
    "    -   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramters = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [10, 15],  # 10 was with feature selection\n",
    "    \"min_samples_split\": [5, 10],  # 5 was with feature selection\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "    \"random_state\": [42],\n",
    "}\n",
    "\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(rf_classifier, param_grid=paramters, cv=5, n_jobs=-1, verbose=2)\n",
    "rf_grid.fit(train_data_processed.drop(columns=[\"mwra\"]), train_data_processed[\"mwra\"])\n",
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This is just a confirmation that we are going to use Random Forest with these parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Sole Random Forest performed better than ensemble models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ktorý model je Váš najlepší model pre nasadenie (deployment)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"ccp_alpha\": 0.001,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aký je data pipeline pre jeho vybudovanie na základe Vášho datasetu v produkcii?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clean_data():\n",
    "    file_path: str = \"../data/raw\"\n",
    "    files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "    dataset: dict[str, pd.DataFrame] = {}\n",
    "    for file in files:\n",
    "        dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "        dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "    df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "    df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    # 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "    train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "    # Using all columns except c.android.vending for outlier detection\n",
    "    columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    max_iterations = 10\n",
    "    iteration = 0\n",
    "\n",
    "    # Iterating after we removed all outliers\n",
    "    while outliers_count > 0:\n",
    "        train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "        outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "        iteration += 1\n",
    "        if iteration >= max_iterations:\n",
    "            break\n",
    "\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "    os.makedirs(\"../data/clean\", exist_ok=True)\n",
    "    train_data.to_csv(\"../data/clean/train_data.csv\", index=False)\n",
    "    test_data.to_csv(\"../data/clean/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_data():\n",
    "    # Load data\n",
    "    train_data = pd.read_csv(\"../data/clean/train_data.csv\")\n",
    "    test_data = pd.read_csv(\"../data/clean/test_data.csv\")\n",
    "\n",
    "    # Define columns\n",
    "    all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "    gaussian_columns = [\n",
    "        \"c.dogalize\",\n",
    "        \"c.android.gm\",\n",
    "        \"c.android.youtube\",\n",
    "        \"c.android.chrome\",\n",
    "        \"c.katana\",\n",
    "        \"c.raider\",\n",
    "        \"p.android.packageinstaller\",\n",
    "        \"p.android.settings\",\n",
    "        \"p.android.documentsui\",\n",
    "        \"p.android.chrome\",\n",
    "        \"p.android.gm\",\n",
    "        \"p.system\",\n",
    "        \"p.android.externalstorage\",\n",
    "        \"p.process.gapps\",\n",
    "        \"p.google\",\n",
    "        \"p.browser.provider\",\n",
    "        \"p.android.defcontainer\",\n",
    "    ]\n",
    "    log_columns = [\"c.android.vending\"]\n",
    "    uniform_columns = [\n",
    "        \"c.UCMobile.x86\",\n",
    "        \"c.updateassist\",\n",
    "        \"c.UCMobile.intl\",\n",
    "        \"p.android.vending\",\n",
    "        \"p.dogalize\",\n",
    "        \"p.olauncher\",\n",
    "        \"p.simulator\",\n",
    "        \"p.inputmethod.latin\",\n",
    "        \"p.android.gms\",\n",
    "        \"p.notifier\",\n",
    "        \"p.katana\",\n",
    "        \"p.gms.persistent\",\n",
    "    ]\n",
    "    transformed_feature_order = pd.Series(gaussian_columns + log_columns + uniform_columns)\n",
    "\n",
    "    # Define pipeline\n",
    "    standard_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "        ]\n",
    "    )\n",
    "    quantile_pipeline = Pipeline(\n",
    "        [\n",
    "            (\"scaler\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "        ]\n",
    "    )\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"standard\", standard_pipeline, gaussian_columns),\n",
    "            (\"quantile\", quantile_pipeline, log_columns + uniform_columns),\n",
    "        ],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    complete_pipeline = Pipeline([(\"preprocessor\", preprocessor)])\n",
    "\n",
    "    # Transform data\n",
    "    train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "    test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "    train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "    test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "    train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "    test_data_processed[\"mwra\"] = test_data[\"mwra\"]\n",
    "\n",
    "    os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "    train_data_processed.to_csv(\"../data/processed/train_data.csv\", index=False)\n",
    "    train_data_processed.to_csv(\"../data/processed/test_data.csv\", index=False)\n",
    "\n",
    "    X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "    y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "    X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "    y_test = test_data_processed[\"mwra\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_clean_data()\n",
    "X_train, y_train, X_test, y_test = get_processed_data()\n",
    "\n",
    "rf_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"ccp_alpha\": 0.001,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "rf_classifier = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "df = get_scores(rf_classifier, \"Processed\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This is our final pipeline and model for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
