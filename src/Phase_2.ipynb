{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import (\n",
    "    anderson,\n",
    "    iqr,\n",
    "    jarque_bera,\n",
    "    kstest,\n",
    "    kurtosis,\n",
    "    probplot,\n",
    "    shapiro,\n",
    "    skew,\n",
    "    zscore,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import (\n",
    "    RFE,\n",
    "    SelectFromModel,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    chi2,\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.svm import SVR, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging \"connections\" and \"processes\" on \"ts\" \"imei\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = dataset[\"devices\"][\"imei\"]\n",
    "print(f\"Number of duplicated imei: {int(devices.duplicated().sum())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = dataset[\"profiles\"][\"imei\"]\n",
    "print(f\"Number of duplicated imei: {int(profiles.duplicated().sum())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   If we want to merge tables \"devices\" and \"profiles\", then we have problem.\n",
    "-   These tables contains duplicated data for imei's, and since these tables don't have column \"ts\", we can't join them effectively.\n",
    "-   If we join them we would perform some cross joins in data and we don't want that.\n",
    "-   Therefore we are only going to join \"connections\" and \"processes\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zadanie:** Dáta si rozdeľte na trénovaciu a testovaciu množinu podľa vami preddefinovaného pomeru. Ďalej pracujte len s trénovacím datasetom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zadanie:** Transformujte dáta na vhodný formát pre ML t.j. jedno pozorovanie musí byť opísané jedným riadkom a každý atribút musí byť v numerickom formáte (encoding). Iteratívne integrujte aj kroky v predspracovaní dát z prvej fázy (missing values, outlier detection) ako celok.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ts = train_data.duplicated(subset=\"ts\").any()\n",
    "print(f\"Are there any duplicate timestamps? {duplicate_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are no duplicates in datetime, therefore every observation is in one row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   All columns are in numerical format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = train_data.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are no missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph showing the boxplot and outliers of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 3:].plot(\n",
    "    kind=\"box\", vert=False, figsize=(20, 20), flierprops=dict(marker=\"o\", color=\"r\", alpha=0.5)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Z-score (3) to detect outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before = train_data.shape[0]\n",
    "print(f\"Number of rows before removing outliers: {number_of_rows_before}\")\n",
    "\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "number_of_rows_after = train_data.shape[0]\n",
    "print(f\"Number of rows after removing outliers: {number_of_rows_after}\")\n",
    "print(f\"Number of rows removed: {number_of_rows_before - number_of_rows_after}\")\n",
    "print(\n",
    "    f\"Percentage of rows removed: {(number_of_rows_before - number_of_rows_after) / number_of_rows_before * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Removing 4% of the is ok, therefore we are not going to use methods to replace outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 3:].plot(\n",
    "    kind=\"box\", vert=False, figsize=(20, 20), flierprops=dict(marker=\"o\", color=\"r\", alpha=0.5)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More in-depth analysis of **\"p.android.vending\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"p.android.vending\"].plot(kind=\"hist\", bins=100)\n",
    "plt.show()\n",
    "train_data[\"p.android.vending\"].plot(kind=\"box\", vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that is that logarithmic scale, that's why it has so many outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(a):\n",
    "    lower = a.quantile(0.25) - 1.5 * iqr(a)\n",
    "    upper = a.quantile(0.75) + 1.5 * iqr(a)\n",
    "\n",
    "    return a[(a > upper) | (a < lower)]\n",
    "\n",
    "\n",
    "train_data_copy = train_data.copy()\n",
    "outliers = train_data_copy[[\"p.android.vending\"]].apply(identify_outliers)\n",
    "train_data_copy = train_data_copy.drop(outliers.index)\n",
    "print(f\"Number of outliers: {train_data_copy[[\"p.android.vending\"]].apply(identify_outliers).count().values[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers: {train_data_copy[[\"p.android.vending\"]].apply(identify_outliers).count().values[0] / train_data_copy.shape[0] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"p.android.vending\"].plot(kind=\"hist\", bins=100)\n",
    "plt.show()\n",
    "train_data[\"p.android.vending\"].plot(kind=\"box\", vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In column **'p.android.vending'** there were too many outliers\n",
    "-   We tried to remove them using 25% and 75% quantiles, but there was almost no noticeable difference\n",
    "-   Using this we would remove 1183 (12.23%) rows which is quite substantial amount, therefore we decided not ot manipulate with outliers in this column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zadanie:** Transformujte atribúty dát pre strojové učenie podľa dostupných techník minimálne: scaling (2 techniky), transformers (2 techniky) a ďalšie. Cieľom je aby ste testovali efekty a vhodne kombinovali v dátovom pipeline (od časti 2.3 a v 3. fáze).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables (only scaling X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"])\n",
    "y = train_data[\"mwra\"]\n",
    "all_columns = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Showing the distribution of the data before scaling/transforming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "normalized_data = X.copy()\n",
    "normalized_data[all_columns] = scaler.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "normalized_data = X.copy()\n",
    "normalized_data[all_columns] = scaler.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing plot without **\"p.android.vending\"** as it has too big range (can't see other graph that well)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data.drop(columns=[\"p.android.vending\"]).plot(\n",
    "    kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PowerTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "normalized_data = X.copy()\n",
    "normalized_data[all_columns] = power_transformer.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using QuantileTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "\n",
    "normalized_data = X.copy()\n",
    "normalized_data[all_columns] = quantile_transformer.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MinMaxScaler then PowerTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler and transformer\n",
    "scaler = MinMaxScaler()\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "normalized_data = X.copy()\n",
    "\n",
    "# Scale data using MinMaxScaler\n",
    "normalized_data[all_columns] = scaler.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Transform data using PowerTransformer\n",
    "normalized_data[all_columns] = power_transformer.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Plot histograms\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StandardScaler then PowerTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler and transformer\n",
    "scaler = StandardScaler()\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "normalized_data = X.copy()\n",
    "\n",
    "# Scale data using StandardScaler\n",
    "normalized_data[all_columns] = scaler.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Transform data using PowerTransformer\n",
    "normalized_data[all_columns] = power_transformer.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Plot histograms\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PowerTransformer then StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define scaler and transformer\n",
    "scaler = StandardScaler()\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "\n",
    "normalized_data = X.copy()\n",
    "\n",
    "# Transform data using PowerTransformer\n",
    "normalized_data[all_columns] = power_transformer.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Scale data using StandardScaler\n",
    "normalized_data[all_columns] = scaler.fit_transform(normalized_data[all_columns])\n",
    "\n",
    "# Plot histograms\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First using StandardScaler then PowerTransformer on all columns similar to gaussian distribution\n",
    "-   Then using QuantileTransformer on all other columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "all_columns = X.columns\n",
    "vending_column = [\"p.android.vending\"]\n",
    "uniform_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "non_gaussian_columns = vending_column + uniform_columns\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "\n",
    "# Define scaler and transformers\n",
    "scaler = StandardScaler()\n",
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "\n",
    "# Normalize data (general columns)\n",
    "normalized_data = X.copy()\n",
    "normalized_data[gaussian_columns] = scaler.fit_transform(normalized_data[gaussian_columns])\n",
    "normalized_data[gaussian_columns] = power_transformer.fit_transform(normalized_data[gaussian_columns])\n",
    "\n",
    "# Normalize data (vending column)\n",
    "normalized_data[non_gaussian_columns] = quantile_transformer.fit_transform(normalized_data[non_gaussian_columns])\n",
    "\n",
    "# Plot histograms\n",
    "normalized_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = normalized_data.columns.tolist()\n",
    "\n",
    "# Original correlations\n",
    "combined_df_orig = pd.concat([X, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_orig = combined_df_orig.corr(method=\"pearson\")\n",
    "filtered_corr_orig = corr_orig.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Transformed correlations\n",
    "combined_df_trans = pd.concat([normalized_data, train_data[\"mwra\"]], axis=1).dropna()\n",
    "corr_trans = combined_df_trans.corr(method=\"pearson\")\n",
    "filtered_corr_trans = corr_trans.loc[feature_columns, [\"mwra\"]]\n",
    "\n",
    "# Create side-by-side plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Plot original correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_orig,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax1,\n",
    ")\n",
    "ax1.set_title(\"Original Correlations\")\n",
    "\n",
    "# Plot transformed correlations\n",
    "sns.heatmap(\n",
    "    filtered_corr_trans,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=[\"mwra\"],\n",
    "    yticklabels=feature_columns,\n",
    "    ax=ax2,\n",
    ")\n",
    "ax2.set_title(\"Transformed Correlations\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use this transformation in the next steps for feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = normalized_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zadanie:** Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections we have gathered these insights:\n",
    "\n",
    "-   Scaling:\n",
    "    -   We identified need for scaling data, because data has different ranges\n",
    "    -   Min-Max Scaling showed promising results, when not combining with anything else\n",
    "    -   Standard Scaler showed also promising results, the only problem was column **'p.android.vending'** as it has too big range and many outliers (we used Z-score(3), maybe using quantile detection could improved this), the scaled graph also showed outliers\n",
    "-   Transformers:\n",
    "    -   Power Transformer didn't show that promising results for uniform graphs (it made the a little logarithmic), but promising results for normal graphs\n",
    "    -   Quantile Transformer transforms data to perfect normal distribution, but it could distort linear correlations\n",
    "-   Combinations:\n",
    "    -   Using Min-Max Scaler and Power Transformer showed promising results, as it transform data to normal distributions and also kept uniform distributions (didn't make them logarithmic, but there is some very slight logarithmic effect)\n",
    "    -   Using Standard Scaler and Power Transformer showed even more promising results compared to Using Min-Max Scaler and Power Transformer, as it transform data to normal distributions and also kept uniform distributions without any logarithmic effect\n",
    "-   Final choice:\n",
    "\n",
    "    -   First using StandardScaler then PowerTransformer on all columns similar to gaussian distribution\n",
    "    -   Then using QuantileTransformer on all other columns\n",
    "\n",
    "-   Correlations remains similar using all transformations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n",
    "### **Zadanie:** Zistite, ktoré atribúty (features) vo vašich dátach pre ML sú informatívne k predikovanej premennej (minimálne 3 techniky s porovnaním medzi sebou).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))\n",
    "selector.fit(X)\n",
    "support_mask = selector.get_support()\n",
    "\n",
    "removed_columns = X.columns[~support_mask].tolist()\n",
    "\n",
    "print(\"Removed columns:\", removed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold_data = X\n",
    "\n",
    "thresholds = np.arange(0.0, 1, 0.05)\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    tmp = transform.fit_transform(variance_threshold_data)\n",
    "    n_features = tmp.shape[1]\n",
    "    print(\">Threshold=%.2f, Features=%d\" % (t, n_features))\n",
    "    results.append(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see different threshold values have no effect on the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = mutual_info_classif(X, y)\n",
    "importance = pd.Series(selector, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = importance.plot(kind=\"barh\")\n",
    "\n",
    "for i, v in enumerate(importance):\n",
    "    ax.text(v + 0.001, i, f\"{v:.2f}\", va=\"center\")\n",
    "\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"Mutual Information Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = mutual_info_classif(X, y)\n",
    "importance = pd.Series(selector, index=X.columns).sort_values(ascending=False)\n",
    "importance = importance[importance > 0.05]\n",
    "\n",
    "importance = pd.DataFrame(importance)\n",
    "importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We got these features as best:\n",
    "    -   **p.android.settings**, **c.katana**, **p.system**, **c.android.youtube**, **p.android.packageinstaller**, **p.android.documentsui**, **p.android.externalstorage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Our target variable **mwra** could be specified as categorical (True, False), so we are gonna try this method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting K-best (k=7) features using F-statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=7)\n",
    "_ = selector.fit_transform(X, y)\n",
    "mask = selector.get_support()\n",
    "\n",
    "selected_columns = X.columns[mask]\n",
    "importance = selector.scores_[mask]\n",
    "\n",
    "selected_features = pd.DataFrame({\"Feature\": selected_columns, \"Importance\": importance})\n",
    "\n",
    "sorted_features = selected_features.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We got these features as best:\n",
    "    -   **p.android.settings**, **c.katana**, **p.system**, **c.android.youtube**, **p.android.chrome**, **p.android.externalstorage**, **p.android.packageinstaller**\n",
    "-   We see different features and it's importance compared to previous tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(estimator=LogisticRegression(max_iter=1000, random_state=42))\n",
    "_ = selector.fit_transform(X, y)\n",
    "\n",
    "mask = selector.get_support()\n",
    "selected_columns = X.columns[mask]\n",
    "importance = np.abs(selector.estimator_.coef_[0][mask])\n",
    "\n",
    "selected_features = pd.DataFrame({\"Feature\": selected_columns, \"Importance\": importance})\n",
    "sorted_features = selected_features.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectFromModel(estimator=LinearSVC(random_state=42))\n",
    "_ = selector.fit_transform(X, y)\n",
    "\n",
    "mask = selector.get_support()\n",
    "selected_columns = X.columns[mask]\n",
    "importance = np.abs(selector.estimator_.coef_[0][mask])\n",
    "\n",
    "selected_features = pd.DataFrame({\"Feature\": selected_columns, \"Importance\": importance})\n",
    "selected_features = selected_features.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelectFromModel(estimator=RandomForestClassifier(random_state=42))\n",
    "_ = model.fit_transform(X, y)\n",
    "\n",
    "selected_columns = X.columns[model.get_support()]\n",
    "importance = model.estimator_.feature_importances_\n",
    "\n",
    "selected_features = pd.DataFrame({\"Feature\": selected_columns, \"Importance\": importance[model.get_support()]})\n",
    "selected_features = selected_features.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "\n",
    "### **Zadanie:** Zoraďte zistené atribúty v poradí podľa dôležitosti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Sorted in previous step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "\n",
    "### **Zadanie:** Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that different methods also selects different features with different importance\n",
    "-   We are going to try to use SelectKBest, with f_classif, but we will try different models and see how they perform later when we have ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting raw data\n",
    "\n",
    "Removing duplicated and outliers.\n",
    "Data is split into train and test datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Removing outliers using iterative Z-score(3) method. (until there are no outliers, max iteration is 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and merging dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearing outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "number_of_rows_after_outliers = train_data.shape[0]\n",
    "print(f\"Number of removed outliers: {number_of_rows_before_outliers - number_of_rows_after_outliers}\")\n",
    "print(\n",
    "    f\"Percentage of removed outliers: {(number_of_rows_before_outliers - number_of_rows_after_outliers) / number_of_rows_before_outliers * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We are not removing outliers from **\"p.android.vending\"**, as it has logarithmic scale and therefore many outliers\n",
    "-   We removed 570 (4.77%) rows using iterative Z-score(3) method. So using this method is ok as it doesn't remove too many rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting to cleaned data .cvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"../data/clean\", exist_ok=True)\n",
    "\n",
    "train_data.to_csv(\"../data/clean/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"../data/clean/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n",
    "### **Zadanie:** Upravte váš kód realizujúci predspracovanie trénovacej množiny tak, aby ho bolo možné bez ďalších úprav znovu použiť na predspracovanie testovacej množiny v kontexte strojového učenia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/clean/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/clean/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature column lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=3))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "X_train_processed = complete_pipeline.fit_transform(train_data[feature_columns], train_data[\"mwra\"])\n",
    "X_test_processed = complete_pipeline.transform(test_data[feature_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=selected_features)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting processed data to .cvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "X_train_processed.to_csv(\"../data/processed/train_data.csv\", index=False)\n",
    "X_test_processed.to_csv(\"../data/processed/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "\n",
    "### **Zadanie:** Využite možnosti sklearn.pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In previous steps we have already used pipelines\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
