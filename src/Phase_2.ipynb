{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import anderson, iqr, skew, zscore\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    f_classif,\n",
    "    f_regression,\n",
    "    mutual_info_classif,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    StandardScaler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../dataset\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"], how=\"inner\")\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n",
    "### **Zadanie:** Dáta si rozdeľte na trénovaciu a testovaciu množinu podľa vami preddefinovaného pomeru. Ďalej pracujte len s trénovacím datasetom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "\n",
    "### **Zadanie:** Transformujte dáta na vhodný formát pre ML t.j. jedno pozorovanie musí byť opísané jedným riadkom a každý atribút musí byť v numerickom formáte (encoding). Iteratívne integrujte aj kroky v predspracovaní dát z prvej fázy (missing values, outlier detection) ako celok.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_ts = train_data.duplicated(subset=\"ts\").any()\n",
    "print(f\"Are there any duplicate timestamps? {duplicate_ts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are no duplicates in datetime, therefore every observation is in one row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   All columns are in numerical format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = train_data.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are no missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph showing the boxplot and outliers of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 3:].plot(\n",
    "    kind=\"box\", vert=False, figsize=(20, 20), flierprops=dict(marker=\"o\", color=\"r\", alpha=0.5)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_distribution = True\n",
    "\n",
    "for col in train_data.columns[2:]:\n",
    "    result = anderson(train_data[col], dist=\"norm\")\n",
    "\n",
    "    if not result.fit_result.success:\n",
    "        norm_distribution = False\n",
    "        print(f\"{col} is not normally distributed\")\n",
    "\n",
    "if norm_distribution:\n",
    "    print(\"All columns are normally distributed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use Z-score (3), to detect outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before = train_data.shape[0]\n",
    "print(f\"Number of rows before removing outliers: {number_of_rows_before}\")\n",
    "\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "number_of_rows_after = train_data.shape[0]\n",
    "print(f\"Number of rows after removing outliers: {number_of_rows_after}\")\n",
    "print(f\"Number of rows removed: {number_of_rows_before - number_of_rows_after}\")\n",
    "print(\n",
    "    f\"Percentage of rows removed: {(number_of_rows_before - number_of_rows_after) / number_of_rows_before * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Removing 4% of the is ok, therefore we are not going to use methods to replace outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 3:].plot(\n",
    "    kind=\"box\", vert=False, figsize=(20, 20), flierprops=dict(marker=\"o\", color=\"r\", alpha=0.5)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"p.android.vending\"].plot(kind=\"hist\", bins=100)\n",
    "plt.show()\n",
    "train_data[\"p.android.vending\"].plot(kind=\"box\", vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers(a):\n",
    "    lower = a.quantile(0.25) - 1.5 * iqr(a)\n",
    "    upper = a.quantile(0.75) + 1.5 * iqr(a)\n",
    "\n",
    "    return a[(a > upper) | (a < lower)]\n",
    "\n",
    "\n",
    "train_data_copy = train_data.copy()\n",
    "outliers = train_data_copy[[\"p.android.vending\"]].apply(identify_outliers)\n",
    "train_data_copy = train_data_copy.drop(outliers.index)\n",
    "print(f\"Number of outliers: {train_data_copy[[\"p.android.vending\"]].apply(identify_outliers).count().values[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers: {train_data_copy[[\"p.android.vending\"]].apply(identify_outliers).count().values[0] / train_data_copy.shape[0] * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"p.android.vending\"].plot(kind=\"hist\", bins=100)\n",
    "plt.show()\n",
    "train_data[\"p.android.vending\"].plot(kind=\"box\", vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In column **'p.android.vending'** there were too many outliers\n",
    "-   We tried to remove them using 25% and 75% quantiles, but there was almost no noticeable difference\n",
    "-   Using this we would remove 1183 (12.23%) rows which is quite substantial amount, therefore we decided not ot manipulate with outliers in this column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "\n",
    "### **Zadanie:** Transformujte atribúty dát pre strojové učenie podľa dostupných techník minimálne: scaling (2 techniky), transformers (2 techniky) a ďalšie. Cieľom je aby ste testovali efekty a vhodne kombinovali v dátovom pipeline (od časti 2.3 a v 3. fáze).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness_type(skew_value: float) -> str:\n",
    "    if skew_value <= -1:\n",
    "        return \"Highly Negative Skew\"\n",
    "\n",
    "    elif skew_value <= -0.5:\n",
    "        return \"Moderately Negative Skew\"\n",
    "\n",
    "    elif skew_value <= 0.5:\n",
    "        return \"Approximately Symmetric\"\n",
    "\n",
    "    elif skew_value <= 1:\n",
    "        return \"Moderately Positive Skew\"\n",
    "\n",
    "    return \"Highly Positive Skew\"\n",
    "\n",
    "\n",
    "def kurtosis_type(kurtosis_value: float) -> str:\n",
    "    if kurtosis_value < -1:\n",
    "        return \"Negative Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < -0.5:\n",
    "        return \"Moderately Negative Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < 0.5:\n",
    "        return \"Approximately Normal Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < 1:\n",
    "        return \"Moderately Positive Kurtosis\"\n",
    "\n",
    "    return \"Positive Kurtosis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = train_data.columns[3:].tolist()\n",
    "skew_values = [skew(train_data[col]) for col in columns]\n",
    "kurtosis_values = [train_data[col].kurtosis() for col in columns]\n",
    "\n",
    "data = {\n",
    "    \"columns\": columns,\n",
    "    \"skew\": skew_values,\n",
    "    \"kurtosis\": kurtosis_values,\n",
    "}\n",
    "\n",
    "shape_train_data = pd.DataFrame(data)\n",
    "\n",
    "shape_train_data[\"result skew\"] = shape_train_data[\"skew\"].apply(skewness_type)\n",
    "shape_train_data[\"result kurtosis\"] = shape_train_data[\"kurtosis\"].apply(kurtosis_type)\n",
    "shape_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.iloc[:, 3:].plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data_minmax = scaler.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "scaled_data_minmax = pd.DataFrame(scaled_data_minmax, columns=train_data.columns[3:])\n",
    "scaled_data_minmax.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data_standard = scaler.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "scaled_data_standard = pd.DataFrame(scaled_data_standard, columns=train_data.columns[3:])\n",
    "\n",
    "scaled_data_standard.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data_standard.drop(columns=[\"p.android.vending\"]).plot(\n",
    "    kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "transformed_data_power = power_transformer.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "transformed_data_power = pd.DataFrame(transformed_data_power, columns=train_data.columns[3:])\n",
    "transformed_data_power.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "transformed_data = quantile_transformer.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "transformed_data = pd.DataFrame(transformed_data, columns=train_data.columns[3:])\n",
    "transformed_data.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "transformed_data_power = power_transformer.fit_transform(scaled_data_minmax)\n",
    "\n",
    "transformed_data_power = pd.DataFrame(transformed_data_power, columns=train_data.columns[3:])\n",
    "transformed_data_power.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "transformed_data_power = power_transformer.fit_transform(scaled_data_standard)\n",
    "\n",
    "transformed_data_power = pd.DataFrame(transformed_data_power, columns=train_data.columns[3:])\n",
    "transformed_data_power.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data_minmax = scaler.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "scaled_data_minmax = pd.DataFrame(scaled_data_minmax, columns=train_data.columns[3:])\n",
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "scaled_data_minmax[\"p.android.vending\"] = quantile_transformer.fit_transform(\n",
    "    scaled_data_minmax[[\"p.android.vending\"]]\n",
    ")\n",
    "scaled_data_minmax = pd.DataFrame(scaled_data_minmax, columns=train_data.columns[3:])\n",
    "\n",
    "scaled_data_minmax.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.xlim(-4, 4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_data_minmax = scaler.fit_transform(train_data.iloc[:, 3:])\n",
    "\n",
    "scaled_data_minmax = pd.DataFrame(scaled_data_minmax, columns=train_data.columns[3:])\n",
    "quantile_transformer = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "scaled_data_minmax[\"p.android.vending\"] = quantile_transformer.fit_transform(\n",
    "    scaled_data_minmax[[\"p.android.vending\"]]\n",
    ")\n",
    "scaled_data_minmax = pd.DataFrame(scaled_data_minmax, columns=train_data.columns[3:])\n",
    "\n",
    "scaled_data_minmax.plot(kind=\"hist\", bins=50, figsize=(20, 20), subplots=True, layout=(10, 3))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n",
    "\n",
    "### **Zadanie:** Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous sections we have gathered these insights:\n",
    "\n",
    "-   Scaling:\n",
    "    -   We identified need for scaling data, because data has different ranges\n",
    "    -   Min-Max Scaling showed promising results, when not combining with anything else\n",
    "    -   Standard Scaler showed also promising results, the only problem was column **'p.android.vending'** as it has too big range and many outliers (we used Z-score(3), maybe using quantile detection could improved this), the scaled graph also showed outliers\n",
    "-   Transformers:\n",
    "    -   Power Transformer didn't show that promising results for uniform graphs (it made the a little logarithmic), but promising results for normal graphs\n",
    "    -   Quantile Transformer transforms data to perfect normal distribution, but it could distort linear correlations\n",
    "-   Combinations:\n",
    "    -   Using Min-Max Scaler and Power Transformer showed promising results, as it transform data to normal distributions and also kept uniform distributions (didn't make them logarithmic, but there is some very slight logarithmic effect)\n",
    "    -   Using Standard Scaler and Power Transformer showed even more promising results compared to Using Min-Max Scaler and Power Transformer, as it transform data to normal distributions and also kept uniform distributions without any logarithmic effect\n",
    "-   Final choice:\n",
    "    -   We are going to use Standard Scaler and Power Transformer, and after this we apply Quantile Transformer for **'p.android.vending'**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n",
    "### **Zadanie:** Zistite, ktoré atribúty (features) vo vašich dátach pre ML sú informatívne k predikovanej premennej (minimálne 3 techniky s porovnaním medzi sebou).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = VarianceThreshold(threshold=(0.8 * (1 - 0.8)))\n",
    "selector.fit(train_data.iloc[:, 3:])\n",
    "support_mask = selector.get_support()\n",
    "\n",
    "removed_columns = train_data.iloc[:, 3:].columns[~support_mask]\n",
    "\n",
    "print(\"Removed columns:\", removed_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see thins method would remove **'p.android.vending'** feature, which is the most problematic feature that we have in our dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance_threshold_data = train_data.iloc[:, 3:]\n",
    "\n",
    "thresholds = np.arange(0.0, 0.55, 0.05)\n",
    "results = list()\n",
    "for t in thresholds:\n",
    "    transform = VarianceThreshold(threshold=t)\n",
    "    tmp = transform.fit_transform(variance_threshold_data)\n",
    "    n_features = tmp.shape[1]\n",
    "    print(\">Threshold=%.2f, Features=%d\" % (t, n_features))\n",
    "    results.append(n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see different threshold values have no effect on the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"])\n",
    "y = train_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mic(X, y):\n",
    "    selector = mutual_info_classif(X, y)\n",
    "    scores = pd.Series(selector, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(16, 16))\n",
    "    ax = scores.plot(kind=\"barh\")\n",
    "\n",
    "    for i, v in enumerate(scores):\n",
    "        ax.text(v + 0.001, i, f\"{v:.2f}\", va=\"center\")\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mic_threshold(X, y):\n",
    "    selector = mutual_info_classif(X, y)\n",
    "    scores = pd.Series(selector, index=X.columns).sort_values(ascending=False)\n",
    "    scores = scores[scores > 0.05]\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    ax = scores.plot(kind=\"barh\")\n",
    "\n",
    "    for i, v in enumerate(scores):\n",
    "        ax.text(v + 0.001, i, f\"{v:.2f}\", va=\"center\")\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic(X, y)\n",
    "mic_threshold(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We got these features as best:\n",
    "    -   **p.android.settings**, **c.katana**, **p.system**, **c.android.youtube**, **p.android.packageinstaller**, **p.android.documentsui**, **p.android.externalstorage**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Squared\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We don't have categorical data, therefore we won't use this method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Statistic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Our target variable **mwra** could be specified as categorical (True, False), so we are gonna try this method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting K-best (k=7) features using F-statistic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=f_regression, k=7)\n",
    "x_tmp = selector.fit_transform(X, y)\n",
    "mask = selector.get_support()\n",
    "\n",
    "selected_columns = X.columns[mask]\n",
    "scores = selector.scores_[mask]\n",
    "\n",
    "selected_features = pd.DataFrame({\"Feature\": selected_columns, \"Score\": scores})\n",
    "\n",
    "sorted_features = selected_features.sort_values(by=\"Score\", ascending=False).reset_index(drop=True)\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We got these features as best:\n",
    "    -   **p.android.settings**, **c.katana**, **p.system**, **c.android.youtube**, **p.android.chrome**, **p.android.externalstorage**, **p.android.packageinstaller**\n",
    "-   Compared to MI, we got some different features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using F-statistic to select features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = f_classif(X, y)\n",
    "x_tmp = pd.Series(selector[0], index=X.columns).sort_values(ascending=False)\n",
    "x_tmp\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_tmp.plot(kind=\"barh\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = f_classif(X, y)\n",
    "x_tmp = pd.Series(selector[0], index=X.columns).sort_values(ascending=False)\n",
    "x_tmp = x_tmp[x_tmp > 900]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_tmp.plot(kind=\"barh\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(x_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We got these features as best:\n",
    "    -   **p.android.settings**, **c.katana**, **p.system**, **c.android.youtube**, **p.android.chrome**, **p.android.externalstorage**, **p.android.packageinstaller**, **c.dogalize**, **p.android.documentsui**\n",
    "-   Now we see a little bit more difference between MI and F-statistic, as we got 2 extra features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "\n",
    "### **Zadanie:** Zoraďte zistené atribúty v poradí podľa dôležitosti.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n",
    "\n",
    "### **Zadanie:** Zdôvodnite Vaše voľby/rozhodnutie pre realizáciu (t.j. zdokumentovanie)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n",
    "\n",
    "### **Zadanie:** Upravte váš kód realizujúci predspracovanie trénovacej množiny tak, aby ho bolo možné bez ďalších úprav znovu použiť na predspracovanie testovacej množiny v kontexte strojového učenia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n",
    "\n",
    "### **Zadanie:** Využite možnosti sklearn.pipeline\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
