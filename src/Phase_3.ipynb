{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from graphviz import Source\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import HTML, SVG\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    ")\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    learning_curve,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import FunctionTransformer, Pipeline\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler,\n",
    "    PowerTransformer,\n",
    "    QuantileTransformer,\n",
    "    RobustScaler,\n",
    "    StandardScaler,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting cleaned data\n",
    "os.makedirs(\"../data/clean\", exist_ok=True)\n",
    "\n",
    "train_data.to_csv(\"../data/clean/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"../data/clean/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/clean/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/clean/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"selector\", SelectKBest(f_classif, k=10))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=selected_features)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=selected_features)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/processed/train_data.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/processed/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    file_path,\n",
    "    files,\n",
    "    file,\n",
    "    df,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    columns_for_zscore,\n",
    "    outliers_count,\n",
    "    max_iterations,\n",
    "    iteration,\n",
    "    all_columns,\n",
    "    non_gaussian_columns,\n",
    "    gaussian_columns,\n",
    "    transformed_feature_order,\n",
    "    general_pipe,\n",
    "    vending_pipeline,\n",
    "    preprocessor,\n",
    "    complete_pipeline,\n",
    "    train_data_processed,\n",
    "    test_data_processed,\n",
    "    dataset,\n",
    "    selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = pd.read_csv(\"../data/processed/train_data.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/processed/test_data.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]\n",
    "\n",
    "del train_data_processed, test_data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=[\"accuracy\", \"precision\", \"recall\", \"f1-score\", \"roc_auc\"],\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Train\", \"Test\"]]),\n",
    "    )\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    f1_train = f1_score(y_train, y_pred_train)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Train\")] = accuracy_train\n",
    "    df.loc[\"precision\", (model_name, \"Train\")] = precision_train\n",
    "    df.loc[\"recall\", (model_name, \"Train\")] = recall_train\n",
    "    df.loc[\"f1-score\", (model_name, \"Train\")] = f1_train\n",
    "    df.loc[\"roc_auc\", (model_name, \"Train\")] = roc_auc_train\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Test\")] = accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Test\")] = precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Test\")] = recall_test\n",
    "    df.loc[\"f1-score\", (model_name, \"Test\")] = f1_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Test\")] = roc_auc_test\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_cv(model, model_name, X, y, cv):\n",
    "    metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]\n",
    "\n",
    "    # Create MultiIndex DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        index=pd.MultiIndex.from_product([[\"Train\", \"Test\"], metrics]),\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Mean\", \"Std\"]]),\n",
    "    )\n",
    "\n",
    "    # Calculate scores for each model\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring={\n",
    "            \"accuracy\": \"accuracy\",\n",
    "            \"precision\": \"precision\",\n",
    "            \"recall\": \"recall\",\n",
    "            \"f1\": \"f1\",\n",
    "            \"roc_auc\": \"roc_auc\",\n",
    "        },\n",
    "        return_train_score=True,\n",
    "    )\n",
    "\n",
    "    # Fill DataFrame with training and test scores\n",
    "    for metric in metrics:\n",
    "        # Training scores\n",
    "        train_key = f\"train_{metric}\"\n",
    "        df.loc[(\"Train\", metric), (model_name, \"Mean\")] = scores[train_key].mean()\n",
    "        df.loc[(\"Train\", metric), (model_name, \"Std\")] = scores[train_key].std()\n",
    "\n",
    "        # Test scores\n",
    "        test_key = f\"test_{metric}\"\n",
    "        df.loc[(\"Test\", metric), (model_name, \"Mean\")] = scores[test_key].mean()\n",
    "        df.loc[(\"Test\", metric), (model_name, \"Std\")] = scores[test_key].std()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, X, y, train_sizes, cv, scoring, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = 1 - train_scores.mean(axis=1)\n",
    "    test_scores_mean = 1 - test_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-Validation Score\")\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(f\"{scoring} Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_complexity_curve(model, X_train, y_train, X_test, y_test, max_depth_range):\n",
    "    # Initialize lists to store the training and validation errors\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    # Loop over the range of max_depth values\n",
    "    for max_depth in max_depth_range:\n",
    "        # Initialize the model with the current max_depth\n",
    "        model = DecisionTreeClassifier(\n",
    "            criterion=\"gini\",\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=1,\n",
    "            ccp_alpha=0.001,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Compute the training and testing data errors\n",
    "        train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "        # Compute the mean errors\n",
    "        train_errors.append(1 - train_score)\n",
    "        val_errors.append(1 - test_score)\n",
    "\n",
    "    # Plot the learning curve for model complexity\n",
    "    plt.figure()\n",
    "    plt.plot(max_depth_range, train_errors, label=\"Training Error\")\n",
    "    plt.plot(max_depth_range, val_errors, label=\"Validation Error\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Learning Curve (Model Complexity)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_confusion_matrix(models, X_train, y_train, X_test, y_test):\n",
    "    _, axes = plt.subplots(1, len(models), figsize=(15, 5))\n",
    "\n",
    "    for i, (model_name, model) in enumerate(models):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, ax=axes[i])\n",
    "        axes[i].set_title(model_name)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(model_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def ignore(line, cell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Jednoduchý klasifikátor na základe závislosti v dátach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujte jednoduchý ID3 klasifikátor s hĺbkou min 2 (vrátane root/koreň).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, attribute=None, value=None, is_leaf=False, prediction=None):\n",
    "        self.attribute = attribute\n",
    "        self.value = value\n",
    "        self.is_leaf = is_leaf\n",
    "        self.prediction = prediction\n",
    "        self.children = {}\n",
    "\n",
    "\n",
    "class ID3Classifier:\n",
    "    def __init__(self, min_depth: int = 2, max_depth: int = None) -> None:\n",
    "        self.max_depth = max_depth\n",
    "        self.min_depth = min_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        data = X.copy()\n",
    "        data[\"label\"] = y\n",
    "        self.tree = self._build_tree(data, current_depth=0)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        proportions = y.value_counts(normalize=True)\n",
    "        return -np.sum(proportions * np.log2(proportions))\n",
    "\n",
    "    def _information_gain(self, data, attribute, target=\"label\"):\n",
    "        total_entropy = self._entropy(data[target])\n",
    "        values = data[attribute].unique()\n",
    "        weighted_entropy = 0\n",
    "        for value in values:\n",
    "            subset = data[data[attribute] == value]\n",
    "            weighted_entropy += (len(subset) / len(data)) * self._entropy(subset[target])\n",
    "        return total_entropy - weighted_entropy\n",
    "\n",
    "    def _best_split(self, data, attributes, target=\"label\"):\n",
    "        best_gain = -1\n",
    "        best_attribute = None\n",
    "        for attribute in attributes:\n",
    "            gain = self._information_gain(data, attribute, target)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_attribute = attribute\n",
    "        return best_attribute\n",
    "\n",
    "    def _build_tree(self, data, current_depth):\n",
    "        X = data.drop(columns=[\"label\"])\n",
    "        y = data[\"label\"]\n",
    "\n",
    "        if len(y.unique()) == 1 or (self.max_depth is not None and current_depth >= self.max_depth):\n",
    "            return Node(is_leaf=True, prediction=y.iloc[0])\n",
    "\n",
    "        if current_depth >= self.min_depth or X.empty:\n",
    "            return Node(is_leaf=True, prediction=y.mode().iloc[0])\n",
    "\n",
    "        best_attribute = self._best_split(data, X.columns)\n",
    "        root = Node(attribute=best_attribute)\n",
    "\n",
    "        for value in data[best_attribute].unique():\n",
    "            subset = data[data[best_attribute] == value]\n",
    "            root.children[value] = self._build_tree(subset, current_depth + 1)\n",
    "\n",
    "        return root\n",
    "\n",
    "    def predict(self, X):\n",
    "        return X.apply(self._predict_row, axis=1, args=(self.tree,))\n",
    "\n",
    "    def _predict_row(self, row, node):\n",
    "        if node.is_leaf:\n",
    "            return node.prediction\n",
    "\n",
    "        attribute_value = row[node.attribute]\n",
    "        if attribute_value in node.children:\n",
    "            return self._predict_row(row, node.children[attribute_value])\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ID3 classifier\n",
    "id3_classifier = ID3Classifier(min_depth=2, max_depth=5)\n",
    "id3_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(\n",
    "    export_graphviz(\n",
    "        id3_classifier, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte Váš ID3 klasifikátor pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = get_scores(dtc_nofs, \"DecisionTreeClassifier - No feature selection\", X_train, y_train, X_test, y_test)\n",
    "df_2 = get_scores(\n",
    "    dtc_fs, \"DecisionTreeClassifier - Feature selection\", X_train_selected, y_train, X_test_selected, y_test\n",
    ")\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that using feature selection gives worse results. We will look at it in more detail later (3.4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zístite či Váš ID3 klasifikátor má overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train and test metrics are close, the model is likely not overfitting.\n",
    "-   But let's also look at learning curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve - train sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Define the training sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 50)\n",
    "\n",
    "# Plot learning curve for accuracy\n",
    "plot_learning_curve(\n",
    "    dtc_nofs,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    title=\"Learning Curve - Train size (Accuracy)\",\n",
    ")\n",
    "\n",
    "# Plot learning curve for precision\n",
    "plot_learning_curve(\n",
    "    dtc_nofs,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"precision\",\n",
    "    title=\"Learning Curve - Train size (Precision)\",\n",
    ")\n",
    "\n",
    "# Plot learning curve for recall\n",
    "plot_learning_curve(\n",
    "    dtc_nofs, X_train, y_train, train_sizes, cv=5, scoring=\"recall\", title=\"Learning Curve - Train size (Recall)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We don't see a big gap in metrics between train and test data (Looking at last point since that is what we used in previous steps).\n",
    "-   However we see that model starts of badly (as expected) and then improves with more data. Around 50% of data for training seems to be enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve - model complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of max_depth values\n",
    "max_depth_range = range(1, 30)\n",
    "\n",
    "# Plot model complexity curve\n",
    "plot_model_complexity_curve(dtc_nofs, X_train, y_train, X_test, y_test, max_depth_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This also shows no overfitting for our graph as there is no significant gap (looking at max_depth=15, as this is what we used in previous steps).\n",
    "-   We can also see that model start of very badly and is underfitting.\n",
    "-   Around max_depth=5 training and testing error stabilizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "dtc_nofs.fit(X_train, y_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for train data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, dtc_nofs.predict_proba(X_train)[:, 1])\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, dtc_nofs.predict_proba(X_test)[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color=\"blue\", lw=2, label=f\"Train ROC curve (area = {roc_auc_train:.5f})\")\n",
    "plt.plot(fpr_test, tpr_test, color=\"red\", lw=2, label=f\"Test ROC curve (area = {roc_auc_test:.5f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   ROC also shows no significant gap between train and test data. So we can conclude that model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    dtc_fs,\n",
    "    dtc_nofs,\n",
    "    fpr_train,\n",
    "    tpr_train,\n",
    "    fpr_test,\n",
    "    tpr_test,\n",
    "    train_sizes,\n",
    "    max_depth_range,\n",
    "    roc_auc_test,\n",
    "    roc_auc_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénovanie a vyhodnotenie klasifikátorov strojového učenia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na trénovanie využite jeden stromový algoritmus v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use some basic parameters to avoid overfitting\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "dst_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = get_scores_cv(dst_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(dst_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Even quite simple model gives good results without overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte s jedným iným nestromovým algoritmom v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = get_scores_cv(log_reg, \"LogisticRegression\", X_train, y_train, cv=5)\n",
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Basic logistic regression gives quite good results without overfitting as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_confusion_matrix(\n",
    "    [(\"RandomForestClassifier\", dst_classifier), (\"LogisticRegression\", log_reg)], X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Basic Logistic Regression gives slightly better results than basic Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte výsledky s ID3 z prvého kroku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizujte natrénované pravidlá minimálne pre jeden Vami vybraný algoritmus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForestClassifier\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dst_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Generate graph\n",
    "graph = Source(\n",
    "    export_graphviz(\n",
    "        dst_classifier, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display graph\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte natrénované modely pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We already did this but let's do it again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "dst_classifier = DecisionTreeClassifier(max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "log_reg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the scores\n",
    "df_1 = get_scores_cv(dst_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_2 = get_scores_cv(log_reg, \"LogisticRegression\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare matrixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_confusion_matrix(\n",
    "    [\n",
    "        (\"DecisionTreeClassifier\", dst_classifier),\n",
    "        (\"LogisticRegression\", log_reg),\n",
    "    ],\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   As we already said, Logistic Regression gives slightly better results than Decision Tree Classifier.\n",
    "-   However, both models are quite good and don't overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dst_classifier, log_reg, graph, style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimalizácia alias hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte rôzne nastavenie hyperparametrov (tuning) pre zvolený algoritmus tak,\n",
    "aby ste optimalizovali výkonnosť (bez underfitingu).\n",
    "\n",
    "We will test parameters for RandomForestClassifier as we know this will be better than DecisionTreeClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree vs Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"ccp_alpha\": [0.0, 0.0005, 0.001, 0.0015],\n",
    "    \"min_impurity_decrease\": [0.0, 0.0005, 0.001, 0.0015],\n",
    "}\n",
    "\n",
    "# Initialize the DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_classifier, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'ccp_alpha': 0.0005, 'max_depth': 10, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
    "-   Best Score: 0.9026635738931255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(\n",
    "    max_depth=10, min_samples_split=2, min_samples_leaf=4, ccp_alpha=0.0005, random_state=42\n",
    ")\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=15, ccp_alpha=0.001, random_state=42)\n",
    "\n",
    "df_1 = get_scores_cv(dt_classifier, \"DecisionTreeClassifier\", X_train, y_train, cv=5)\n",
    "df_2 = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "del df_1, df_2\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This shows difference between optimized Decision Tree and some basic Random Forest (with basic paramters to avoid overfitting).\n",
    "-   We can clearly see that Random Forest is better and therefore we are gonna use it for further steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, find baseline parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that performance is not increasing with n_estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ccp_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.002)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.005)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see ccp_alpha 0.002 is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"sqrt\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"log2\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   When also using max_depth, there is difference in ROC AUC, sqrt is better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Entropy is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In previous step, we found that n_estimators=100, ccp_alpha=0.002, max_features=sqrt, criterion=entropy are better.\n",
    "-   However we are not gonna look at max_features and criterion in first tuning method as we will look at them at the end when we have best primary hyperparameters.\n",
    "-   Not using max_features and criterion in first tuning gives more priority to other hyperparameters.\n",
    "-   Our primary hyperparameters are n_estimators, max_depth, min_samples_split, min_samples_leaf, ccp_alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the broad parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"ccp_alpha\": [0.001, 0.002, 0.003],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 15, 'ccp_alpha': 0.002}\n",
    "-   Best Score: 0.91579\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Best parameters from RandomizedSearchCV\n",
    "best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"ccp_alpha\": 0.002,\n",
    "}\n",
    "\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        best_params[\"n_estimators\"] - 100,\n",
    "        best_params[\"n_estimators\"],\n",
    "        best_params[\"n_estimators\"] + 100,\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        best_params[\"max_depth\"] - 5,\n",
    "        best_params[\"max_depth\"],\n",
    "        best_params[\"max_depth\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        best_params[\"min_samples_split\"] - 5,\n",
    "        best_params[\"min_samples_split\"],\n",
    "        best_params[\"min_samples_split\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        best_params[\"min_samples_leaf\"],\n",
    "        best_params[\"min_samples_leaf\"] + 1,\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        best_params[\"ccp_alpha\"],\n",
    "        best_params[\"ccp_alpha\"] + 0.001,\n",
    "        best_params[\"ccp_alpha\"] + 0.002,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.002, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 15, 'n_estimators': 400}\n",
    "-   Best Score after Refining: 0.91606\n",
    "-   The trend for n_estimators is, bigger is better (until some point). So in next iteration we will ignore this parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My colleague found different parameters to be better, so we will look at them in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        200\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        15\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        5,10\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        1,2\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        0.001, 0.002\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "-   Best Score after Refining: 0.91596\n",
    "-   From these observations:\n",
    "    -   We can conclude that max_depth=~15 is the best, as it was 15 in every iteration.\n",
    "    -   We can conclude that ccp_alpha=~0.001 or cpp_alpha=~0.002 is the best.\n",
    "    -   We can conclude that min_samples_leaf=~1 or a little higher is the best.\n",
    "    -   We can conclude that min_samples_split=~10 is the best.\n",
    "    -   We can conclude that higher n_estimators is better (at some value it will be worse).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at cpp_alpha and max_depth in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [14, 15, 16],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.0095, 0.01, 0.0105],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   cpp_alpha=0.001 is best\n",
    "-   max_depth=15 is best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at best n_estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   n_estimators=300 is best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, we will also look at max_features and criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "-   Best Score after Refining: 0.91632\n",
    "-   We can see that max_features=sqrt and criterion=gini are best and they are also default values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=15\n",
    "-   min_samples_split=10\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n",
    "-   max_features=\"sqrt\"\n",
    "-   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To note we did these test without having feature selection in pipeline as we were getting better result without it. (Will be discussed further in 3.4)\n",
    "\n",
    "Since we discovered that it is best practise to always use feature selection, we will use it it next step and look if parameters change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [10, 15],\n",
    "    \"min_samples_split\": [5, 10],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "    \"ccp_alpha\": [0.001, 0.0015],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 300}\n",
    "-   Best Score after Refining: 0.91591\n",
    "-   Best score without feature selection was 0.91632 vs 0.91591 with feature selection.\n",
    "-   As hinted before, using feature selection gives slightly worse results. But we will look at it in more detail in 3.4.\n",
    "-   We can also see that parameters did change and therefore it is always crucial to try different parameters when changing pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters with feature selection are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=10\n",
    "-   min_samples_split=5\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n",
    "-   max_features=\"sqrt\"\n",
    "-   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV - Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Let's also look at Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [None, \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "    \"tol\": [0.0001, 0.0005, 0.001, 0.005],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'tol': 0.0005}\n",
    "-   Best Score: 0.91217\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "    \"tol\": [0.0001, 0.0005, 0.001, 0.005],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', 'tol': 0.001}\n",
    "-   Best Score: 0.91243\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.01, max_iter=100, penalty=\"l2\", solver=\"lbfgs\", tol=0.001, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_1 = get_scores(log_reg, \"LogisticRegression - lbfgs\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.01, max_iter=100, penalty=\"l1\", solver=\"liblinear\", tol=0.001, random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_2 = get_scores(log_reg, \"LogisticRegression - liblinear\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'C': 0.01, 'max_iter': 100, 'penalty': 'l2', solver=\"lbfgs\",'tol': 0.001}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte kombinácie modelov (ensemble) pre zvolený algoritmus tak, aby ste\n",
    "optimalizovali výkonnosť (bez underfitingu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already used RandomForestClassifier in previous step, but we will now explore more ensemble methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 10,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"ccp_alpha\": 0.001,\n",
    "    \"random_state\": 42,\n",
    "}\n",
    "\n",
    "lgr_best_params = {\"C\": 0.01, \"max_iter\": 100, \"penalty\": \"l2\", \"solver\": \"lbfgs\", \"tol\": 0.001}\n",
    "\n",
    "hbg_best_params = {\n",
    "    \"l2_regularization\": 2.0,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 7,\n",
    "    \"max_iter\": 200,\n",
    "    \"min_samples_leaf\": 20,\n",
    "    \"random_state\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Ensemble Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    **rf_best_params,\n",
    ")\n",
    "\n",
    "# Print the scores\n",
    "df_rf = get_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)\n",
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Print the scores\n",
    "df_gb = get_scores_cv(gb_classifier, \"GradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb_classifier = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Print the scores\n",
    "df_hgb = get_scores_cv(hgb_classifier, \"HistGradientBoostingClassifier\", X_train, y_train, cv=5)\n",
    "df_hgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_rf, df_gb, df_hgb], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that RandomForestClassifier is the best model for our data as we used hyperparameter tuning on it.\n",
    "-   HistGradientBoostingClassifier shows promising results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying to tune HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.01, 0.1, 0.3],\n",
    "    \"max_depth\": [3, 5, 7],\n",
    "    \"max_iter\": [100, 200],\n",
    "    \"min_samples_leaf\": [20, 50],\n",
    "    \"l2_regularization\": [0, 1.0, 2.0],\n",
    "}\n",
    "\n",
    "# Base classifier\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Grid search with 5-fold CV\n",
    "grid_search = GridSearchCV(estimator=hgb, param_grid=param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1)\n",
    "\n",
    "# Assuming X and y are your features and target\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Create optimized model with best parameters\n",
    "best_model = HistGradientBoostingClassifier(**grid_search.best_params_, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgb = HistGradientBoostingClassifier(\n",
    "    l2_regularization=2.0, learning_rate=0.1, max_depth=7, max_iter=200, min_samples_leaf=20, random_state=42\n",
    ")\n",
    "\n",
    "df_hgb = get_scores_cv(hgb, \"HistGradientBoostingClassifier - Refined\", X_train, y_train, cv=5)\n",
    "df = pd.concat([df_rf, df_hgb], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that RandomForestClassifier is still the best model for our data as we used hyperparameter tuning on it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    HistGradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define base models with different algorithms\n",
    "rf = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "lgr = LogisticRegression(**lgr_best_params)\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(**hbg_best_params)\n",
    "\n",
    "# Create voting ensemble with different algorithms\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"hgb\", hgb)], n_jobs=-1, voting=\"soft\")\n",
    "\n",
    "# Create stacking ensemble\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"hgb\", hgb)], final_estimator=LogisticRegression(random_state=42), cv=5\n",
    ")\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    \"Random Forest\": rf,\n",
    "    \"Logistic Regression\": lgr,\n",
    "    \"HistGradient Boosting\": hgb,\n",
    "    \"Voting Ensemble\": voting_clf,\n",
    "    \"Stacking Ensemble\": stacking_clf,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nEvaluating {name}\")\n",
    "    df_tmp = get_scores_cv(model, name, X_train, y_train, cv=5)\n",
    "    df = pd.concat([df, df_tmp], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Main finding is that Voting Classifier/Stacking Ensemble is better than sole RandomForestClassifier.\n",
    "-   There are only slight differences between Voting Classifier and Stacking Ensemble.\n",
    "-   Only concern using Voting Ensemble is difference between train/test score for roc_auc, but since other metrics are close there might not be overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Využite krížovú validáciu (cross validation) na trénovacej množine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We were already using cross validation in previous steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokážte že Váš nastavený najlepší model je bez overfitingu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base models with different algorithms\n",
    "rf = RandomForestClassifier(**rf_best_params)\n",
    "\n",
    "lgr = LogisticRegression(**lgr_best_params)\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(**hbg_best_params)\n",
    "\n",
    "# Create voting ensemble with different algorithms\n",
    "voting_clf = VotingClassifier(estimators=[(\"rf\", rf), (\"lgr\", lgr), (\"hgb\", hgb)], n_jobs=-1, voting=\"soft\")\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "df = get_scores(voting_clf, \"VotingClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train metrics and test metrics are close, the model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnotenie vplyvu zvolenej stratégie riešenia na klasifikáciu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratégie riešenia chýbajúcich hodnôt a outlierov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_raw():\n",
    "    file_path: str = \"../data/raw\"\n",
    "    files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "    dataset: dict[str, pd.DataFrame] = {}\n",
    "    for file in files:\n",
    "        dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "        dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "    df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "    df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    train_data = train_data.reset_index(drop=True)\n",
    "    test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Define columns\n",
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(data, columns):\n",
    "    Q1 = data[columns].quantile(0.25)\n",
    "    Q3 = data[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[columns] >= lower_bound).all(axis=1) & (data[columns] <= upper_bound).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers = pd.DataFrame(\n",
    "    index=[\"Number of Outliers\", \"Percentage of Outliers\"],\n",
    "    columns=[\"None-iterative IQR\", \"Iterative IQR\", \"None-iterative Z-score\", \"Iterative Z-score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "# Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Print number of outliers removed\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of rows removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Save number of outliers removed\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# # Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1iqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1iqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Define columns for IQR\n",
    "columns_for_iqr = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "\n",
    "# Remove outliers using IQR method (in all columns)\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "# Get number of outliers removed in next iteration\n",
    "outliers_count = (\n",
    "    ~(\n",
    "        (\n",
    "            train_data[columns_for_iqr]\n",
    "            >= train_data[columns_for_iqr].quantile(0.25)\n",
    "            - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "        & (\n",
    "            train_data[columns_for_iqr]\n",
    "            <= train_data[columns_for_iqr].quantile(0.75)\n",
    "            + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "    ).all(axis=1)\n",
    ").sum()\n",
    "\n",
    "# Define maximum number of iterations\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Remove outliers iteratively\n",
    "while outliers_count > 0:\n",
    "    # Remove outliers using IQR method (in all columns except p.android.vending)\n",
    "    train_data = remove_outliers_iqr(train_data, columns_for_iqr)\n",
    "\n",
    "    # Get number of outliers removed in next iteration\n",
    "    outliers_count = (\n",
    "        ~(\n",
    "            (\n",
    "                train_data[columns_for_iqr]\n",
    "                >= train_data[columns_for_iqr].quantile(0.25)\n",
    "                - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "            & (\n",
    "                train_data[columns_for_iqr]\n",
    "                <= train_data[columns_for_iqr].quantile(0.75)\n",
    "                + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "        ).all(axis=1)\n",
    "    ).sum()\n",
    "\n",
    "    # Increment iteration and stop if maximum number of iterations reached\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "# Reset index\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "# Print number of outliers removed\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "# Save number of outliers removed\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itiqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itiqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# Get number of rows before removing outliers\n",
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "# Remove outliers using Z-score method\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1zscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_data_raw()\n",
    "\n",
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itzscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itzscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using on Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1iqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1iqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_1iqr = get_scores(rf_classifier, \"1 IQR\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itiqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itiqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_itiqr = get_scores(rf_classifier, \"Iterative IQR\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1zscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1zscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_1zscore = get_scores(rf_classifier, \"1 Z-score\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itzscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itzscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_itzscore = get_scores(rf_classifier, \"Iterative Z-score\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat([scores_1iqr, scores_itiqr, scores_1zscore, scores_itzscore], axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dátová transformácia (scaling, transformer, ...).\n",
    "\n",
    "-   In previous step, we found that Iterative Z-score outlier detection gives better results, so we are going to use this method in this step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_clean():\n",
    "    file_path: str = \"../data/clean_methods\"\n",
    "\n",
    "    # Load cleaned data (Iterative Z-score)\n",
    "    train_data = pd.read_csv(f\"{file_path}/train_itzscore.csv\")\n",
    "    test_data = pd.read_csv(f\"{file_path}/test_itzscore.csv\")\n",
    "\n",
    "    # Define train & test data\n",
    "    X_train = train_data.drop(columns=[\"mwra\"])\n",
    "    y_train = train_data[\"mwra\"]\n",
    "\n",
    "    X_test = test_data.drop(columns=[\"mwra\"])\n",
    "    y_test = test_data[\"mwra\"]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "gaussian_columns = [\n",
    "    \"c.dogalize\",\n",
    "    \"c.android.gm\",\n",
    "    \"c.android.youtube\",\n",
    "    \"c.android.chrome\",\n",
    "    \"c.katana\",\n",
    "    \"c.raider\",\n",
    "    \"p.android.packageinstaller\",\n",
    "    \"p.android.settings\",\n",
    "    \"p.android.documentsui\",\n",
    "    \"p.android.chrome\",\n",
    "    \"p.android.gm\",\n",
    "    \"p.system\",\n",
    "    \"p.android.externalstorage\",\n",
    "    \"p.process.gapps\",\n",
    "    \"p.google\",\n",
    "    \"p.browser.provider\",\n",
    "    \"p.android.defcontainer\",\n",
    "]\n",
    "\n",
    "log_columns = [\"c.android.vending\"]\n",
    "\n",
    "uniform_columns = [\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "\n",
    "\n",
    "transformed_feature_order = pd.Series(gaussian_columns + log_columns + uniform_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define different pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our pipeline from 2nd Phase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "non_gaussian_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", RobustScaler()),\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", gaussian_pipeline, gaussian_columns),\n",
    "        (\"vending\", non_gaussian_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "robust_pipeline = Pipeline([(\"preprocessor\", preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining other pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline 1: Basic scaling\n",
    "basic_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "\n",
    "# Pipeline 2: Robust scaling for outlier handling\n",
    "robust_pipeline = Pipeline([(\"scaler\", RobustScaler()), (\"power\", PowerTransformer(method=\"yeo-johnson\"))])\n",
    "\n",
    "# Pipeline 3: Quantile transformation\n",
    "quantile_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"quantile\", QuantileTransformer(output_distribution=\"normal\", n_quantiles=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipeline 4: Min-max scaling with power transform\n",
    "minmax_pipeline = Pipeline([(\"scaler\", MinMaxScaler()), (\"power\", PowerTransformer(method=\"yeo-johnson\"))])\n",
    "\n",
    "# Pipeline 5: Log transform for skewed data\n",
    "log_pipeline = Pipeline([(\"log\", FunctionTransformer(func=np.log1p)), (\"scaler\", StandardScaler())])\n",
    "\n",
    "\n",
    "# Create column transformers for different data types\n",
    "def create_full_pipeline(gaussian_pipe, uniform_pipe, log_pipe):\n",
    "    return ColumnTransformer(\n",
    "        [\n",
    "            (\"gaussian\", gaussian_pipe, gaussian_columns),\n",
    "            (\"uniform\", uniform_pipe, uniform_columns),\n",
    "            (\"log\", log_pipe, log_columns),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "# Create different combinations to test\n",
    "pipeline_combinations = [\n",
    "    # Combination 1: Basic scaling for all\n",
    "    create_full_pipeline(basic_pipeline, basic_pipeline, basic_pipeline),\n",
    "    # Combination 2: Robust handling\n",
    "    create_full_pipeline(robust_pipeline, robust_pipeline, robust_pipeline),\n",
    "    # Combination 3: Type-specific transforms\n",
    "    create_full_pipeline(basic_pipeline, quantile_pipeline, log_pipeline),\n",
    "    # Combination 4: Quantile for all\n",
    "    create_full_pipeline(quantile_pipeline, quantile_pipeline, quantile_pipeline),\n",
    "    # Combination 5: MinMax with power transforms\n",
    "    create_full_pipeline(minmax_pipeline, minmax_pipeline, minmax_pipeline),\n",
    "]\n",
    "\n",
    "\n",
    "# Function to evaluate pipeline performance\n",
    "def evaluate_pipeline(pipeline, X_train, X_test, y_train, y_test):\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výber atribútov, výber algoritmov, hyperparameter tuning, ensemble learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ktorý model je Váš najlepší model pre nasadenie (deployment)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aký je data pipeline pre jeho vybudovanie na základe Vášho datasetu v produkcii?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
