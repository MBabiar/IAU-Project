{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from graphviz import Source\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython.display import HTML, SVG\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    learning_curve,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer, StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting cleaned data\n",
    "os.makedirs(\"../data/clean\", exist_ok=True)\n",
    "\n",
    "train_data.to_csv(\"../data/clean/train_data.csv\", index=False)\n",
    "test_data.to_csv(\"../data/clean/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/clean/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/clean/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Get selected features immediately after fitting\n",
    "# feature_mask = complete_pipeline.named_steps[\"selector\"].get_support()\n",
    "# selected_features = transformed_feature_order[feature_mask]  # order of features is preserved\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/processed/train_data.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/processed/test_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    file_path,\n",
    "    files,\n",
    "    file,\n",
    "    df,\n",
    "    train_data,\n",
    "    test_data,\n",
    "    columns_for_zscore,\n",
    "    outliers_count,\n",
    "    max_iterations,\n",
    "    iteration,\n",
    "    all_columns,\n",
    "    non_gaussian_columns,\n",
    "    gaussian_columns,\n",
    "    transformed_feature_order,\n",
    "    general_pipe,\n",
    "    vending_pipeline,\n",
    "    preprocessor,\n",
    "    complete_pipeline,\n",
    "    train_data_processed,\n",
    "    test_data_processed,\n",
    "    dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_processed = pd.read_csv(\"../data/processed/train_data.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/processed/test_data.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]\n",
    "\n",
    "# Train data with feature selection\n",
    "selector = SelectKBest(f_classif, k=7)\n",
    "train_selected = selector.fit_transform(X_train, y_train)\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "del selector, train_data_processed, test_data_processed, train_selected, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"],\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Train\", \"Test\"]]),\n",
    "    )\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Train\")] = accuracy_train\n",
    "    df.loc[\"precision\", (model_name, \"Train\")] = precision_train\n",
    "    df.loc[\"recall\", (model_name, \"Train\")] = recall_train\n",
    "    df.loc[\"roc_auc\", (model_name, \"Train\")] = roc_auc_train\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Test\")] = accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Test\")] = precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Test\")] = recall_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Test\")] = roc_auc_test\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores_cv(model, model_name, X, y, cv):\n",
    "    accuracy_scores = cross_val_score(model, X, y, cv=cv, scoring=\"accuracy\")\n",
    "    precision_scores = cross_val_score(model, X, y, cv=cv, scoring=\"precision\")\n",
    "    recall_scores = cross_val_score(model, X, y, cv=cv, scoring=\"recall\")\n",
    "    roc_auc_scores = cross_val_score(model, X, y, cv=cv, scoring=\"roc_auc\")\n",
    "\n",
    "    print(\"Accuracy scores:\")\n",
    "    print(accuracy_scores)\n",
    "    print(f\"Mean: {accuracy_scores.mean():.5f}\")\n",
    "    print(f\"Standard Deviation: {accuracy_scores.std():.5f}\")\n",
    "\n",
    "    print(\"\\nPrecision scores:\")\n",
    "    print(precision_scores)\n",
    "    print(f\"Mean: {precision_scores.mean():.5f}\")\n",
    "    print(f\"Standard Deviation: {precision_scores.std():.5f}\")\n",
    "\n",
    "    print(\"\\nRecall scores:\")\n",
    "    print(recall_scores)\n",
    "    print(f\"Mean: {recall_scores.mean():.5f}\")\n",
    "    print(f\"Standard Deviation: {recall_scores.std():.5f}\")\n",
    "\n",
    "    print(\"\\nROC AUC scores:\")\n",
    "    print(roc_auc_scores)\n",
    "    print(f\"Mean: {roc_auc_scores.mean():.5f}\")\n",
    "    print(f\"Standard Deviation: {roc_auc_scores.std():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def ignore(line, cell):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Jednoduchý klasifikátor na základe závislosti v dátach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naimplementujte jednoduchý ID3 klasifikátor s hĺbkou min 2 (vrátane root/koreň).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not using feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ID3 classifier\n",
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dtc_nofs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(\n",
    "    export_graphviz(dtc_nofs, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train.columns)\n",
    ")\n",
    "\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ID3 classifier\n",
    "dtc_fs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dtc_fs.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Source(\n",
    "    export_graphviz(\n",
    "        dtc_fs, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train_selected.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)\n",
    "\n",
    "del style, graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   There are slight differences between using and not using feature selection.\n",
    "-   We are going to use no feature selection for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte Váš ID3 klasifikátor pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_scores(dtc_nofs, \"DecisionTreeClassifier - No feature selection\", X_train, y_train, X_test, y_test)\n",
    "df = pd.concat(\n",
    "    [\n",
    "        df,\n",
    "        get_scores(\n",
    "            dtc_fs,\n",
    "            \"DecisionTreeClassifier - Feature selection\",\n",
    "            X_train_selected,\n",
    "            y_train,\n",
    "            X_test_selected,\n",
    "            y_test,\n",
    "        ),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that using feature selection gives worse results. We will look at it in more detail later (3.4).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zístite či Váš ID3 klasifikátor má overfit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train and test metrics are close, the model is likely not overfitting.\n",
    "-   But let's also look at learning curves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve - train sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Define the training sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 50)\n",
    "\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curve(estimator, X, y, train_sizes, cv, scoring, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = 1 - train_scores.mean(axis=1)\n",
    "    test_scores_mean = 1 - test_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-Validation Score\")\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(f\"{scoring} Error\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot learning curve for accuracy\n",
    "plot_learning_curve(\n",
    "    dtc_nofs,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    title=\"Learning Curve - Train size (Accuracy)\",\n",
    ")\n",
    "\n",
    "# Plot learning curve for precision\n",
    "plot_learning_curve(\n",
    "    dtc_nofs,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    train_sizes,\n",
    "    cv=5,\n",
    "    scoring=\"precision\",\n",
    "    title=\"Learning Curve - Train size (Precision)\",\n",
    ")\n",
    "\n",
    "# Plot learning curve for recall\n",
    "plot_learning_curve(\n",
    "    dtc_nofs, X_train, y_train, train_sizes, cv=5, scoring=\"recall\", title=\"Learning Curve - Train size (Recall)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We don't see a big gap in metrics between train and test data (Looking at last point since that is what we used in previous steps).\n",
    "-   However we see that model starts of badly (as expected) and then improves with more data. Around 50% of data for training seems to be enough.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curve - model complexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of max_depth values\n",
    "max_depth_range = range(1, 30)\n",
    "\n",
    "\n",
    "def plot_model_complexity_curve(model, X_train, y_train, X_test, y_test, max_depth_range):\n",
    "    # Initialize lists to store the training and validation errors\n",
    "    train_errors = []\n",
    "    val_errors = []\n",
    "\n",
    "    # Loop over the range of max_depth values\n",
    "    for max_depth in max_depth_range:\n",
    "        # Initialize the model with the current max_depth\n",
    "        model = DecisionTreeClassifier(\n",
    "            criterion=\"gini\",\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=10,\n",
    "            min_samples_leaf=1,\n",
    "            ccp_alpha=0.001,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Compute the training and testing data errors\n",
    "        train_score = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_score = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "        # Compute the mean errors\n",
    "        train_errors.append(1 - train_score)\n",
    "        val_errors.append(1 - test_score)\n",
    "\n",
    "    # Plot the learning curve for model complexity\n",
    "    plt.figure()\n",
    "    plt.plot(max_depth_range, train_errors, label=\"Training Error\")\n",
    "    plt.plot(max_depth_range, val_errors, label=\"Validation Error\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.title(\"Learning Curve (Model Complexity)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot model complexity curve\n",
    "plot_model_complexity_curve(dtc_nofs, X_train, y_train, X_test, y_test, max_depth_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This also shows no overfitting for our graph as there is no significant gap (looking at max_depth=15, as this is what we used in previous steps).\n",
    "-   We can also see that model start of very badly and is underfitting.\n",
    "-   Around max_depth=5 training and testing error stabilizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "dtc_nofs.fit(X_train, y_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for train data\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, dtc_nofs.predict_proba(X_train)[:, 1])\n",
    "roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Compute ROC curve and ROC area for test data\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, dtc_nofs.predict_proba(X_test)[:, 1])\n",
    "roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr_train, tpr_train, color=\"blue\", lw=2, label=f\"Train ROC curve (area = {roc_auc_train:.5f})\")\n",
    "plt.plot(fpr_test, tpr_test, color=\"red\", lw=2, label=f\"Test ROC curve (area = {roc_auc_test:.5f})\")\n",
    "plt.plot([0, 1], [0, 1], color=\"gray\", lw=2, linestyle=\"--\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   ROC also shows no significant gap between train and test data. So we can conclude that model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (\n",
    "    dtc_fs,\n",
    "    dtc_nofs,\n",
    "    fpr_train,\n",
    "    tpr_train,\n",
    "    fpr_test,\n",
    "    tpr_test,\n",
    "    train_sizes,\n",
    "    max_depth_range,\n",
    "    roc_auc_test,\n",
    "    roc_auc_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trénovanie a vyhodnotenie klasifikátorov strojového učenia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na trénovanie využite jeden stromový algoritmus v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte s jedným iným nestromovým algoritmom v scikit-learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LogisticRegression classifier\n",
    "log_reg = LogisticRegression(max_iter=100, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   LogisticRegression with default parameters gives slightly worse results than RandomForestClassifier with best parameters found in 3.3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porovnajte výsledky s ID3 z prvého kroku.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dtc_nofs.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df_1 = get_scores(dtc_nofs, \"DecisionTreeClassifier - No feature selection\", X_train, y_train, X_test, y_test)\n",
    "df_2 = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df_3 = get_scores(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "df = pd.concat([df_1, df_2, df_3], axis=1)\n",
    "del df_1, df_2, df_3\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   RandomForestClassifier and LogisticRegression preforms better than ID3 classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizualizujte natrénované pravidlá minimálne pre jeden Vami vybraný algoritmus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Get tree from random forest\n",
    "estimator = rf_classifier.estimators_[0]\n",
    "\n",
    "# Generate graph\n",
    "graph = Source(\n",
    "    export_graphviz(\n",
    "        estimator, out_file=None, class_names=[\"no\", \"yes\"], filled=True, feature_names=X_train.columns\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display graph\n",
    "display(SVG(graph.pipe(format=\"svg\")))\n",
    "\n",
    "style = \"<style>svg{width:100%;height:70%;}</style>\"\n",
    "HTML(style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnoťte natrénované modely pomocou metrík accuracy, precision a recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers\n",
    "dtc_nofs = DecisionTreeClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\", max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "dtc_nofs.fit(X_train, y_train)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df_1 = get_scores(dtc_nofs, \"DecisionTreeClassifier - No feature selection\", X_train, y_train, X_test, y_test)\n",
    "df_2 = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df_3 = get_scores(log_reg, \"LogisticRegression\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "df = pd.concat([df_1, df_2, df_3], axis=1)\n",
    "del df_1, df_2, df_3\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that RandomForestClassifier (with best parameters found in 3.3) gives the best results.\n",
    "-   LogisticRegression with default parameters gives slightly worse results.\n",
    "-   ID3 classifier gives the worst results.\n",
    "-   None of these models are overfitting because train and test metrics are close.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dtc_nofs, rf_classifier, log_reg, graph, style, estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimalizácia alias hyperparameter tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte rôzne nastavenie hyperparametrov (tuning) pre zvolený algoritmus tak,\n",
    "aby ste optimalizovali výkonnosť (bez underfitingu).\n",
    "\n",
    "We will test parameters for RandomForestClassifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, find baseline parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### n_estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_depth=20, ccp_alpha=0.001\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that performance is not increasing with n_estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ccp_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.002)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(criterion=\"entropy\", n_estimators=100, random_state=42, ccp_alpha=0.005)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see ccp_alpha 0.002 is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"sqrt\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    "    max_features=\"log2\",\n",
    "    max_depth=7,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   When also using max_depth, there is difference in ROC AUC, sqrt is better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criterion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"entropy\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    criterion=\"gini\",\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    ccp_alpha=0.002,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Entropy is better considering ROC AUC factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In previous step, we found that n_estimators=100, ccp_alpha=0.002, max_features=sqrt, criterion=entropy are better.\n",
    "-   However we are not gonna look at max_features and criterion in first tuning method as we will look at them at the end when we have best primary hyperparameters.\n",
    "-   Not using max_features and criterion in first tuning gives more priority to other hyperparameters.\n",
    "-   Our primary hyperparameters are n_estimators, max_depth, min_samples_split, min_samples_leaf, ccp_alpha.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the broad parameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"max_depth\": [5, 10, 15, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "    \"ccp_alpha\": [0.001, 0.002, 0.003],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_classifier,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=100,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters: {'n_estimators': 300, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_depth': 15, 'ccp_alpha': 0.002}\n",
    "-   Best Score: 0.91579\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Best parameters from RandomizedSearchCV\n",
    "best_params = {\n",
    "    \"n_estimators\": 300,\n",
    "    \"max_depth\": 15,\n",
    "    \"min_samples_split\": 10,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"ccp_alpha\": 0.002,\n",
    "}\n",
    "\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        best_params[\"n_estimators\"] - 100,\n",
    "        best_params[\"n_estimators\"],\n",
    "        best_params[\"n_estimators\"] + 100,\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        best_params[\"max_depth\"] - 5,\n",
    "        best_params[\"max_depth\"],\n",
    "        best_params[\"max_depth\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        best_params[\"min_samples_split\"] - 5,\n",
    "        best_params[\"min_samples_split\"],\n",
    "        best_params[\"min_samples_split\"] + 5,\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        best_params[\"min_samples_leaf\"],\n",
    "        best_params[\"min_samples_leaf\"] + 1,\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        best_params[\"ccp_alpha\"],\n",
    "        best_params[\"ccp_alpha\"] + 0.001,\n",
    "        best_params[\"ccp_alpha\"] + 0.002,\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.002, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 15, 'n_estimators': 400}\n",
    "-   Best Score after Refining: 0.91606\n",
    "-   The trend for n_estimators is, bigger is better (until some point). So in next iteration we will ignore this parameter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My colleague found different parameters to be better, so we will look at them in this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [\n",
    "        200\n",
    "    ],\n",
    "    \"max_depth\": [\n",
    "        15\n",
    "    ],\n",
    "    \"min_samples_split\": [\n",
    "        5,10\n",
    "    ],\n",
    "    \"min_samples_leaf\": [\n",
    "        1,2\n",
    "    ],\n",
    "    \"ccp_alpha\": [\n",
    "        0.001, 0.002\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
    "-   Best Score after Refining: 0.91596\n",
    "-   From these observations:\n",
    "    -   We can conclude that max_depth=~15 is the best, as it was 15 in every iteration.\n",
    "    -   We can conclude that ccp_alpha=~0.001 or cpp_alpha=~0.002 is the best.\n",
    "    -   We can conclude that min_samples_leaf=~1 or a little higher is the best.\n",
    "    -   We can conclude that min_samples_split=~10 is the best.\n",
    "    -   We can conclude that higher n_estimators is better (at some value it will be worse).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at cpp_alpha and max_depth in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [14, 15, 16],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.0095, 0.01, 0.0105],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   cpp_alpha=0.001 is best\n",
    "-   max_depth=15 is best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at best n_estimators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   n_estimators=300 is best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=15\n",
    "-   min_samples_split=10\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, we will also look at max_features and criterion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%ignore\n",
    "# Define the refined parameter grid for GridSearchCV\n",
    "param_grid_refined = {\n",
    "    \"n_estimators\": [300],\n",
    "    \"max_depth\": [15],\n",
    "    \"min_samples_split\": [10],\n",
    "    \"min_samples_leaf\": [1],\n",
    "    \"ccp_alpha\": [0.001],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"],\n",
    "    \"criterion\": [\"entropy\", \"gini\"],\n",
    "}\n",
    "\n",
    "# Initialize the RandomForestClassifier with the best parameters from RandomizedSearchCV\n",
    "rf_refined = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_refined = GridSearchCV(\n",
    "    estimator=rf_refined,\n",
    "    param_grid=param_grid_refined,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    cv=10,  # 10-fold cross-validation for more reliable results\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "grid_search_refined.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_refined = grid_search_refined.best_params_\n",
    "best_score_refined = grid_search_refined.best_score_\n",
    "\n",
    "print(f\"Best Parameters after Refining: {best_params_refined}\")\n",
    "print(f\"Best Score after Refining: {best_score_refined:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Best Parameters after Refining: {'ccp_alpha': 0.001, 'criterion': 'gini', 'max_depth': 15, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300}\n",
    "-   Best Score after Refining: 0.91632\n",
    "-   Just to note these are default values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final parameters are:\n",
    "\n",
    "-   n_estimators=300\n",
    "-   max_depth=15\n",
    "-   min_samples_split=10\n",
    "-   min_samples_leaf=1\n",
    "-   ccp_alpha=0.001\n",
    "-   max_features=\"sqrt\"\n",
    "-   criterion=\"gini\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyskúšajte kombinácie modelov (ensemble) pre zvolený algoritmus tak, aby ste\n",
    "optimalizovali výkonnosť (bez underfitingu).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Využite krížovú validáciu (cross validation) na trénovacej množine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print_scores_cv(rf_classifier, \"RandomForestClassifier\", X_train, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokážte že Váš nastavený najlepší model je bez overfitingu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "df = get_scores(rf_classifier, \"RandomForestClassifier\", X_train, y_train, X_test, y_test)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Since train metrics and test metrics are close, the model is not overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Define the training sizes\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "\n",
    "# Function to plot learning curves\n",
    "def plot_learning_curve(estimator, X, y, train_sizes, cv, scoring, title):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, train_sizes=train_sizes, scoring=scoring, n_jobs=-1\n",
    "    )\n",
    "    train_scores_mean = 1 - train_scores.mean(axis=1)\n",
    "    test_scores_mean = 1 - test_scores.mean(axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_sizes, train_scores_mean, label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, label=\"Cross-Validation Score\")\n",
    "    plt.xlabel(\"Training Size\")\n",
    "    plt.ylabel(scoring)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot learning curve for accuracy\n",
    "plot_learning_curve(\n",
    "    rf_classifier, X_train, y_train, train_sizes, cv=5, scoring=\"accuracy\", title=\"Learning Curve (Accuracy)\"\n",
    ")\n",
    "\n",
    "# Plot learning curve for precision\n",
    "plot_learning_curve(\n",
    "    rf_classifier, X_train, y_train, train_sizes, cv=5, scoring=\"precision\", title=\"Learning Curve (Precision)\"\n",
    ")\n",
    "\n",
    "# Plot learning curve for recall\n",
    "plot_learning_curve(\n",
    "    rf_classifier, X_train, y_train, train_sizes, cv=5, scoring=\"recall\", title=\"Learning Curve (Recall)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see that early stopping could also help, as there is slight decline in performance after some point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vyhodnotenie vplyvu zvolenej stratégie riešenia na klasifikáciu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratégie riešenia chýbajúcich hodnôt a outlierov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "# Define columns\n",
    "all_columns = train_data.drop(columns=[\"mwra\", \"ts\", \"imei\"]).columns\n",
    "non_gaussian_columns = [\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "    \"p.android.vending\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.olauncher\",\n",
    "    \"p.simulator\",\n",
    "    \"p.inputmethod.latin\",\n",
    "    \"p.android.gms\",\n",
    "    \"p.notifier\",\n",
    "    \"p.katana\",\n",
    "    \"p.gms.persistent\",\n",
    "]\n",
    "gaussian_columns = all_columns[~all_columns.isin(non_gaussian_columns)]\n",
    "transformed_feature_order = pd.Series(gaussian_columns.tolist() + non_gaussian_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pipelies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_pipe = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"power_transformer\", PowerTransformer(method=\"yeo-johnson\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "vending_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"quantile_transformer\", QuantileTransformer(output_distribution=\"normal\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"general\", general_pipe, gaussian_columns),\n",
    "        (\"vending\", vending_pipeline, non_gaussian_columns),\n",
    "    ],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create complete pipeline\n",
    "complete_pipeline = Pipeline([(\"preprocessor\", preprocessor)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers = pd.DataFrame(\n",
    "    index=[\"Number of Outliers\", \"Percentage of Outliers\"],\n",
    "    columns=[\"None-iterative IQR\", \"Iterative IQR\", \"None-iterative Z-score\", \"Iterative Z-score\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(data, columns):\n",
    "    Q1 = data[columns].quantile(0.25)\n",
    "    Q3 = data[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[columns] >= lower_bound).all(axis=1) & (data[columns] <= upper_bound).all(axis=1)]\n",
    "\n",
    "\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of rows removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# # Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export cleaned data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1iqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1iqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "columns_for_iqr = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "\n",
    "\n",
    "def remove_outliers_iqr(data, columns):\n",
    "    Q1 = data[columns].quantile(0.25)\n",
    "    Q3 = data[columns].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[columns] >= lower_bound).all(axis=1) & (data[columns] <= upper_bound).all(axis=1)]\n",
    "\n",
    "\n",
    "train_data = remove_outliers_iqr(train_data, train_data.iloc[:, 3:].columns)\n",
    "\n",
    "outliers_count = (\n",
    "    ~(\n",
    "        (\n",
    "            train_data[columns_for_iqr]\n",
    "            >= train_data[columns_for_iqr].quantile(0.25)\n",
    "            - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "        & (\n",
    "            train_data[columns_for_iqr]\n",
    "            <= train_data[columns_for_iqr].quantile(0.75)\n",
    "            + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "        )\n",
    "    ).all(axis=1)\n",
    ").sum()\n",
    "\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "while outliers_count > 0:\n",
    "    train_data = remove_outliers_iqr(train_data, columns_for_iqr)\n",
    "    outliers_count = (\n",
    "        ~(\n",
    "            (\n",
    "                train_data[columns_for_iqr]\n",
    "                >= train_data[columns_for_iqr].quantile(0.25)\n",
    "                - 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "            & (\n",
    "                train_data[columns_for_iqr]\n",
    "                <= train_data[columns_for_iqr].quantile(0.75)\n",
    "                + 1.5 * (train_data[columns_for_iqr].quantile(0.75) - train_data[columns_for_iqr].quantile(0.25))\n",
    "            )\n",
    "        ).all(axis=1)\n",
    "    ).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative IQR\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative IQR\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itiqr.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itiqr.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows_before_outliers = train_data.shape[0]\n",
    "\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"None-iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"None-iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_1zscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_1zscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path: str = \"../data/raw\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")\n",
    "    dataset[file] = dataset[file].drop_duplicates()\n",
    "\n",
    "df = pd.merge(dataset[\"connections\"], dataset[\"processes\"], on=[\"imei\", \"ts\", \"mwra\"])\n",
    "df[\"ts\"] = pd.to_datetime(df.ts)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 iteration of cleaning whole dataset of outliers (including p.android.vending)\n",
    "train_data = train_data[(np.abs(zscore(train_data.iloc[:, 3:])) < 3).all(axis=1)]\n",
    "\n",
    "# Using all columns except c.android.vending for outlier detection\n",
    "columns_for_zscore = train_data.iloc[:, 3:].columns.difference([\"p.android.vending\"])\n",
    "outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "max_iterations = 10\n",
    "iteration = 0\n",
    "\n",
    "# Iterating after we removed all outliers\n",
    "while outliers_count > 0:\n",
    "    train_data = train_data[(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)]\n",
    "    outliers_count = (~(np.abs(zscore(train_data[columns_for_zscore])) < 3).all(axis=1)).sum()\n",
    "    iteration += 1\n",
    "    if iteration >= max_iterations:\n",
    "        break\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of outliers removed: {number_of_rows_before_outliers - train_data.shape[0]}\")\n",
    "print(\n",
    "    f\"Percentage of outliers removed: {((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100:.2f}%\"\n",
    ")\n",
    "number_of_outliers.loc[\"Number of Outliers\", \"Iterative Z-score\"] = (\n",
    "    number_of_rows_before_outliers - train_data.shape[0]\n",
    ")\n",
    "number_of_outliers.loc[\"Percentage of Outliers\", \"Iterative Z-score\"] = round(\n",
    "    ((number_of_rows_before_outliers - train_data.shape[0]) / number_of_rows_before_outliers) * 100, 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pipelines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform training data, transform test data\n",
    "train_data_processed = complete_pipeline.fit_transform(train_data[all_columns], train_data[\"mwra\"])\n",
    "test_data_processed = complete_pipeline.transform(test_data[all_columns])\n",
    "\n",
    "# Create DataFrames with selected feature names\n",
    "train_data_processed = pd.DataFrame(train_data_processed, columns=transformed_feature_order)\n",
    "test_data_processed = pd.DataFrame(test_data_processed, columns=transformed_feature_order)\n",
    "\n",
    "# Add mwra column back to the DataFrames\n",
    "train_data_processed[\"mwra\"] = train_data[\"mwra\"]\n",
    "test_data_processed[\"mwra\"] = test_data[\"mwra\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Export processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../data/clean_methods\", exist_ok=True)\n",
    "\n",
    "train_data_processed.to_csv(\"../data/clean_methods/train_itzscore.csv\", index=False)\n",
    "test_data_processed.to_csv(\"../data/clean_methods/test_itzscore.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using on Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1iqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1iqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_1iqr = get_scores(rf_classifier, \"RandomForestClassifier - 1 IQR\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative IQR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itiqr.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itiqr.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_itiqr = get_scores(\n",
    "    rf_classifier, \"RandomForestClassifier - Iterative IQR\", X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### None-iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_1zscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_1zscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_1zscore = get_scores(rf_classifier, \"RandomForestClassifier - 1 Z-score\", X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative Z-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_data_processed = pd.read_csv(\"../data/clean_methods/train_itzscore.csv\")\n",
    "test_data_processed = pd.read_csv(\"../data/clean_methods/test_itzscore.csv\")\n",
    "\n",
    "# Train data without feature selection\n",
    "X_train = train_data_processed.drop(columns=[\"mwra\"])\n",
    "y_train = train_data_processed[\"mwra\"]\n",
    "\n",
    "# Test data without feature selection\n",
    "X_test = test_data_processed.drop(columns=[\"mwra\"])\n",
    "y_test = test_data_processed[\"mwra\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=10, min_samples_leaf=1, ccp_alpha=0.001, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Print the scores\n",
    "scores_itzscore = get_scores(\n",
    "    rf_classifier, \"RandomForestClassifier - Iterative Z-score\", X_train, y_train, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.concat([scores_1iqr, scores_itiqr, scores_1zscore, scores_itzscore], axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dátová transformácia (scaling, transformer, ...).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Výber atribútov, výber algoritmov, hyperparameter tuning, ensemble learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ktorý model je Váš najlepší model pre nasadenie (deployment)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aký je data pipeline pre jeho vybudovanie na základe Vášho datasetu v produkcii?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model, model_name, X_train, y_train, X_test, y_test):\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        index=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"],\n",
    "        columns=pd.MultiIndex.from_product([[model_name], [\"Train\", \"Test\"]]),\n",
    "    )\n",
    "\n",
    "    accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "    precision_train = precision_score(y_train, y_pred_train)\n",
    "    recall_train = recall_score(y_train, y_pred_train)\n",
    "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Train\")] = accuracy_train\n",
    "    df.loc[\"precision\", (model_name, \"Train\")] = precision_train\n",
    "    df.loc[\"recall\", (model_name, \"Train\")] = recall_train\n",
    "    df.loc[\"roc_auc\", (model_name, \"Train\")] = roc_auc_train\n",
    "\n",
    "    accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "    precision_test = precision_score(y_test, y_pred_test)\n",
    "    recall_test = recall_score(y_test, y_pred_test)\n",
    "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
    "\n",
    "    df.loc[\"accuracy\", (model_name, \"Test\")] = accuracy_test\n",
    "    df.loc[\"precision\", (model_name, \"Test\")] = precision_test\n",
    "    df.loc[\"recall\", (model_name, \"Test\")] = recall_test\n",
    "    df.loc[\"roc_auc\", (model_name, \"Test\")] = roc_auc_test\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [None, \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "\n",
    "parameters = {\n",
    "    \"penalty\": [\"l1\", \"l2\"],\n",
    "    \"C\": [0.001, 0.005, 0.01, 0.05],\n",
    "    \"max_iter\": [100, 150, 200, 250, 300],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    log_reg,\n",
    "    param_grid=parameters,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Score: {best_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=0.005, max_iter=100, penalty=\"l2\", solver=\"lbfgs\", random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_1 = get_scores(log_reg, \"LogisticRegression - lbfgs\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.005, max_iter=100, penalty=\"l2\", solver=\"liblinear\", random_state=42)\n",
    "log_reg.fit(X_train, y_train)\n",
    "df_2 = get_scores(log_reg, \"LogisticRegression - liblinear\", X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "df = pd.concat([df_1, df_2], axis=1)\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
