{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "file_path: str = \"../dataset\"\n",
    "files: tuple[str, ...] = (\"connections\", \"devices\", \"processes\", \"profiles\")\n",
    "\n",
    "dataset: dict[str, pd.DataFrame] = {}\n",
    "for file in files:\n",
    "    dataset[file] = pd.read_csv(f\"{file_path}/{file}.csv\", sep=\"\\t\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   importing the necessary libraries\n",
    "-   declaring the variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Bacis description of data along with their characteristics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connections description\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Total Entries: 15,108\n",
    "-   Total Columns: 13\n",
    "-   Column Types:\n",
    "-   11 columns of type float64\n",
    "-   1 column of type int64\n",
    "-   1 column of type object\n",
    "-   There are no missing values in this data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"][\"ts\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   The object column \"ts\" is date and time\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"][\"ts\"] = pd.to_datetime(dataset[\"connections\"][\"ts\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Cast the \"ts\" column to datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "connection_summary = dataset[\"connections\"].describe()\n",
    "median = (\n",
    "    dataset[\"connections\"].select_dtypes(include=[\"float64\", \"int64\"]).median()\n",
    ")  ## adding median to describe method output\n",
    "connection_summary.loc[\"median\"] = median\n",
    "\n",
    "## dropping imei, as it has no meaning to make these statistics out of it\n",
    "connection_summary.drop(columns=[\"imei\"], inplace=True)\n",
    "connection_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   From these tables above we can see imei is a long integer and looks like an ID, if we look at processes table, we can also see same values indicating this could be an Id of device.\n",
    "-   Another assumption we can make is that columns specifying a connection type (columns starting with c. such as c.android.youtube) have values ranging from 0 to 100, this could indicate that it is a percentage amount of time that the connection was established.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First few rows might indicate that the data was sample in a 1 minute interval.\n",
    "-   Let's look at it closer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "times = dataset[\"connections\"].sort_values(by=\"ts\")[\"ts\"]\n",
    "times"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Now we see it looks like samples are in a 1 minute interval.\n",
    "-   Let's go further.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "previous_time: Optional[pd.Series] = None\n",
    "\n",
    "same_times: int = 0\n",
    "non_minute_differences: int = 0\n",
    "\n",
    "\n",
    "for current_time in times:\n",
    "    if previous_time is None:\n",
    "        previous_time = current_time\n",
    "        continue\n",
    "\n",
    "    if (current_time - previous_time).seconds == 0:\n",
    "        same_times += 1\n",
    "\n",
    "    elif (current_time - previous_time).seconds != 60:\n",
    "        non_minute_differences += 1\n",
    "\n",
    "    previous_time = current_time\n",
    "\n",
    "print(f\"Non minute differences: {non_minute_differences}\")\n",
    "print(f\"Same times: {same_times}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   From this we can see, that there are data every minute, sometimes more than once at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "times = (\n",
    "    dataset[\"connections\"]\n",
    "    .groupby(by=\"imei\")[[\"imei\", \"ts\"]]\n",
    "    .apply(lambda val: val.sort_values(by=\"ts\", ascending=True))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "times"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   If we assume that columns starting with \"c.\" are representing percentage amount of time being active during a time window, we need to group them by device serial number (imei) and then look at the time difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"][\"mwra\"].value_counts()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   mwra is (Malware-related-activity)\n",
    "-   In data there are only values 1.0 and 0.0 indicating if there was a malware activity in specific time frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devices description\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"devices\"].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "devices_summary = dataset[\"devices\"].describe()\n",
    "median = (\n",
    "    dataset[\"devices\"].select_dtypes(include=[\"float64\", \"int64\"]).median()\n",
    ")  ## adding median to describe method output\n",
    "devices_summary.loc[\"median\"] = median\n",
    "\n",
    "## dropping imei, as it has no meaning to make these statistics out of it\n",
    "devices_summary.drop(columns=[\"imei\"], inplace=True)\n",
    "devices_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"devices\"].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   \"store_name\" object is a string\n",
    "-   \"code\" is string, holding code for state\n",
    "-   \"location\" is a string, containing continent and city\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processes description\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"processes\"].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"processes\"][\"ts\"] = pd.to_datetime(dataset[\"processes\"][\"ts\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "processes_summary = dataset[\"processes\"].describe()\n",
    "median = (\n",
    "    dataset[\"processes\"].select_dtypes(include=[\"float64\", \"int64\"]).median()\n",
    ")  ## adding median to describe method output\n",
    "processes_summary.loc[\"median\"] = median\n",
    "\n",
    "## dropping imei, as it has no meaning to make these statistics out of it\n",
    "processes_summary.drop(columns=[\"imei\"], inplace=True)\n",
    "processes_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"processes\"].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profiles description\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"profiles\"].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset[\"profiles\"][\"birthdate\"] = pd.to_datetime(dataset[\"profiles\"][\"birthdate\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "profiles_summary = dataset[\"profiles\"].describe()\n",
    "median = (\n",
    "    dataset[\"profiles\"].select_dtypes(include=[\"float64\", \"int64\"]).median()\n",
    ")  ## adding median to describe method output\n",
    "profiles_summary.loc[\"median\"] = median\n",
    "\n",
    "## dropping imei, as it has no meaning to make these statistics out of it\n",
    "profiles_summary.drop(columns=[\"imei\"], inplace=True)\n",
    "profiles_summary"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"profiles\"].head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "null_values = {file: data.isnull().sum() for file, data in dataset.items()}\n",
    "for file, nulls in null_values.items():\n",
    "    if nulls.sum() == 0:\n",
    "        continue\n",
    "    print(f\"Null values in {file} dataset:\")\n",
    "    print(nulls)\n",
    "    print(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MWRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   First we look at the most important column \"mwra\" and look at it more in depth.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"][\"mwra\"].value_counts(normalize=True) * 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   In \"connections\" we can see that positive mwra is ~62%, indicating that there are more positive cases and therefore in future when we put it into our model might falsely evaluate some connections. I would say the closer we are to 50/50 the better.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"processes\"][\"mwra\"].value_counts(normalize=True) * 100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   \"mwra\" is the same for \"processes\" as it is for \"connections\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "chrome_data = dataset[\"connections\"][\"c.android.chrome\"]\n",
    "chrome_mean = chrome_data.mean()\n",
    "chrome_std = chrome_data.std()\n",
    "\n",
    "dogalize_data = dataset[\"connections\"][\"c.dogalize\"]\n",
    "dogalize_mean = dogalize_data.mean()\n",
    "dogalize_std = dogalize_data.std()\n",
    "\n",
    "gm_data = dataset[\"connections\"][\"c.android.gm\"]\n",
    "gm_mean = gm_data.mean()\n",
    "gm_std = gm_data.std()\n",
    "\n",
    "youtube_data = dataset[\"connections\"][\"c.android.youtube\"]\n",
    "youtube_mean = youtube_data.mean()\n",
    "youtube_std = youtube_data.std()\n",
    "\n",
    "katana_data = dataset[\"connections\"][\"c.katana\"]\n",
    "katana_mean = katana_data.mean()\n",
    "katana_std = katana_data.std()\n",
    "\n",
    "raider_data = dataset[\"connections\"][\"c.raider\"]\n",
    "raider_mean = raider_data.mean()\n",
    "raider_std = raider_data.std()\n",
    "\n",
    "vending_data = dataset[\"connections\"][\"c.android.vending\"]\n",
    "vending_mean = vending_data.mean()\n",
    "vending_std = vending_data.std()\n",
    "\n",
    "x86_data = dataset[\"connections\"][\"c.UCMobile.x86\"]\n",
    "x86_mean = x86_data.mean()\n",
    "x86_std = x86_data.std()\n",
    "\n",
    "\n",
    "updateassist_data = dataset[\"connections\"][\"c.updateassist\"]\n",
    "updateassist_mean = updateassist_data.mean()\n",
    "updateassist_std = updateassist_data.std()\n",
    "\n",
    "intl_data = dataset[\"connections\"][\"c.UCMobile.intl\"]\n",
    "intl_mean = intl_data.mean()\n",
    "intl_std = intl_data.std()\n",
    "\n",
    "all_str_connections: list[str] = [\n",
    "    \"c.android.chrome\",\n",
    "    \"c.dogalize\",\n",
    "    \"c.android.gm\",\n",
    "    \"c.android.youtube\",\n",
    "    \"c.katana\",\n",
    "    \"c.raider\",\n",
    "    \"c.android.vending\",\n",
    "    \"c.UCMobile.x86\",\n",
    "    \"c.updateassist\",\n",
    "    \"c.UCMobile.intl\",\n",
    "]\n",
    "\n",
    "all_data_connections: list[pd.Series] = [\n",
    "    chrome_data,\n",
    "    dogalize_data,\n",
    "    gm_data,\n",
    "    youtube_data,\n",
    "    katana_data,\n",
    "    raider_data,\n",
    "    vending_data,\n",
    "    x86_data,\n",
    "    updateassist_data,\n",
    "    intl_data,\n",
    "]\n",
    "\n",
    "all_means_connections: list[float] = [\n",
    "    chrome_mean,\n",
    "    dogalize_mean,\n",
    "    gm_mean,\n",
    "    youtube_mean,\n",
    "    katana_mean,\n",
    "    raider_mean,\n",
    "    vending_mean,\n",
    "    x86_mean,\n",
    "    updateassist_mean,\n",
    "    intl_mean,\n",
    "]\n",
    "\n",
    "all_std_connections: list[float] = [\n",
    "    chrome_std,\n",
    "    dogalize_std,\n",
    "    gm_std,\n",
    "    youtube_std,\n",
    "    katana_std,\n",
    "    raider_std,\n",
    "    vending_std,\n",
    "    x86_std,\n",
    "    updateassist_std,\n",
    "    intl_std,\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of dispersion\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = dataset[\"connections\"].iloc[:, 3:].describe()\n",
    "df.loc[\"full_range\"] = df.loc[\"max\"] - df.loc[\"min\"]\n",
    "df.loc[\"interquartile_range\"] = df.loc[\"75%\"] - df.loc[\"25%\"]\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of center\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "apps = all_str_connections\n",
    "\n",
    "means = all_means_connections\n",
    "\n",
    "medians = [\n",
    "    chrome_data.median(),\n",
    "    dogalize_data.median(),\n",
    "    gm_data.median(),\n",
    "    youtube_data.median(),\n",
    "    katana_data.median(),\n",
    "    raider_data.median(),\n",
    "    vending_data.median(),\n",
    "    x86_data.median(),\n",
    "    updateassist_data.median(),\n",
    "    intl_data.median(),\n",
    "]\n",
    "\n",
    "max_values = []\n",
    "most_occurring_values = []\n",
    "\n",
    "value_counts = chrome_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = dogalize_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = gm_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = youtube_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = katana_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = raider_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = vending_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = x86_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = updateassist_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "value_counts = intl_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "for i in range(len(most_occurring_values)):\n",
    "    most_occurring_values[i] = most_occurring_values[i][: min(3, len(most_occurring_values[i]))]\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"connection\": apps,\n",
    "    \"mean\": means,\n",
    "    \"median\": medians,\n",
    "    \"mode_count\": max_values,\n",
    "    \"mode_values\": most_occurring_values,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of shape\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def skewness_type(skew_value: float) -> str:\n",
    "    if skew_value <= -1:\n",
    "        return \"Highly Negative Skew\"\n",
    "\n",
    "    elif skew_value <= -0.5:\n",
    "        return \"Moderately Negative Skew\"\n",
    "\n",
    "    elif skew_value <= 0.5:\n",
    "        return \"Approximately Symmetric\"\n",
    "\n",
    "    elif skew_value <= 1:\n",
    "        return \"Moderately Positive Skew\"\n",
    "\n",
    "    return \"Highly Positive Skew\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def kurtosis_type(kurtosis_value: float) -> str:\n",
    "    if kurtosis_value < -1:\n",
    "        return \"Negative Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < -0.5:\n",
    "        return \"Moderately Negative Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < 0.5:\n",
    "        return \"Approximately Normal Kurtosis\"\n",
    "\n",
    "    elif kurtosis_value < 1:\n",
    "        return \"Moderately Positive Kurtosis\"\n",
    "\n",
    "    return \"Positive Kurtosis\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = {\n",
    "    \"connection\": all_str_connections,\n",
    "    \"skew\": [\n",
    "        stats.skew(chrome_data),\n",
    "        stats.skew(dogalize_data),\n",
    "        stats.skew(gm_data),\n",
    "        stats.skew(youtube_data),\n",
    "        stats.skew(katana_data),\n",
    "        stats.skew(raider_data),\n",
    "        stats.skew(vending_data),\n",
    "        stats.skew(x86_data),\n",
    "        stats.skew(updateassist_data),\n",
    "        stats.skew(intl_data),\n",
    "    ],\n",
    "    \"kurtosis\": [\n",
    "        stats.kurtosis(chrome_data),\n",
    "        stats.kurtosis(dogalize_data),\n",
    "        stats.kurtosis(gm_data),\n",
    "        stats.kurtosis(youtube_data),\n",
    "        stats.kurtosis(katana_data),\n",
    "        stats.kurtosis(raider_data),\n",
    "        stats.kurtosis(vending_data),\n",
    "        stats.kurtosis(x86_data),\n",
    "        stats.kurtosis(updateassist_data),\n",
    "        stats.kurtosis(intl_data),\n",
    "    ],\n",
    "}\n",
    "\n",
    "shape_df = pd.DataFrame(data)\n",
    "\n",
    "shape_df[\"result skew\"] = shape_df[\"skew\"].apply(skewness_type)\n",
    "shape_df[\"result kurtosis\"] = shape_df[\"kurtosis\"].apply(kurtosis_type)\n",
    "shape_df.set_index(\"connection\", inplace=True)\n",
    "shape_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms with KDE\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(5, 2, figsize=(16, 26))\n",
    "\n",
    "\n",
    "## Having all the data in a list, we can iterate over it and plot the histogram with KDE for each connection.\n",
    "for i in range(len(all_data_connections)):\n",
    "    sns.histplot(all_data_connections[i], bins=30, kde=True, ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        all_means_connections[i], color=\"r\", linestyle=\"--\", label=f\"Mean: {all_means_connections[i]:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        linestyle=\"\", label=f\"Skewness: {shape_df.loc[all_str_connections[i]]['skew']:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        linestyle=\"\", label=f\"Kurtosis: {shape_df.loc[all_str_connections[i]]['kurtosis']:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_connections[i]}\")\n",
    "    axes[i // 2, i % 2].legend()\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "## Adjust the layout\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(5, 2, figsize=(16, 26))\n",
    "\n",
    "for i in range(len(all_data_connections)):\n",
    "    sns.histplot(\n",
    "        data=all_data_connections[i],\n",
    "        bins=30,\n",
    "        stat=\"density\",\n",
    "        alpha=0.3,\n",
    "        color=\"gray\",\n",
    "        label=\"Histogram\",\n",
    "        ax=axes[i // 2, i % 2],\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=all_data_connections[i],\n",
    "        color=\"blue\",\n",
    "        label=\"Actual Distribution\",\n",
    "        linewidth=2,\n",
    "        ax=axes[i // 2, i % 2],\n",
    "    )\n",
    "\n",
    "    if i <= 5:  ## We know that first 6 connections are not uniform\n",
    "        x = np.linspace(all_data_connections[i].min(), all_data_connections[i].max(), 100)\n",
    "        gaussian = stats.norm.pdf(x, all_means_connections[i], all_std_connections[i])\n",
    "        axes[i // 2, i % 2].plot(\n",
    "            x, gaussian, color=\"red\", linestyle=\"--\", label=\"Gaussian Model\", linewidth=2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        a = all_data_connections[i].min()\n",
    "        b = all_data_connections[i].max()\n",
    "        x = np.linspace(a, b, all_data_connections[i].size)\n",
    "        uniform_dist = stats.uniform(loc=a, scale=b - a)\n",
    "        axes[i // 2, i % 2].plot(\n",
    "            x, uniform_dist.pdf(x), color=\"red\", linestyle=\"--\", label=\"Uniform Model\", linewidth=2\n",
    "        )\n",
    "\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_connections[i]}\")\n",
    "    axes[i // 2, i % 2].legend()\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "## Adjust the layout\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(5, 2, figsize=(16, 26))\n",
    "\n",
    "for i in range(len(all_data_connections)):\n",
    "    sns.boxplot(all_data_connections[i], ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_connections[i]}\")\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "## Adjust the layout\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Q plots\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(5, 2, figsize=(16, 26))\n",
    "\n",
    "for i in range(len(all_data_connections)):\n",
    "    sm.qqplot(all_data_connections[i], fit=True, line=\"45\", ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_connections[i]}\")\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "## Adjust the layout\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "chrome_data = dataset[\"processes\"][\"p.android.chrome\"]\n",
    "chrome_mean = chrome_data.mean()\n",
    "chrome_std = chrome_data.std()\n",
    "\n",
    "dogalize_data = dataset[\"processes\"][\"p.dogalize\"]\n",
    "dogalize_mean = dogalize_data.mean()\n",
    "dogalize_std = dogalize_data.std()\n",
    "\n",
    "katana_data = dataset[\"processes\"][\"p.katana\"]\n",
    "katana_mean = katana_data.mean()\n",
    "katana_std = katana_data.std()\n",
    "\n",
    "settings_data = dataset[\"processes\"][\"p.android.settings\"]\n",
    "settings_mean = settings_data.mean()\n",
    "settings_std = settings_data.std()\n",
    "\n",
    "system_data = dataset[\"processes\"][\"p.system\"]\n",
    "system_mean = system_data.mean()\n",
    "system_std = system_data.std()\n",
    "\n",
    "simulator_data = dataset[\"processes\"][\"p.simulator\"]\n",
    "simulator_mean = simulator_data.mean()\n",
    "simulator_std = simulator_data.std()\n",
    "\n",
    "all_str_processes: list[str] = [\n",
    "    \"p.android.chrome\",\n",
    "    \"p.dogalize\",\n",
    "    \"p.katana\",\n",
    "    \"p.android.settings\",\n",
    "    \"p.system\",\n",
    "    \"p.simulator\",\n",
    "]\n",
    "\n",
    "all_data_processes: list[pd.Series] = [\n",
    "    chrome_data,\n",
    "    dogalize_data,\n",
    "    katana_data,\n",
    "    settings_data,\n",
    "    system_data,\n",
    "    simulator_data,\n",
    "]\n",
    "\n",
    "all_means_processes: list[float] = [\n",
    "    chrome_mean,\n",
    "    dogalize_mean,\n",
    "    katana_mean,\n",
    "    settings_mean,\n",
    "    system_mean,\n",
    "    simulator_mean,\n",
    "]\n",
    "\n",
    "all_std_processes: list[float] = [\n",
    "    chrome_std,\n",
    "    dogalize_std,\n",
    "    katana_std,\n",
    "    settings_std,\n",
    "    system_std,\n",
    "    simulator_std,\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of dispersion\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df = dataset[\"processes\"][all_str_processes].describe()\n",
    "df.loc[\"full_range\"] = df.loc[\"max\"] - df.loc[\"min\"]\n",
    "df.loc[\"interquartile_range\"] = df.loc[\"75%\"] - df.loc[\"25%\"]\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of center\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "apps = all_str_processes\n",
    "\n",
    "means = all_means_processes\n",
    "\n",
    "medians = [\n",
    "    chrome_data.median(),\n",
    "    dogalize_data.median(),\n",
    "    katana_data.median(),\n",
    "    settings_data.median(),\n",
    "    system_data.median(),\n",
    "    simulator_data.median(),\n",
    "]\n",
    "\n",
    "max_values = []\n",
    "most_occurring_values = []\n",
    "\n",
    "value_counts = chrome_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = dogalize_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = katana_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = settings_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = system_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "\n",
    "value_counts = simulator_data.value_counts()\n",
    "max_val = value_counts.max()\n",
    "max_values.append(max_val)\n",
    "most_occurring_values.append(value_counts[value_counts == max_val].index.tolist())\n",
    "\n",
    "for i in range(len(most_occurring_values)):\n",
    "    most_occurring_values[i] = most_occurring_values[i][: min(3, len(most_occurring_values[i]))]\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"process\": apps,\n",
    "    \"mean\": means,\n",
    "    \"median\": medians,\n",
    "    \"mode_count\": max_values,\n",
    "    \"mode_values\": most_occurring_values,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure of shape\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = {\n",
    "    \"process\": all_str_processes,\n",
    "    \"skew\": [\n",
    "        stats.skew(chrome_data),\n",
    "        stats.skew(dogalize_data),\n",
    "        stats.skew(katana_data),\n",
    "        stats.skew(settings_data),\n",
    "        stats.skew(system_data),\n",
    "        stats.skew(simulator_data),\n",
    "    ],\n",
    "    \"kurtosis\": [\n",
    "        stats.kurtosis(chrome_data),\n",
    "        stats.kurtosis(dogalize_data),\n",
    "        stats.kurtosis(katana_data),\n",
    "        stats.kurtosis(settings_data),\n",
    "        stats.kurtosis(system_data),\n",
    "        stats.kurtosis(simulator_data),\n",
    "    ],\n",
    "}\n",
    "\n",
    "shape_df = pd.DataFrame(data)\n",
    "shape_df[\"result skew\"] = shape_df[\"skew\"].apply(skewness_type)\n",
    "shape_df[\"result kurtosis\"] = shape_df[\"kurtosis\"].apply(kurtosis_type)\n",
    "shape_df.set_index(\"process\", inplace=True)\n",
    "shape_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms with KDE\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "for i in range(len(all_data_processes)):\n",
    "    sns.histplot(all_data_processes[i], bins=30, kde=True, ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        all_means_processes[i], color=\"r\", linestyle=\"--\", label=f\"Mean: {all_means_processes[i]:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        linestyle=\"\", label=f\"Skewness: {shape_df.loc[all_str_processes[i]]['skew']:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].axvline(\n",
    "        linestyle=\"\", label=f\"Kurtosis: {shape_df.loc[all_str_processes[i]]['kurtosis']:.2f}\"\n",
    "    )\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_processes[i]}\")\n",
    "    axes[i // 2, i % 2].legend()\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "gaussian_models = [chrome_data, settings_data, system_data]\n",
    "\n",
    "for i in range(len(all_data_processes)):\n",
    "    sns.histplot(\n",
    "        data=all_data_processes[i],\n",
    "        bins=30,\n",
    "        stat=\"density\",\n",
    "        alpha=0.3,\n",
    "        color=\"gray\",\n",
    "        label=\"Histogram\",\n",
    "        ax=axes[i // 2, i % 2],\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        data=all_data_processes[i],\n",
    "        color=\"blue\",\n",
    "        label=\"Actual Distribution\",\n",
    "        linewidth=2,\n",
    "        ax=axes[i // 2, i % 2],\n",
    "    )\n",
    "\n",
    "    if any(all_data_processes[i] is model for model in gaussian_models):\n",
    "        x = np.linspace(all_data_processes[i].min(), all_data_processes[i].max(), 100)\n",
    "        gaussian = stats.norm.pdf(x, all_means_processes[i], all_std_processes[i])\n",
    "        axes[i // 2, i % 2].plot(\n",
    "            x, gaussian, color=\"red\", linestyle=\"--\", label=\"Gaussian Model\", linewidth=2\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        a = all_data_processes[i].min()\n",
    "        b = all_data_processes[i].max()\n",
    "        x = np.linspace(a, b, all_data_processes[i].size)\n",
    "        uniform_dist = stats.uniform(loc=a, scale=b - a)\n",
    "        axes[i // 2, i % 2].plot(\n",
    "            x, uniform_dist.pdf(x), color=\"red\", linestyle=\"--\", label=\"Uniform Model\", linewidth=2\n",
    "        )\n",
    "\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_processes[i]}\")\n",
    "    axes[i // 2, i % 2].legend()\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "for i in range(len(all_data_processes)):\n",
    "    sns.boxplot(all_data_processes[i], ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_processes[i]}\")\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Q plots\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(3, 2, figsize=(16, 16))\n",
    "\n",
    "\n",
    "for i in range(len(all_data_processes)):\n",
    "    sm.qqplot(all_data_processes[i], fit=True, line=\"45\", ax=axes[i // 2, i % 2])\n",
    "    axes[i // 2, i % 2].set_title(f\"Distribution of {all_str_processes[i]}\")\n",
    "    axes[i // 2, i % 2].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "plt.tight_layout(w_pad=3, h_pad=3)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "matica = dataset[\"connections\"].iloc[:, 2:].corr()\n",
    "\n",
    "mask = np.triu(np.ones_like(matica, dtype=bool))\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(matica, mask=mask, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "matica = dataset[\"connections\"].iloc[:, 2:].corr(method=\"spearman\")\n",
    "\n",
    "mask = np.triu(np.ones_like(matica, dtype=bool))\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(matica, mask=mask, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "matica = dataset[\"processes\"].iloc[:, 2:].corr()\n",
    "matica = matica.round(2)\n",
    "\n",
    "mask = np.triu(np.ones_like(matica, dtype=bool))\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "sns.heatmap(matica, mask=mask, annot=True, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sns.pairplot(dataset[\"connections\"].iloc[:, 2:])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "processes_columns = [\"p.android.chrome\", \"p.dogalize\", \"p.katana\", \"p.android.gm\", \"p.android.vending\"]\n",
    "connections_columns = [\"c.android.chrome\", \"c.dogalize\", \"c.katana\", \"c.android.gm\", \"c.android.vending\"]\n",
    "\n",
    "combined_df = pd.concat(\n",
    "    [dataset[\"processes\"][processes_columns], dataset[\"connections\"][connections_columns]], axis=1\n",
    ")\n",
    "\n",
    "correlation_matrix = combined_df.corr(method=\"pearson\")\n",
    "\n",
    "filtered_corr = correlation_matrix.loc[processes_columns, connections_columns]\n",
    "\n",
    "mask = np.ones_like(filtered_corr, dtype=bool)\n",
    "np.fill_diagonal(mask, False)\n",
    "\n",
    "sns.heatmap(\n",
    "    filtered_corr,\n",
    "    mask=mask,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=connections_columns,\n",
    "    yticklabels=processes_columns,\n",
    ")\n",
    "plt.xlabel(\"Connections\")\n",
    "plt.ylabel(\"Processes\")\n",
    "plt.title(\"Korelácia medzi Processes a Connections (len stredná diagonála)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "mwra = dataset[\"connections\"][\"mwra\"]\n",
    "katana = dataset[\"connections\"][\"c.katana\"]\n",
    "\n",
    "sns.regplot(x=mwra, y=katana, line_kws={\"color\": \"red\"})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D.)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "connections_list = all_str_connections\n",
    "mwra_list = [\"mwra\"]\n",
    "\n",
    "combined_df = pd.concat([dataset[\"connections\"][connections_list], dataset[\"connections\"][mwra_list]], axis=1)\n",
    "\n",
    "correlation_matrix = combined_df.corr(method=\"pearson\")\n",
    "\n",
    "filtered_corr = correlation_matrix.loc[connections_list, mwra_list]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    filtered_corr,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=mwra_list,\n",
    "    yticklabels=connections_list,\n",
    ")\n",
    "plt.xlabel(\"Connections\")\n",
    "plt.ylabel(\"Processes\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "processes_list = dataset[\"processes\"].columns[3:]\n",
    "mwra_list = [\"mwra\"]\n",
    "\n",
    "combined_df = pd.concat([dataset[\"processes\"][processes_list], dataset[\"processes\"][mwra_list]], axis=1)\n",
    "\n",
    "correlation_matrix = combined_df.corr(method=\"pearson\")\n",
    "\n",
    "filtered_corr = correlation_matrix.loc[processes_list, mwra_list]\n",
    "\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(\n",
    "    filtered_corr,\n",
    "    annot=True,\n",
    "    cmap=\"coolwarm\",\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    xticklabels=mwra_list,\n",
    "    yticklabels=processes_list,\n",
    ")\n",
    "plt.xlabel(\"Connections\")\n",
    "plt.ylabel(\"Processes\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokumentujte Vaše prvotné zamyslenie k riešeniu zadania projektu, napr. sú\n",
    "niektoré atribúty medzi sebou závislé? od ktorých atribútov závisí predikovaná\n",
    "premenná? či je potrebné kombinovať záznamy z viacerých súborov?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations\n",
    "\n",
    "Connections:\n",
    "\n",
    "-   Correlation matrix for **mwra** in **connections** shows slight correlations between **c.dogalize** and **c.android.youtube**\n",
    "-   Surprisingly there is a negative correlation between **c.katana**\n",
    "\n",
    "Processes:\n",
    "\n",
    "-   Correlation matrix for **mwra** in **processes** shows slight correlation between **p.android.settings**\n",
    "-   There is also slight negative correlation between **p.system**\n",
    "\n",
    "#### Combination of data\n",
    "\n",
    "-   We can combine data from **connections** and **processes** by **imei** to get more data, especially important data that will be crucial in our model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 - Identification of problems, integration and data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 1.1-A we already transformed the \"ts\" column to datetime. WIth this we can expect every instance to be of the same format, if there were any errors in the data, the function would throw an error.\n",
    "\n",
    "But here is also a simple function to check if the data is in the correct format.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def check_correct_format(date: str) -> bool:\n",
    "    if date[4] == \"-\" and date[7] == \"-\" and date[10] == \" \" and date[13] == \":\" and date[16] == \":\":\n",
    "        return True\n",
    "    print(f\"Date {date} is not in correct format\")\n",
    "    return False\n",
    "\n",
    "\n",
    "# Since we already changed the format of the datetime, we will read cvs again\n",
    "df = pd.read_csv(\"../dataset/connections.csv\", sep=\"\\t\")\n",
    "bool_val = df[\"ts\"].apply(check_correct_format).all()\n",
    "print(bool_val)\n",
    "\n",
    "df = pd.read_csv(\"../dataset/processes.csv\", sep=\"\\t\")\n",
    "bool_val = df[\"ts\"].apply(check_correct_format).all()\n",
    "print(bool_val)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for key in dataset:\n",
    "    has_missing_values = dataset[key].isnull().values.any()\n",
    "    print(\n",
    "        f\"DataFrame {key:<12} has {dataset[key].isnull().sum().sum() if has_missing_values else 'no':<4} missing values\"\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We see that **profiles** have some missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "missing_columns = dataset[\"profiles\"].columns[dataset[\"profiles\"].isnull().any()]\n",
    "print(\"Columns with missing values:\", missing_columns)\n",
    "\n",
    "dataset[\"profiles\"][dataset[\"profiles\"].isnull().any(axis=1)][missing_columns].sample(10)\n",
    "dataset[\"profiles\"].info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   We can see the columns with missing values in **profiles**\n",
    "-   These columns hold no important information for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset[\"connections\"].iloc[:, 3:].plot(kind=\"box\")\n",
    "plt.show()\n",
    "# plt.boxplot(dataset[\"connections\"].iloc[:, 3:])\n",
    "# plt.invert_yaxis()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "for key in dataset:\n",
    "    has_duplicates = dataset[key].duplicated().any()\n",
    "    print(\n",
    "        f'DataFrame {key:<12} has {dataset[key].duplicated().sum() if has_duplicates else \"no\":<4} duplicates'\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"duplicates in connections:\")\n",
    "dataset[\"connections\"][dataset[\"connections\"].duplicated(keep=\"first\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"duplicates in devices:\")\n",
    "dataset[\"devices\"][dataset[\"devices\"].duplicated(keep=\"first\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"duplicates in processes:\")\n",
    "dataset[\"processes\"][dataset[\"processes\"].duplicated(keep=\"first\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"duplicates in profiles:\")\n",
    "dataset[\"profiles\"][dataset[\"profiles\"].duplicated(keep=\"first\")]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## B.)\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "profiles_copy = dataset[\"profiles\"].copy()\n",
    "\n",
    "before_drop = dataset[\"profiles\"].isnull().sum().sum()\n",
    "profiles_copy.dropna(inplace=True)\n",
    "\n",
    "after_drop = profiles_copy.isnull().sum().sum()\n",
    "\n",
    "print(f'DataFrame profiles had {before_drop} missing values, after drop {after_drop}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "-   Based on the data, after using first technique, that of a dropping rows with nan in them, we successfully removed all missing values from the **profiles** dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "profiles_copy = dataset[\"profiles\"].copy()\n",
    "\n",
    "before_manipulating = profiles_copy.isnull().sum().sum()\n",
    "number_of_rows = len(profiles_copy)\n",
    "number_of_values = profiles_copy.count().sum()\n",
    "missing_values = profiles_copy.isnull().sum().sum()\n",
    "\n",
    "profiles_copy['job'].fillna(profiles_copy['job'].mode()[0], inplace=True) #using mode\n",
    "profiles_copy['residence'].fillna(profiles_copy['residence'].mode()[0], inplace=True)\n",
    "\n",
    "profiles_copy['birth_year'] = profiles_copy['birthdate'].dt.year # taking out the year\n",
    "profiles_copy['birth_year'] = profiles_copy['birth_year'].interpolate(method='linear') #using interpolation\n",
    "\n",
    "profiles_copy['birthdate'] = pd.to_datetime(profiles_copy['birth_year'].round().astype(int), format='%Y', errors='coerce') # making the date to be 1.1.{mean_year}\n",
    "profiles_copy.drop(columns='birth_year', inplace=True) \n",
    "\n",
    "\n",
    "le_residence = LabelEncoder()\n",
    "profiles_copy['residence_encoded'] = le_residence.fit_transform(profiles_copy['residence'].astype(str)) # Encoding\n",
    "\n",
    "le_address = LabelEncoder()\n",
    "profiles_copy['address_encoded'] = le_address.fit_transform(profiles_copy['address'].fillna('NaN').astype(str)) # keeping the values\n",
    "\n",
    "subset = profiles_copy[['address_encoded', 'residence_encoded']]\n",
    "subset.loc[profiles_copy['address'].isnull(), 'address_encoded'] = np.nan\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "subset_imputed = imputer.fit_transform(subset) # using kNN\n",
    "\n",
    "profiles_copy['address_encoded'] = subset_imputed[:, 0]\n",
    "profiles_copy['address'] = le_address.inverse_transform(profiles_copy['address_encoded'].round().astype(int))\n",
    "\n",
    "profiles_copy.drop(columns=['address_encoded', 'residence_encoded'], inplace=True)\n",
    "\n",
    "after_manipulating = profiles_copy.isnull().sum().sum()\n",
    "number_of_rows_new = len(profiles_copy)\n",
    "number_of_values_new = profiles_copy.count().sum()\n",
    "missing_values_new = profiles_copy.isnull().sum().sum()\n",
    "\n",
    "print(f\"Before manipulating: {before_manipulating} nan rows, after manipulating: {after_manipulating} nan rows\")\n",
    "print(f\"Number of rows before: {number_of_rows}, after: {number_of_rows_new}\")\n",
    "print(f\"Number of values before: {number_of_values} + {missing_values} missing ({number_of_values + missing_values}), after: {number_of_values_new} + {missing_values_new} missing ({number_of_values_new + missing_values_new})\")\n",
    "profiles_copy"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
